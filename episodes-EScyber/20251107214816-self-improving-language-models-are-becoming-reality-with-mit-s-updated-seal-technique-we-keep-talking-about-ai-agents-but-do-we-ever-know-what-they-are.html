<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Self-improving language models are becoming reality with MIT's updated SEAL technique + We keep talking about AI agents, but do we ever know what they are?</title>
    <meta name="description" content="Episode page for Self-improving language models are becoming reality with MIT's updated SEAL technique + We keep talking about AI agents, but do we ever know what they are?" />
    <meta property="og:title" content="Self-improving language models are becoming reality with MIT's updated SEAL technique + We keep talking about AI agents, but do we ever know what they are?" />
    <meta property="og:type" content="music.song" />
    <meta name="music:duration" content="1775" />
    <style>
        body { font-family: system-ui, -apple-system, Segoe UI, Roboto, sans-serif; margin: 2rem; max-width: 70ch; }
        header { margin-bottom: 1.5rem; }
        ul { line-height: 1.6; }
        .meta { color: #555; font-size: 0.95rem; }
    </style>
    <meta name="date" content="2025-11-07T21:48:16.144225+00:00" />
    <meta name="duration" content="1775" />
    <meta name="audio" content="20251107214816-self-improving-language-models-are-becoming-reality-with-mit-s-updated-seal-technique-we-keep-talking-about-ai-agents-but-do-we-ever-know-what-they-are.mp3" />
    <link rel="alternate" type="audio/mpeg" href="20251107214816-self-improving-language-models-are-becoming-reality-with-mit-s-updated-seal-technique-we-keep-talking-about-ai-agents-but-do-we-ever-know-what-they-are.mp3" />
    <link rel="canonical" href="20251107214816-self-improving-language-models-are-becoming-reality-with-mit-s-updated-seal-technique-we-keep-talking-about-ai-agents-but-do-we-ever-know-what-they-are.html" />
    <script type="application/ld+json">{
        "@context": "https://schema.org",
        "@type": "PodcastEpisode",
        "name": "Self-improving language models are becoming reality with MIT's updated SEAL technique + We keep talking about AI agents, but do we ever know what they are?",
        "datePublished": "2025-11-07T21:48:16.144225+00:00",
        "timeRequired": "PT1775S",
        "associatedMedia": {
            "@type": "MediaObject",
            "contentUrl": "20251107214816-self-improving-language-models-are-becoming-reality-with-mit-s-updated-seal-technique-we-keep-talking-about-ai-agents-but-do-we-ever-know-what-they-are.mp3",
            "encodingFormat": "audio/mpeg"
        }
    }</script>
</head>
<body>
    <header>
        <h1>Self-improving language models are becoming reality with MIT's updated SEAL technique + We keep talking about AI agents, but do we ever know what they are?</h1>
        <p class="meta">Published: 2025-11-07T21:48:16.144225+00:00 · Duration: 29.6 min</p>
        <p><a href="20251107214816-self-improving-language-models-are-becoming-reality-with-mit-s-updated-seal-technique-we-keep-talking-about-ai-agents-but-do-we-ever-know-what-they-are.mp3">Download MP3</a></p>
    </header>
    <main>
        <h2>Articles discussed</h2>
        <ul><li><a href="https://venturebeat.com/ai/self-improving-language-models-are-becoming-reality-with-mits-updated-seal">Self-improving language models are becoming reality with MIT's updated SEAL technique</a>: Researchers at the Massachusetts Institute of Technology (MIT) are gaining renewed attention for developing and open sourcing a technique that allows large language models (LLMs) — like those underpinning ChatGPT and most modern AI chatbots — to improve themselves by generating synthetic data to fine-tune upon. The technique, known as SEAL (Self-Adapting LLMs), was first described in a paper published back in June and covered by VentureBeat at the time.A significantly expanded and updated version of the paper was released last month, as well as open source code posted on Github (under an MIT License, allowing for commercial and enterprise usage), and is making new waves among AI power users on the social network X this week.SEAL allows LLMs to autonomously generate and apply their own fine-tuning strategies. Unlike conventional models that rely on fixed external data and human-crafted optimization pipelines, SEAL enables models to evolve by producing their own synthetic training data and corresponding optimization directives.The development comes from a team affiliated with MIT’s Improbable AI</li><li><a href="https://venturebeat.com/ai/we-keep-talking-about-ai-agents-but-do-we-ever-know-what-they-are">We keep talking about AI agents, but do we ever know what they are?</a>: Imagine you do two things on a Monday morning.First, you ask a chatbot to summarize your new emails. Next, you ask an AI tool to figure out why your top competitor grew so fast last quarter. The AI silently gets to work. It scours financial reports, news articles and social media sentiment. It cross-references that data with your internal sales numbers, drafts a strategy outlining three potential reasons for the competitor's success and schedules a 30-minute meeting with your team to present its findings.We're calling both of these "AI agents," but they represent worlds of difference in intelligence, capability and the level of trust we place in them. This ambiguity creates a fog that makes it difficult to build, evaluate, and safely govern these powerful new tools. If we can't agree on what we're building, how can we know when we've succeeded?This post won't try to sell you on yet another definitive framework. Instead, think of it as a survey of the current landscape of agent autonomy, a map to help us all navigate the terrain together.What are we even talking about? Defining an "AI agent"Befo</li><li><a href="https://www.microsoft.com/en-us/security/blog/2025/11/07/whisper-leak-a-novel-side-channel-cyberattack-on-remote-language-models/">Whisper Leak: A novel side-channel attack on remote language models</a>: Microsoft has identified a novel side-channel attack on remote language models, dubbed "Whisper Leak," which exploits network packet sizes and timings to infer conversation topics despite end-to-end encryption. This attack allows adversaries to observe encrypted traffic and deduce sensitive information, posing risks in oppressive regimes or sensitive sectors like healthcare and law. The attack leverages the autoregressive nature of language models, which generate responses in sequential steps, making it possible to infer topics based on packet size variations. Microsoft collaborated with multiple vendors to mitigate these risks, implementing privacy enhancements in their language model frameworks. The research highlights the importance of robust anonymization and encryption techniques to safeguard user data in AI-driven interactions.</li></ul>
    </main>
</body>
</html>
