<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Whispering poetry at AI can make it break its own rules + Critical React, Next.js flaw lets hackers execute code on servers</title>
    <meta name="description" content="Episode page for Whispering poetry at AI can make it break its own rules + Critical React, Next.js flaw lets hackers execute code on servers" />
    <meta property="og:title" content="Whispering poetry at AI can make it break its own rules + Critical React, Next.js flaw lets hackers execute code on servers" />
    <meta property="og:type" content="music.song" />
    <meta name="music:duration" content="1832" />
    <style>
        body { font-family: system-ui, -apple-system, Segoe UI, Roboto, sans-serif; margin: 2rem; max-width: 70ch; }
        header { margin-bottom: 1.5rem; }
        ul { line-height: 1.6; }
        .meta { color: #555; font-size: 0.95rem; }
    </style>
    <meta name="date" content="2025-12-05T09:56:05.932060+00:00" />
    <meta name="duration" content="1832" />
    <meta name="audio" content="20251205095605-whispering-poetry-at-ai-can-make-it-break-its-own-rules-critical-react-next-js-flaw-lets-hackers-execute-code-on-servers.mp3" />
    <link rel="alternate" type="audio/mpeg" href="20251205095605-whispering-poetry-at-ai-can-make-it-break-its-own-rules-critical-react-next-js-flaw-lets-hackers-execute-code-on-servers.mp3" />
    <link rel="canonical" href="20251205095605-whispering-poetry-at-ai-can-make-it-break-its-own-rules-critical-react-next-js-flaw-lets-hackers-execute-code-on-servers.html" />
    <script type="application/ld+json">{
        "@context": "https://schema.org",
        "@type": "PodcastEpisode",
        "name": "Whispering poetry at AI can make it break its own rules + Critical React, Next.js flaw lets hackers execute code on servers",
        "datePublished": "2025-12-05T09:56:05.932060+00:00",
        "timeRequired": "PT1832S",
        "associatedMedia": {
            "@type": "MediaObject",
            "contentUrl": "20251205095605-whispering-poetry-at-ai-can-make-it-break-its-own-rules-critical-react-next-js-flaw-lets-hackers-execute-code-on-servers.mp3",
            "encodingFormat": "audio/mpeg"
        }
    }</script>
</head>
<body>
    <header>
        <h1>Whispering poetry at AI can make it break its own rules + Critical React, Next.js flaw lets hackers execute code on servers</h1>
        <p class="meta">Published: 2025-12-05T09:56:05.932060+00:00 · Duration: 30.5 min</p>
        <p><a href="20251205095605-whispering-poetry-at-ai-can-make-it-break-its-own-rules-critical-react-next-js-flaw-lets-hackers-execute-code-on-servers.mp3">Download MP3</a></p>
    </header>
    <main>
        <h2>Articles discussed</h2>
        <ul><li><a href="https://www.malwarebytes.com/blog/news/2025/12/whispering-poetry-at-ai-can-make-it-break-its-own-rules">Whispering poetry at AI can make it break its own rules</a>: Researchers at Icaro Lab, Sapienza University, and DEXAI tested whether giving AI instructions as poetry would make it harder to detect different types of dangerous content. The researchers found that poetic elements such as metaphor, rhythm, and unconventional framing might disrupt pattern-matching heuristics that the AI’s guardrails rely on to spot harmful content. The tests covered models across nine providers, including all the usual suspects: Google, OpenAI, Anthropic, Deepseek, and Meta. One way the researchers calculated the scores was by measuring the attack success rate (ASR) across each provider’s models. The researchers found that DeepSeek (an open-source model developed by researchers in China) was the least safe, with a 62% ASR. Google was the second least safe. Anthropic, which produces Claude, was the safest, with a 6.95 ASR. OpenAI, which makes ChatGPT, was the second most safe.</li><li><a href="https://www.bleepingcomputer.com/news/security/critical-react2shell-flaw-in-react-nextjs-lets-hackers-run-javascript-code/">Critical React, Next.js flaw lets hackers execute code on servers</a>: ## Critical React, Next.js Flaw Lets Hackers Execute Code on Servers A critical vulnerability, dubbed 'React2Shell', in the React Server Components (RSC) 'Flight' protocol allows remote code execution without authentication in React and Next.js applications. The security issue stems from insecure deserialization. It received a severity score of 10/10 and has been assigned the identifiers CVE-2025-55182 for React and CVE-2025-66478 (CVE rejected in the National Vulnerability Database) for Next.js. Security researcher Lachlan Davidson discovered the flaw and reported it to React on November 29. He found that an attacker could achieve remote code execution (RCE) by sending a specially crafted HTTP request to React Server Function endpoints. "Even if your app does not implement any React Server Function endpoints, it may still be vulnerable if your app supports React Server Components [RCS]," warns the security advisory from React. The following packages in their default configuration are impacted: react-server-dom-parcel react-server-dom-turbopack and react-server-dom-webpack React is an open-source JavaScript library for building user interfaces. It's maintained by Meta and widely adopted by organizations of all sizes for front-end web development. Next.js, maintained by Vercel, is a framework built on top of React that adds server-side rendering, routing, and API endpoints. Both solutions are</li><li><a href="https://go.theregister.com/feed/www.theregister.com/2025/12/03/exploitation_is_imminent_react_vulnerability/">'Exploitation is imminent' as 39 percent of cloud environs have max-severity React hole</a>: A critical vulnerability in the JavaScript library React has been identified, affecting 39% of cloud environments. This flaw, tracked as CVE-2025-55182, allows unauthenticated, remote attackers to execute malicious code on vulnerable instances. The React team disclosed the vulnerability, recommending immediate upgrades to versions 19.0.1, 19.1.2, and 19.2.1 to mitigate the risk. Vercel, the maintainer of Next.js, also issued a patch and alert for the affected framework. Researchers reported the flaw to Meta, which collaborated with React to swiftly release an emergency patch. The vulnerability's high severity and ease of exploitation make immediate patching critical, with potential for widespread attention and public disclosure of technical details.</li><li><a href="https://www.schneier.com/blog/archives/2025/11/prompt-injection-through-poetry.html">Prompt Injection Through Poetry</a>: Researchers have demonstrated that poetic prompts can effectively jailbreak large language models (LLMs), achieving high attack success rates (ASR) across various models. This method, termed "poetic jailbreaking," involves converting harmful prompts into verse, which bypasses contemporary safety mechanisms and highlights limitations in current alignment methods and evaluation protocols. The study utilized a curated dataset of 20 handcrafted adversarial poems in English and Italian, along with an additional 1,200 prompts from the MLCommons AILuminate Safety Benchmark, to test the effectiveness of poetic reframing in inducing aligned models to bypass refusal heuristics. The findings indicate that poetic framing can substantially outperform non-poetic baselines, with an average jailbreak success rate of 62% for hand-crafted poems and approximately 43% for meta-prompt conversions. This research underscores the need for improved safety mechanisms in LLMs to mitigate the risks posed by adversarial poetry.</li><li><a href="https://www.bleepingcomputer.com/news/security/north-korea-lures-engineers-to-rent-identities-in-fake-it-worker-scheme/">North Korea lures engineers to rent identities in fake IT worker scheme</a>: North Korean state-sponsored hackers, known as the Lazarus Group, have been using social engineering tactics to recruit engineers to rent their identities for illicit fundraising. The group uses stolen identities and AI, including deepfake videos, to infiltrate Western companies for espionage and revenue generation. The group uses a front-man engineer to pose as a figurehead in the operation to get a remote job at a targeted company. The front-man engineer would receive a percentage of the salary, between 20% and 35% for the duration of the contract. The compromised engineer takes all the risk as they rented their identity and will be the only one responsible for any damage done. The compromised engineer would have to let DPRK agents use their computer to hide the North Korean’s location and their traces.</li><li><a href="https://towardsdatascience.com/why-weve-been-optimizing-the-wrong-thing-in-llms-for-years/">Why We’ve Been Optimizing the Wrong Thing in LLMs for Years</a>: Researchers at Meta FAIR have introduced a novel approach to training Large Language Models (LLMs) called Multi-Token Prediction (MTP). This method leverages the latent capacity of LLMs to process multiple future tokens simultaneously, enhancing performance and inference speed. By dividing the model into a shared trunk and independent heads, MTP predicts future tokens concurrently, reducing inference time by up to three times. The architecture addresses memory bottlenecks through a sequential forward/backward pass strategy, maintaining similar batch sizes as standard models. Experimental results show MTP significantly outperforms traditional Next-Token Prediction (NTP) methods across various benchmarks. The study also explores design choices, such as parameter parity and head topology, revealing that parallel heads outperform causal heads in certain scenarios.</li><li><a href="https://www.bleepingcomputer.com/news/security/malicious-llms-empower-inexperienced-hackers-with-advanced-tools/">Malicious LLMs empower inexperienced hackers with advanced tools</a>: Unrestricted large language models (LLMs) like WormGPT 4 and KawaiiGPT are improving their capabilities to generate malicious code, delivering functional scripts for ransomware encryptors and lateral movement. [...]</li></ul>
    </main>
</body>
</html>
