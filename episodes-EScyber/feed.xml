<?xml version="1.0" ?>
<rss xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" version="2.0">
  <channel>
    <title>EScyber</title>
    <description>An AI generated podcast about cybersecurity and AI in Spanish.</description>
    <link>https://raw.githubusercontent.com/refugeek/podcast-feed/main/episodes-EScyber/feed.xml</link>
    <language>es-es</language>
    <itunes:author>Rfgk</itunes:author>
    <itunes:summary>An AI generated podcast about cybersecurity and AI in Spanish.</itunes:summary>
    <itunes:image href="https://raw.githubusercontent.com/refugeek/podcast-feed/main/episodes-EScyber/cover-EScyber.png"/>
    <item>
      <title>20251029091515-what-brain-privacy-will-look-like-in-the-age-of-neurotech-zero-trust-has-a-blind-spot-your-ai-agents</title>
      <description>Articles discussed
What brain privacy will look like in the age of neurotech (https://therecord.media/what-brain-privacy-will-look-like): Nita Farahany spoke with Recorded Future News about whether brain data will be commodified and the role artificial intelligence plays in allowing internal speech to be decoded.
Zero Trust Has a Blind Spot—Your AI Agents (https://www.bleepingcomputer.com/news/security/zero-trust-has-a-blind-spot-your-ai-agents/): AI agents now act, decide, and access systems on their own — creating new blind spots Zero Trust can't see. Token Security helps organizations govern AI identities so every agent's access, intent, and action are verified and accountable. [...]
New AI training method creates powerful software agents with just 78 examples (https://venturebeat.com/ai/new-ai-training-method-creates-powerful-software-agents-with-just-78): A new study by Shanghai Jiao Tong University and SII Generative AI Research Lab (GAIR) shows that training large language models (LLMs) for complex, autonomous tasks does not require massive datasets. Their framework, LIMI (Less Is More for Intelligent Agency), builds on similar work in other areas of LLM research and finds that “machine autonomy emerges not from data abundance but from strategic curation of high-quality agentic demonstrations.” In other words, it's data quality, not quantity, that matters. In experiments, the researchers found that with a small, but carefully curated, dataset of just 78 examples, they could train LLMs to outperform models trained on thousands of examples by a considerable margin on key industry benchmarks. This discovery could have important implications for enterprise applications where data is scarce or expensive to collect.The challenge of building agents that workThe researchers define agency as “the emergent capacity of AI systems to function as autonomous agents–actively discovering problems, formulating hypotheses, and executing solutions through self-d
Beatings, killings, and lasting fear: The human toll of MoD's Afghan data breach (https://go.theregister.com/feed/www.theregister.com/2025/10/28/impact_afghan_data_breach/): Research submitted to Parliament details deaths, raids, and mental trauma linked to 2022 relocation leak Research submitted to the UK Parliament has revealed explicit threats to life and the deaths of family members and colleagues directly linked to the Ministry of Defence's 2022 Afghan relocation scheme data breach.…
North Korea's Lazarus Group shares its malware with IT work scammers (https://go.theregister.com/feed/www.theregister.com/2025/09/25/lazarus_group_shares_malware_with_it_scammers/): North Korean-linked cybercriminals have enhanced their malware capabilities, sharing advanced tools with IT work scammers. DeceptiveDevelopment, a North Korea-aligned group active since 2023, overlaps with campaigns like Contagious Interview and WageMole, and is tracked by CrowdStrike as Famous Chollima. The group uses social engineering tactics, such as fake job profiles and CAPTCHA prompts, to infect victims' computers with trojanized codebases. They steal data and identities to help North Korean IT workers secure jobs with Western companies, funneling salaries back to Pyongyang. DeceptiveDevelopment's malware payloads include BeaverTail, InvisibleFerret, and OtterCookie, with BeaverTail evolving into OtterCookie. BeaverTail is an infostealer targeting cryptocurrency wallets and browser logins, while InvisibleFerret is a Python-based malware with remote control capabilities. Tropidoor, a sophisticated backdoor, supports Windows commands like schtasks and ping, and TsunamiKit, a toolkit for stealing information and cryptocurrency, predates DeceptiveDevelopment's activity. The overlap between DeceptiveDevelopment's campaigns and North Korean IT worker operations highlights the need for defenders to consider broader threat ecosystems.</description>
      <itunes:summary>Articles discussed
What brain privacy will look like in the age of neurotech (https://therecord.media/what-brain-privacy-will-look-like): Nita Farahany spoke with Recorded Future News about whether brain data will be commodified and the role artificial intelligence plays in allowing internal speech to be decoded.
Zero Trust Has a Blind Spot—Your AI Agents (https://www.bleepingcomputer.com/news/security/zero-trust-has-a-blind-spot-your-ai-agents/): AI agents now act, decide, and access systems on their own — creating new blind spots Zero Trust can't see. Token Security helps organizations govern AI identities so every agent's access, intent, and action are verified and accountable. [...]
New AI training method creates powerful software agents with just 78 examples (https://venturebeat.com/ai/new-ai-training-method-creates-powerful-software-agents-with-just-78): A new study by Shanghai Jiao Tong University and SII Generative AI Research Lab (GAIR) shows that training large language models (LLMs) for complex, autonomous tasks does not require massive datasets. Their framework, LIMI (Less Is More for Intelligent Agency), builds on similar work in other areas of LLM research and finds that “machine autonomy emerges not from data abundance but from strategic curation of high-quality agentic demonstrations.” In other words, it's data quality, not quantity, that matters. In experiments, the researchers found that with a small, but carefully curated, dataset of just 78 examples, they could train LLMs to outperform models trained on thousands of examples by a considerable margin on key industry benchmarks. This discovery could have important implications for enterprise applications where data is scarce or expensive to collect.The challenge of building agents that workThe researchers define agency as “the emergent capacity of AI systems to function as autonomous agents–actively discovering problems, formulating hypotheses, and executing solutions through self-d
Beatings, killings, and lasting fear: The human toll of MoD's Afghan data breach (https://go.theregister.com/feed/www.theregister.com/2025/10/28/impact_afghan_data_breach/): Research submitted to Parliament details deaths, raids, and mental trauma linked to 2022 relocation leak Research submitted to the UK Parliament has revealed explicit threats to life and the deaths of family members and colleagues directly linked to the Ministry of Defence's 2022 Afghan relocation scheme data breach.…
North Korea's Lazarus Group shares its malware with IT work scammers (https://go.theregister.com/feed/www.theregister.com/2025/09/25/lazarus_group_shares_malware_with_it_scammers/): North Korean-linked cybercriminals have enhanced their malware capabilities, sharing advanced tools with IT work scammers. DeceptiveDevelopment, a North Korea-aligned group active since 2023, overlaps with campaigns like Contagious Interview and WageMole, and is tracked by CrowdStrike as Famous Chollima. The group uses social engineering tactics, such as fake job profiles and CAPTCHA prompts, to infect victims' computers with trojanized codebases. They steal data and identities to help North Korean IT workers secure jobs with Western companies, funneling salaries back to Pyongyang. DeceptiveDevelopment's malware payloads include BeaverTail, InvisibleFerret, and OtterCookie, with BeaverTail evolving into OtterCookie. BeaverTail is an infostealer targeting cryptocurrency wallets and browser logins, while InvisibleFerret is a Python-based malware with remote control capabilities. Tropidoor, a sophisticated backdoor, supports Windows commands like schtasks and ping, and TsunamiKit, a toolkit for stealing information and cryptocurrency, predates DeceptiveDevelopment's activity. The overlap between DeceptiveDevelopment's campaigns and North Korean IT worker operations highlights the need for defenders to consider broader threat ecosystems.</itunes:summary>
      <description>Articles discussed
What brain privacy will look like in the age of neurotech (https://therecord.media/what-brain-privacy-will-look-like): Nita Farahany spoke with Recorded Future News about whether brain data will be commodified and the role artificial intelligence plays in allowing internal speech to be decoded.
Zero Trust Has a Blind Spot—Your AI Agents (https://www.bleepingcomputer.com/news/security/zero-trust-has-a-blind-spot-your-ai-agents/): AI agents now act, decide, and access systems on their own — creating new blind spots Zero Trust can't see. Token Security helps organizations govern AI identities so every agent's access, intent, and action are verified and accountable. [...]
New AI training method creates powerful software agents with just 78 examples (https://venturebeat.com/ai/new-ai-training-method-creates-powerful-software-agents-with-just-78): A new study by Shanghai Jiao Tong University and SII Generative AI Research Lab (GAIR) shows that training large language models (LLMs) for complex, autonomous tasks does not require massive datasets. Their framework, LIMI (Less Is More for Intelligent Agency), builds on similar work in other areas of LLM research and finds that “machine autonomy emerges not from data abundance but from strategic curation of high-quality agentic demonstrations.” In other words, it's data quality, not quantity, that matters. In experiments, the researchers found that with a small, but carefully curated, dataset of just 78 examples, they could train LLMs to outperform models trained on thousands of examples by a considerable margin on key industry benchmarks. This discovery could have important implications for enterprise applications where data is scarce or expensive to collect.The challenge of building agents that workThe researchers define agency as “the emergent capacity of AI systems to function as autonomous agents–actively discovering problems, formulating hypotheses, and executing solutions through self-d
Beatings, killings, and lasting fear: The human toll of MoD's Afghan data breach (https://go.theregister.com/feed/www.theregister.com/2025/10/28/impact_afghan_data_breach/): Research submitted to Parliament details deaths, raids, and mental trauma linked to 2022 relocation leak Research submitted to the UK Parliament has revealed explicit threats to life and the deaths of family members and colleagues directly linked to the Ministry of Defence's 2022 Afghan relocation scheme data breach.…
North Korea's Lazarus Group shares its malware with IT work scammers (https://go.theregister.com/feed/www.theregister.com/2025/09/25/lazarus_group_shares_malware_with_it_scammers/): North Korean-linked cybercriminals have enhanced their malware capabilities, sharing advanced tools with IT work scammers. DeceptiveDevelopment, a North Korea-aligned group active since 2023, overlaps with campaigns like Contagious Interview and WageMole, and is tracked by CrowdStrike as Famous Chollima. The group uses social engineering tactics, such as fake job profiles and CAPTCHA prompts, to infect victims' computers with trojanized codebases. They steal data and identities to help North Korean IT workers secure jobs with Western companies, funneling salaries back to Pyongyang. DeceptiveDevelopment's malware payloads include BeaverTail, InvisibleFerret, and OtterCookie, with BeaverTail evolving into OtterCookie. BeaverTail is an infostealer targeting cryptocurrency wallets and browser logins, while InvisibleFerret is a Python-based malware with remote control capabilities. Tropidoor, a sophisticated backdoor, supports Windows commands like schtasks and ping, and TsunamiKit, a toolkit for stealing information and cryptocurrency, predates DeceptiveDevelopment's activity. The overlap between DeceptiveDevelopment's campaigns and North Korean IT worker operations highlights the need for defenders to consider broader threat ecosystems.</description>
      <itunes:summary>Articles discussed
What brain privacy will look like in the age of neurotech (https://therecord.media/what-brain-privacy-will-look-like): Nita Farahany spoke with Recorded Future News about whether brain data will be commodified and the role artificial intelligence plays in allowing internal speech to be decoded.
Zero Trust Has a Blind Spot—Your AI Agents (https://www.bleepingcomputer.com/news/security/zero-trust-has-a-blind-spot-your-ai-agents/): AI agents now act, decide, and access systems on their own — creating new blind spots Zero Trust can't see. Token Security helps organizations govern AI identities so every agent's access, intent, and action are verified and accountable. [...]
New AI training method creates powerful software agents with just 78 examples (https://venturebeat.com/ai/new-ai-training-method-creates-powerful-software-agents-with-just-78): A new study by Shanghai Jiao Tong University and SII Generative AI Research Lab (GAIR) shows that training large language models (LLMs) for complex, autonomous tasks does not require massive datasets. Their framework, LIMI (Less Is More for Intelligent Agency), builds on similar work in other areas of LLM research and finds that “machine autonomy emerges not from data abundance but from strategic curation of high-quality agentic demonstrations.” In other words, it's data quality, not quantity, that matters. In experiments, the researchers found that with a small, but carefully curated, dataset of just 78 examples, they could train LLMs to outperform models trained on thousands of examples by a considerable margin on key industry benchmarks. This discovery could have important implications for enterprise applications where data is scarce or expensive to collect.The challenge of building agents that workThe researchers define agency as “the emergent capacity of AI systems to function as autonomous agents–actively discovering problems, formulating hypotheses, and executing solutions through self-d
Beatings, killings, and lasting fear: The human toll of MoD's Afghan data breach (https://go.theregister.com/feed/www.theregister.com/2025/10/28/impact_afghan_data_breach/): Research submitted to Parliament details deaths, raids, and mental trauma linked to 2022 relocation leak Research submitted to the UK Parliament has revealed explicit threats to life and the deaths of family members and colleagues directly linked to the Ministry of Defence's 2022 Afghan relocation scheme data breach.…
North Korea's Lazarus Group shares its malware with IT work scammers (https://go.theregister.com/feed/www.theregister.com/2025/09/25/lazarus_group_shares_malware_with_it_scammers/): North Korean-linked cybercriminals have enhanced their malware capabilities, sharing advanced tools with IT work scammers. DeceptiveDevelopment, a North Korea-aligned group active since 2023, overlaps with campaigns like Contagious Interview and WageMole, and is tracked by CrowdStrike as Famous Chollima. The group uses social engineering tactics, such as fake job profiles and CAPTCHA prompts, to infect victims' computers with trojanized codebases. They steal data and identities to help North Korean IT workers secure jobs with Western companies, funneling salaries back to Pyongyang. DeceptiveDevelopment's malware payloads include BeaverTail, InvisibleFerret, and OtterCookie, with BeaverTail evolving into OtterCookie. BeaverTail is an infostealer targeting cryptocurrency wallets and browser logins, while InvisibleFerret is a Python-based malware with remote control capabilities. Tropidoor, a sophisticated backdoor, supports Windows commands like schtasks and ping, and TsunamiKit, a toolkit for stealing information and cryptocurrency, predates DeceptiveDevelopment's activity. The overlap between DeceptiveDevelopment's campaigns and North Korean IT worker operations highlights the need for defenders to consider broader threat ecosystems.</itunes:summary>
      <pubDate>Sat, 17 Jan 2026 15:58:59 GMT</pubDate>
      <enclosure url="https://raw.githubusercontent.com/Refugeek/podcast-feed/main/episodes-EScyber/20251029091515-what-brain-privacy-will-look-like-in-the-age-of-neurotech-zero-trust-has-a-blind-spot-your-ai-agents.mp3" type="audio/mpeg" length="34708364"/>
      <itunes:duration>1735</itunes:duration>
    </item>
    <item>
      <title>20251102104026-is-rag-dead-the-rise-of-context-engineering-and-semantic-layers-for-agentic-ai-greg-kroah-hartman-explains-the-cyber-resilience-act-for-open-source-developers</title>
      <description>Articles discussed
Is RAG Dead? The Rise of Context Engineering and Semantic Layers for Agentic AI (https://towardsdatascience.com/beyond-rag/): Retrieval-Augmented Generation (RAG) has evolved into a broader field known as context engineering, which is essential for agentic AI systems. Initially popularized by ChatGPT, RAG aimed to enhance LLM responses by retrieving relevant information at query time. However, its limitations in handling incorrect or irrelevant data led to the development of new techniques like re-rankers. GraphRAG, a variation of RAG using knowledge graphs, has gained prominence for its ability to reason over entities and relationships, improving LLM accuracy and explainability. This evolution has spurred significant industry activity, with companies like Microsoft, Samsung, and Ontotext acquiring companies to advance knowledge graphs and semantic technologies. The rise of GraphRAG has reignited interest in knowledge graphs, positioning them as critical components in AI systems, moving beyond metadata management to become the semantic backbone for AI applications.
Greg Kroah-Hartman explains the Cyber Resilience Act for open source developers (https://go.theregister.com/feed/www.theregister.com/2025/09/30/cyber_reiliance_act_opinion_column/): The European Union's Cyber Resilience Act (CRA) mandates digital product producers to document vulnerabilities and respond to security incidents, impacting open-source software developers. Greg Kroah-Hartman, a Linux kernel maintainer, argues the CRA benefits open-source contributors by ensuring transparency and accountability. The CRA distinguishes between commercial entities and unpaid contributors, allowing individuals to publish software without legal risk. It applies to commercial manufacturers using open-source code, requiring Software Bill of Materials (SBOMs) and proactive incident management. While the CRA primarily affects EU-based manufacturers, its global reach extends to companies selling products accessible within the EU. The act is expected to increase demand for well-supported open-source projects, countering fears of reduced open-source use. Foundations and large projects are collaborating with the EU to simplify compliance through checklists and templates.
This security hole can crash billions of Chromium browsers, and Google hasn't patched it yet (https://go.theregister.com/feed/www.theregister.com/2025/10/29/brash_dos_attack_crashes_chromium/): Security researcher Jose Pino discovered a critical vulnerability in Chromium's Blink rendering engine, which can be exploited to crash many Chromium-based browsers within seconds. This flaw, known as Brash, affects over 3 billion users worldwide, as it can be used to crash browsers like Chrome, Microsoft Edge, Brave, and Vivaldi. Pino demonstrated the vulnerability by testing it on 11 major browsers, finding that it works on nine of them. The attack exploits an architectural flaw in Blink, specifically the absence of rate limiting on document.title API updates, allowing for millions of DOM mutations per second. This saturates the main thread, disrupting the event loop and causing the interface to collapse. Pino disclosed the issue to the Chromium security team, but did not receive a response.</description>
      <itunes:summary>Articles discussed
Is RAG Dead? The Rise of Context Engineering and Semantic Layers for Agentic AI (https://towardsdatascience.com/beyond-rag/): Retrieval-Augmented Generation (RAG) has evolved into a broader field known as context engineering, which is essential for agentic AI systems. Initially popularized by ChatGPT, RAG aimed to enhance LLM responses by retrieving relevant information at query time. However, its limitations in handling incorrect or irrelevant data led to the development of new techniques like re-rankers. GraphRAG, a variation of RAG using knowledge graphs, has gained prominence for its ability to reason over entities and relationships, improving LLM accuracy and explainability. This evolution has spurred significant industry activity, with companies like Microsoft, Samsung, and Ontotext acquiring companies to advance knowledge graphs and semantic technologies. The rise of GraphRAG has reignited interest in knowledge graphs, positioning them as critical components in AI systems, moving beyond metadata management to become the semantic backbone for AI applications.
Greg Kroah-Hartman explains the Cyber Resilience Act for open source developers (https://go.theregister.com/feed/www.theregister.com/2025/09/30/cyber_reiliance_act_opinion_column/): The European Union's Cyber Resilience Act (CRA) mandates digital product producers to document vulnerabilities and respond to security incidents, impacting open-source software developers. Greg Kroah-Hartman, a Linux kernel maintainer, argues the CRA benefits open-source contributors by ensuring transparency and accountability. The CRA distinguishes between commercial entities and unpaid contributors, allowing individuals to publish software without legal risk. It applies to commercial manufacturers using open-source code, requiring Software Bill of Materials (SBOMs) and proactive incident management. While the CRA primarily affects EU-based manufacturers, its global reach extends to companies selling products accessible within the EU. The act is expected to increase demand for well-supported open-source projects, countering fears of reduced open-source use. Foundations and large projects are collaborating with the EU to simplify compliance through checklists and templates.
This security hole can crash billions of Chromium browsers, and Google hasn't patched it yet (https://go.theregister.com/feed/www.theregister.com/2025/10/29/brash_dos_attack_crashes_chromium/): Security researcher Jose Pino discovered a critical vulnerability in Chromium's Blink rendering engine, which can be exploited to crash many Chromium-based browsers within seconds. This flaw, known as Brash, affects over 3 billion users worldwide, as it can be used to crash browsers like Chrome, Microsoft Edge, Brave, and Vivaldi. Pino demonstrated the vulnerability by testing it on 11 major browsers, finding that it works on nine of them. The attack exploits an architectural flaw in Blink, specifically the absence of rate limiting on document.title API updates, allowing for millions of DOM mutations per second. This saturates the main thread, disrupting the event loop and causing the interface to collapse. Pino disclosed the issue to the Chromium security team, but did not receive a response.</itunes:summary>
      <description>Articles discussed
Is RAG Dead? The Rise of Context Engineering and Semantic Layers for Agentic AI (https://towardsdatascience.com/beyond-rag/): Retrieval-Augmented Generation (RAG) has evolved into a broader field known as context engineering, which is essential for agentic AI systems. Initially popularized by ChatGPT, RAG aimed to enhance LLM responses by retrieving relevant information at query time. However, its limitations in handling incorrect or irrelevant data led to the development of new techniques like re-rankers. GraphRAG, a variation of RAG using knowledge graphs, has gained prominence for its ability to reason over entities and relationships, improving LLM accuracy and explainability. This evolution has spurred significant industry activity, with companies like Microsoft, Samsung, and Ontotext acquiring companies to advance knowledge graphs and semantic technologies. The rise of GraphRAG has reignited interest in knowledge graphs, positioning them as critical components in AI systems, moving beyond metadata management to become the semantic backbone for AI applications.
Greg Kroah-Hartman explains the Cyber Resilience Act for open source developers (https://go.theregister.com/feed/www.theregister.com/2025/09/30/cyber_reiliance_act_opinion_column/): The European Union's Cyber Resilience Act (CRA) mandates digital product producers to document vulnerabilities and respond to security incidents, impacting open-source software developers. Greg Kroah-Hartman, a Linux kernel maintainer, argues the CRA benefits open-source contributors by ensuring transparency and accountability. The CRA distinguishes between commercial entities and unpaid contributors, allowing individuals to publish software without legal risk. It applies to commercial manufacturers using open-source code, requiring Software Bill of Materials (SBOMs) and proactive incident management. While the CRA primarily affects EU-based manufacturers, its global reach extends to companies selling products accessible within the EU. The act is expected to increase demand for well-supported open-source projects, countering fears of reduced open-source use. Foundations and large projects are collaborating with the EU to simplify compliance through checklists and templates.
This security hole can crash billions of Chromium browsers, and Google hasn't patched it yet (https://go.theregister.com/feed/www.theregister.com/2025/10/29/brash_dos_attack_crashes_chromium/): Security researcher Jose Pino discovered a critical vulnerability in Chromium's Blink rendering engine, which can be exploited to crash many Chromium-based browsers within seconds. This flaw, known as Brash, affects over 3 billion users worldwide, as it can be used to crash browsers like Chrome, Microsoft Edge, Brave, and Vivaldi. Pino demonstrated the vulnerability by testing it on 11 major browsers, finding that it works on nine of them. The attack exploits an architectural flaw in Blink, specifically the absence of rate limiting on document.title API updates, allowing for millions of DOM mutations per second. This saturates the main thread, disrupting the event loop and causing the interface to collapse. Pino disclosed the issue to the Chromium security team, but did not receive a response.</description>
      <itunes:summary>Articles discussed
Is RAG Dead? The Rise of Context Engineering and Semantic Layers for Agentic AI (https://towardsdatascience.com/beyond-rag/): Retrieval-Augmented Generation (RAG) has evolved into a broader field known as context engineering, which is essential for agentic AI systems. Initially popularized by ChatGPT, RAG aimed to enhance LLM responses by retrieving relevant information at query time. However, its limitations in handling incorrect or irrelevant data led to the development of new techniques like re-rankers. GraphRAG, a variation of RAG using knowledge graphs, has gained prominence for its ability to reason over entities and relationships, improving LLM accuracy and explainability. This evolution has spurred significant industry activity, with companies like Microsoft, Samsung, and Ontotext acquiring companies to advance knowledge graphs and semantic technologies. The rise of GraphRAG has reignited interest in knowledge graphs, positioning them as critical components in AI systems, moving beyond metadata management to become the semantic backbone for AI applications.
Greg Kroah-Hartman explains the Cyber Resilience Act for open source developers (https://go.theregister.com/feed/www.theregister.com/2025/09/30/cyber_reiliance_act_opinion_column/): The European Union's Cyber Resilience Act (CRA) mandates digital product producers to document vulnerabilities and respond to security incidents, impacting open-source software developers. Greg Kroah-Hartman, a Linux kernel maintainer, argues the CRA benefits open-source contributors by ensuring transparency and accountability. The CRA distinguishes between commercial entities and unpaid contributors, allowing individuals to publish software without legal risk. It applies to commercial manufacturers using open-source code, requiring Software Bill of Materials (SBOMs) and proactive incident management. While the CRA primarily affects EU-based manufacturers, its global reach extends to companies selling products accessible within the EU. The act is expected to increase demand for well-supported open-source projects, countering fears of reduced open-source use. Foundations and large projects are collaborating with the EU to simplify compliance through checklists and templates.
This security hole can crash billions of Chromium browsers, and Google hasn't patched it yet (https://go.theregister.com/feed/www.theregister.com/2025/10/29/brash_dos_attack_crashes_chromium/): Security researcher Jose Pino discovered a critical vulnerability in Chromium's Blink rendering engine, which can be exploited to crash many Chromium-based browsers within seconds. This flaw, known as Brash, affects over 3 billion users worldwide, as it can be used to crash browsers like Chrome, Microsoft Edge, Brave, and Vivaldi. Pino demonstrated the vulnerability by testing it on 11 major browsers, finding that it works on nine of them. The attack exploits an architectural flaw in Blink, specifically the absence of rate limiting on document.title API updates, allowing for millions of DOM mutations per second. This saturates the main thread, disrupting the event loop and causing the interface to collapse. Pino disclosed the issue to the Chromium security team, but did not receive a response.</itunes:summary>
      <pubDate>Sat, 17 Jan 2026 15:58:59 GMT</pubDate>
      <enclosure url="https://raw.githubusercontent.com/Refugeek/podcast-feed/main/episodes-EScyber/20251102104026-is-rag-dead-the-rise-of-context-engineering-and-semantic-layers-for-agentic-ai-greg-kroah-hartman-explains-the-cyber-resilience-act-for-open-source-developers.mp3" type="audio/mpeg" length="32911244"/>
      <itunes:duration>1645</itunes:duration>
    </item>
    <item>
      <title>20251102131848-as-scientists-show-they-can-read-inner-speech-brain-implant-pioneers-fight-for-neural-data-privacy-access-rights-what-we-know-about-the-npm-supply-chain-attack</title>
      <description>Articles discussed
As scientists show they can read inner speech, brain implant ‘pioneers’ fight for neural data privacy, access rights (https://therecord.media/neural-data-privacy-brain-implants): J. Galen Buckwalter, a quadriplegic, underwent a craniotomy in 2024 to participate in a Caltech study. The study aims to decode how his neurons signal his hands to grasp objects. Buckwalter expressed concerns about the lack of explicit protections for his neural data privacy and access rights. With advancements in brain-computer interface (BCI) research, scientists can decode inner speech from neural data, raising privacy concerns. Buckwalter and others are forming the BCI Pioneers Coalition to advocate for ethical and legal norms for future research studies. They plan to draft guidelines for informed consent agreements to protect participants' data rights.
What We Know About the NPM Supply Chain Attack (https://www.trendmicro.com/en_us/research/25/i/npm-supply-chain-attack.html): A supply chain attack on Node Package Manager (NPM) compromised accounts and injected malicious code into JavaScript packages, affecting software ecosystems globally. Attackers used phishing to steal credentials, exploiting vulnerabilities in NPM maintainer accounts. The attack injected the Shai-hulud worm, which autonomously replicates and spreads through compromised packages, stealing cloud service tokens and deploying secret-scanning tools. This worm exploits post-install scripts to infect additional projects, making it difficult to contain. The attack impacted over 500 NPM packages, particularly those with high global download rates, threatening organizations' trust in open-source dependencies. Security measures like Trend Vision One™ can detect and block indicators of compromise, providing tailored threat hunting and intelligence reports.
When AI Agents Go Rogue: Agent Session Smuggling Attack in A2A Systems (https://unit42.paloaltonetworks.com/agent-session-smuggling-in-agent2agent-systems/): Agent session smuggling is a new attack vector that exploits stateful communication sessions in A2A systems. A malicious remote agent intercepts a legitimate client request and server response, injecting covert instructions into the conversation. This hidden manipulation can lead to context poisoning, data exfiltration, or unauthorized tool execution. The attack exploits the built-in trust relationships between agents in stateful protocols, making it particularly dangerous. Mitigation strategies include human-in-the-loop enforcement, remote agent verification, and context-grounding techniques.
Invisible npm malware pulls a disappearing act – then nicks your tokens (https://go.theregister.com/feed/www.theregister.com/2025/10/30/phantomraven_npm_malware/): PhantomRaven is a supply chain attack that has been active since at least August 2025. It involves 126 malicious packages published by multiple accounts. At least 86,000 downloads were recorded before the campaign was exposed this week, and more than 80 of the infected packages were still live at the time of disclosure. What sets PhantomRaven apart is its use of a new technique the researchers call Remote Dynamic Dependencies (RDD). Unlike typical npm malware, which relies on visible dependencies or post-install scripts, PhantomRaven packages initially appear empty – no dependencies, no suspicious code. But when a user installs them, the package fetches additional code from a remote server controlled by the attacker. The malicious payload is then executed locally, stealing data and exfiltrating it to the attacker's infrastructure. This makes the attack extremely difficult to detect using conventional methods. Security tools that rely on static analysis of package metadata or dependency graphs will see nothing out of the ordinary, because the harmful code doesn't exist in the registry itself. Instead, it's dynamically retrieved during installation, leaving no obvious trace in the source files. The stolen information includes npm and GitHub tokens, cloud credentials, SSH keys, and other sensitive environment</description>
      <itunes:summary>Articles discussed
As scientists show they can read inner speech, brain implant ‘pioneers’ fight for neural data privacy, access rights (https://therecord.media/neural-data-privacy-brain-implants): J. Galen Buckwalter, a quadriplegic, underwent a craniotomy in 2024 to participate in a Caltech study. The study aims to decode how his neurons signal his hands to grasp objects. Buckwalter expressed concerns about the lack of explicit protections for his neural data privacy and access rights. With advancements in brain-computer interface (BCI) research, scientists can decode inner speech from neural data, raising privacy concerns. Buckwalter and others are forming the BCI Pioneers Coalition to advocate for ethical and legal norms for future research studies. They plan to draft guidelines for informed consent agreements to protect participants' data rights.
What We Know About the NPM Supply Chain Attack (https://www.trendmicro.com/en_us/research/25/i/npm-supply-chain-attack.html): A supply chain attack on Node Package Manager (NPM) compromised accounts and injected malicious code into JavaScript packages, affecting software ecosystems globally. Attackers used phishing to steal credentials, exploiting vulnerabilities in NPM maintainer accounts. The attack injected the Shai-hulud worm, which autonomously replicates and spreads through compromised packages, stealing cloud service tokens and deploying secret-scanning tools. This worm exploits post-install scripts to infect additional projects, making it difficult to contain. The attack impacted over 500 NPM packages, particularly those with high global download rates, threatening organizations' trust in open-source dependencies. Security measures like Trend Vision One™ can detect and block indicators of compromise, providing tailored threat hunting and intelligence reports.
When AI Agents Go Rogue: Agent Session Smuggling Attack in A2A Systems (https://unit42.paloaltonetworks.com/agent-session-smuggling-in-agent2agent-systems/): Agent session smuggling is a new attack vector that exploits stateful communication sessions in A2A systems. A malicious remote agent intercepts a legitimate client request and server response, injecting covert instructions into the conversation. This hidden manipulation can lead to context poisoning, data exfiltration, or unauthorized tool execution. The attack exploits the built-in trust relationships between agents in stateful protocols, making it particularly dangerous. Mitigation strategies include human-in-the-loop enforcement, remote agent verification, and context-grounding techniques.
Invisible npm malware pulls a disappearing act – then nicks your tokens (https://go.theregister.com/feed/www.theregister.com/2025/10/30/phantomraven_npm_malware/): PhantomRaven is a supply chain attack that has been active since at least August 2025. It involves 126 malicious packages published by multiple accounts. At least 86,000 downloads were recorded before the campaign was exposed this week, and more than 80 of the infected packages were still live at the time of disclosure. What sets PhantomRaven apart is its use of a new technique the researchers call Remote Dynamic Dependencies (RDD). Unlike typical npm malware, which relies on visible dependencies or post-install scripts, PhantomRaven packages initially appear empty – no dependencies, no suspicious code. But when a user installs them, the package fetches additional code from a remote server controlled by the attacker. The malicious payload is then executed locally, stealing data and exfiltrating it to the attacker's infrastructure. This makes the attack extremely difficult to detect using conventional methods. Security tools that rely on static analysis of package metadata or dependency graphs will see nothing out of the ordinary, because the harmful code doesn't exist in the registry itself. Instead, it's dynamically retrieved during installation, leaving no obvious trace in the source files. The stolen information includes npm and GitHub tokens, cloud credentials, SSH keys, and other sensitive environment</itunes:summary>
      <description>Articles discussed
As scientists show they can read inner speech, brain implant ‘pioneers’ fight for neural data privacy, access rights (https://therecord.media/neural-data-privacy-brain-implants): J. Galen Buckwalter, a quadriplegic, underwent a craniotomy in 2024 to participate in a Caltech study. The study aims to decode how his neurons signal his hands to grasp objects. Buckwalter expressed concerns about the lack of explicit protections for his neural data privacy and access rights. With advancements in brain-computer interface (BCI) research, scientists can decode inner speech from neural data, raising privacy concerns. Buckwalter and others are forming the BCI Pioneers Coalition to advocate for ethical and legal norms for future research studies. They plan to draft guidelines for informed consent agreements to protect participants' data rights.
What We Know About the NPM Supply Chain Attack (https://www.trendmicro.com/en_us/research/25/i/npm-supply-chain-attack.html): A supply chain attack on Node Package Manager (NPM) compromised accounts and injected malicious code into JavaScript packages, affecting software ecosystems globally. Attackers used phishing to steal credentials, exploiting vulnerabilities in NPM maintainer accounts. The attack injected the Shai-hulud worm, which autonomously replicates and spreads through compromised packages, stealing cloud service tokens and deploying secret-scanning tools. This worm exploits post-install scripts to infect additional projects, making it difficult to contain. The attack impacted over 500 NPM packages, particularly those with high global download rates, threatening organizations' trust in open-source dependencies. Security measures like Trend Vision One™ can detect and block indicators of compromise, providing tailored threat hunting and intelligence reports.
When AI Agents Go Rogue: Agent Session Smuggling Attack in A2A Systems (https://unit42.paloaltonetworks.com/agent-session-smuggling-in-agent2agent-systems/): Agent session smuggling is a new attack vector that exploits stateful communication sessions in A2A systems. A malicious remote agent intercepts a legitimate client request and server response, injecting covert instructions into the conversation. This hidden manipulation can lead to context poisoning, data exfiltration, or unauthorized tool execution. The attack exploits the built-in trust relationships between agents in stateful protocols, making it particularly dangerous. Mitigation strategies include human-in-the-loop enforcement, remote agent verification, and context-grounding techniques.
Invisible npm malware pulls a disappearing act – then nicks your tokens (https://go.theregister.com/feed/www.theregister.com/2025/10/30/phantomraven_npm_malware/): PhantomRaven is a supply chain attack that has been active since at least August 2025. It involves 126 malicious packages published by multiple accounts. At least 86,000 downloads were recorded before the campaign was exposed this week, and more than 80 of the infected packages were still live at the time of disclosure. What sets PhantomRaven apart is its use of a new technique the researchers call Remote Dynamic Dependencies (RDD). Unlike typical npm malware, which relies on visible dependencies or post-install scripts, PhantomRaven packages initially appear empty – no dependencies, no suspicious code. But when a user installs them, the package fetches additional code from a remote server controlled by the attacker. The malicious payload is then executed locally, stealing data and exfiltrating it to the attacker's infrastructure. This makes the attack extremely difficult to detect using conventional methods. Security tools that rely on static analysis of package metadata or dependency graphs will see nothing out of the ordinary, because the harmful code doesn't exist in the registry itself. Instead, it's dynamically retrieved during installation, leaving no obvious trace in the source files. The stolen information includes npm and GitHub tokens, cloud credentials, SSH keys, and other sensitive environment</description>
      <itunes:summary>Articles discussed
As scientists show they can read inner speech, brain implant ‘pioneers’ fight for neural data privacy, access rights (https://therecord.media/neural-data-privacy-brain-implants): J. Galen Buckwalter, a quadriplegic, underwent a craniotomy in 2024 to participate in a Caltech study. The study aims to decode how his neurons signal his hands to grasp objects. Buckwalter expressed concerns about the lack of explicit protections for his neural data privacy and access rights. With advancements in brain-computer interface (BCI) research, scientists can decode inner speech from neural data, raising privacy concerns. Buckwalter and others are forming the BCI Pioneers Coalition to advocate for ethical and legal norms for future research studies. They plan to draft guidelines for informed consent agreements to protect participants' data rights.
What We Know About the NPM Supply Chain Attack (https://www.trendmicro.com/en_us/research/25/i/npm-supply-chain-attack.html): A supply chain attack on Node Package Manager (NPM) compromised accounts and injected malicious code into JavaScript packages, affecting software ecosystems globally. Attackers used phishing to steal credentials, exploiting vulnerabilities in NPM maintainer accounts. The attack injected the Shai-hulud worm, which autonomously replicates and spreads through compromised packages, stealing cloud service tokens and deploying secret-scanning tools. This worm exploits post-install scripts to infect additional projects, making it difficult to contain. The attack impacted over 500 NPM packages, particularly those with high global download rates, threatening organizations' trust in open-source dependencies. Security measures like Trend Vision One™ can detect and block indicators of compromise, providing tailored threat hunting and intelligence reports.
When AI Agents Go Rogue: Agent Session Smuggling Attack in A2A Systems (https://unit42.paloaltonetworks.com/agent-session-smuggling-in-agent2agent-systems/): Agent session smuggling is a new attack vector that exploits stateful communication sessions in A2A systems. A malicious remote agent intercepts a legitimate client request and server response, injecting covert instructions into the conversation. This hidden manipulation can lead to context poisoning, data exfiltration, or unauthorized tool execution. The attack exploits the built-in trust relationships between agents in stateful protocols, making it particularly dangerous. Mitigation strategies include human-in-the-loop enforcement, remote agent verification, and context-grounding techniques.
Invisible npm malware pulls a disappearing act – then nicks your tokens (https://go.theregister.com/feed/www.theregister.com/2025/10/30/phantomraven_npm_malware/): PhantomRaven is a supply chain attack that has been active since at least August 2025. It involves 126 malicious packages published by multiple accounts. At least 86,000 downloads were recorded before the campaign was exposed this week, and more than 80 of the infected packages were still live at the time of disclosure. What sets PhantomRaven apart is its use of a new technique the researchers call Remote Dynamic Dependencies (RDD). Unlike typical npm malware, which relies on visible dependencies or post-install scripts, PhantomRaven packages initially appear empty – no dependencies, no suspicious code. But when a user installs them, the package fetches additional code from a remote server controlled by the attacker. The malicious payload is then executed locally, stealing data and exfiltrating it to the attacker's infrastructure. This makes the attack extremely difficult to detect using conventional methods. Security tools that rely on static analysis of package metadata or dependency graphs will see nothing out of the ordinary, because the harmful code doesn't exist in the registry itself. Instead, it's dynamically retrieved during installation, leaving no obvious trace in the source files. The stolen information includes npm and GitHub tokens, cloud credentials, SSH keys, and other sensitive environment</itunes:summary>
      <pubDate>Sat, 17 Jan 2026 15:58:59 GMT</pubDate>
      <enclosure url="https://raw.githubusercontent.com/Refugeek/podcast-feed/main/episodes-EScyber/20251102131848-as-scientists-show-they-can-read-inner-speech-brain-implant-pioneers-fight-for-neural-data-privacy-access-rights-what-we-know-about-the-npm-supply-chain-attack.mp3" type="audio/mpeg" length="32936684"/>
      <itunes:duration>1646</itunes:duration>
    </item>
    <item>
      <title>20251104083555-self-propagating-supply-chain-attack-hits-187-npm-packages-enisa-will-operate-the-eu-cybersecurity-reserve-what-this-means-for-managed-security-service-providers</title>
      <description>Articles discussed
Self-propagating supply chain attack hits 187 npm packages (https://www.bleepingcomputer.com/news/security/self-propagating-supply-chain-attack-hits-187-npm-packages/): A coordinated worm-style campaign dubbed 'Shai-Hulud' has compromised at least 187 npm packages, starting with the @ctrl/tinycolor package. This attack uses a self-propagating payload to infect other packages, exploiting vulnerabilities in package management systems. Researchers identified the attack's origin in npmjs.com and linked it to CrowdStrike's npm namespace. The malware employs TruffleHog, a legitimate secret scanner, to exfiltrate sensitive data like API keys and credentials. This incident follows high-profile supply chain attacks, highlighting the fragility of modern software supply chains and the need for enhanced security measures.
ENISA Will Operate the EU Cybersecurity Reserve. What This Means for Managed Security Service Providers (https://www.tripwire.com/state-of-security/enisa-operate-eu-cybersecurity-reserve-managed-security-service): The European Union is building a new line of defense. On August 26, 2025, the European Commission and the EU Agency for Cybersecurity (ENISA) signed a contribution agreement that hands ENISA the keys to the EU Cybersecurity Reserve. The deal comes with funding: €36 million over three years. ENISA's mission is straightforward, if not simple. It will administer, operate, and monitor the bloc’s emergency cyber response capabilities. The Cybersecurity Reserve is designed as a pool of pre-contracted services from trusted managed security providers. When a major incident hits, Member States, EU institutions, or even associated countries can call on those services. The Reserve offers a way to respond faster and with more weight than national resources alone. How the Cybersecurity Reserve Works ENISA will run the procurement process and will sign contracts with managed security service providers (MSSPs). These providers, vetted through public tenders, will stand ready to deploy incident response teams, forensic expertise, and recovery services. The aim is efficiency. Money spent should strengthen resilience, whether or not the alarm bells ring. The Money and the Mandate Contribution agreements like this aren’t new. ENISA has received similar top-up funding for special projects: the Cybersecurity Support Action, the Single Reporting
Defeating KASLR by Doing Nothing at All (https://googleprojectzero.blogspot.com/2025/11/defeating-kaslr-by-doing-nothing-at-all.html): Researchers Jann and Seth Jenkins discovered a vulnerability in the Linux kernel's linear mapping, which allows arbitrary reads without KASLR. They found that Pixel phones' bootloaders compress the kernel at a static physical address, making it possible to calculate a static kernel virtual address for any kernel .data entry. This enables attackers to access kernel symbols at predictable addresses, even with KASLR enabled. The impact is significant, as it allows for arbitrary read-write access to kernel memory, potentially bypassing defenses. This finding highlights the importance of addressing non-randomization in kernel memory management to prevent such vulnerabilities.
SesameOp: Novel backdoor uses OpenAI Assistants API for command and control (https://www.microsoft.com/en-us/security/blog/2025/11/03/sesameop-novel-backdoor-uses-openai-assistants-api-for-command-and-control/): Researchers from Microsoft’s Incident Response – Detection and Response Team (DART) discovered a sophisticated backdoor called SesameOp in July 2025. This backdoor exploits the OpenAI Assistants API for command-and-control (C2) communications, enabling stealthy orchestration of malicious activities within compromised environments. SesameOp leverages Microsoft Visual Studio utilities, specifically .NET AppDomainManager injection, to evade detection and maintain persistence. The backdoor employs advanced obfuscation techniques, including payload compression and layered encryption, to secure communications. Researchers identified the infection chain, which includes a loader (Netapi64.dll) and a NET-based backdoor (OpenAIAgent.Netapi64), demonstrating threat actors' adaptability in exploiting emerging technologies. Microsoft and OpenAI collaborated to mitigate the threat by disabling an API key associated with the actor. SesameOp underscores the evolving tactics of cyber adversaries and the importance of continuous vigilance against novel attack vectors.</description>
      <itunes:summary>Articles discussed
Self-propagating supply chain attack hits 187 npm packages (https://www.bleepingcomputer.com/news/security/self-propagating-supply-chain-attack-hits-187-npm-packages/): A coordinated worm-style campaign dubbed 'Shai-Hulud' has compromised at least 187 npm packages, starting with the @ctrl/tinycolor package. This attack uses a self-propagating payload to infect other packages, exploiting vulnerabilities in package management systems. Researchers identified the attack's origin in npmjs.com and linked it to CrowdStrike's npm namespace. The malware employs TruffleHog, a legitimate secret scanner, to exfiltrate sensitive data like API keys and credentials. This incident follows high-profile supply chain attacks, highlighting the fragility of modern software supply chains and the need for enhanced security measures.
ENISA Will Operate the EU Cybersecurity Reserve. What This Means for Managed Security Service Providers (https://www.tripwire.com/state-of-security/enisa-operate-eu-cybersecurity-reserve-managed-security-service): The European Union is building a new line of defense. On August 26, 2025, the European Commission and the EU Agency for Cybersecurity (ENISA) signed a contribution agreement that hands ENISA the keys to the EU Cybersecurity Reserve. The deal comes with funding: €36 million over three years. ENISA's mission is straightforward, if not simple. It will administer, operate, and monitor the bloc’s emergency cyber response capabilities. The Cybersecurity Reserve is designed as a pool of pre-contracted services from trusted managed security providers. When a major incident hits, Member States, EU institutions, or even associated countries can call on those services. The Reserve offers a way to respond faster and with more weight than national resources alone. How the Cybersecurity Reserve Works ENISA will run the procurement process and will sign contracts with managed security service providers (MSSPs). These providers, vetted through public tenders, will stand ready to deploy incident response teams, forensic expertise, and recovery services. The aim is efficiency. Money spent should strengthen resilience, whether or not the alarm bells ring. The Money and the Mandate Contribution agreements like this aren’t new. ENISA has received similar top-up funding for special projects: the Cybersecurity Support Action, the Single Reporting
Defeating KASLR by Doing Nothing at All (https://googleprojectzero.blogspot.com/2025/11/defeating-kaslr-by-doing-nothing-at-all.html): Researchers Jann and Seth Jenkins discovered a vulnerability in the Linux kernel's linear mapping, which allows arbitrary reads without KASLR. They found that Pixel phones' bootloaders compress the kernel at a static physical address, making it possible to calculate a static kernel virtual address for any kernel .data entry. This enables attackers to access kernel symbols at predictable addresses, even with KASLR enabled. The impact is significant, as it allows for arbitrary read-write access to kernel memory, potentially bypassing defenses. This finding highlights the importance of addressing non-randomization in kernel memory management to prevent such vulnerabilities.
SesameOp: Novel backdoor uses OpenAI Assistants API for command and control (https://www.microsoft.com/en-us/security/blog/2025/11/03/sesameop-novel-backdoor-uses-openai-assistants-api-for-command-and-control/): Researchers from Microsoft’s Incident Response – Detection and Response Team (DART) discovered a sophisticated backdoor called SesameOp in July 2025. This backdoor exploits the OpenAI Assistants API for command-and-control (C2) communications, enabling stealthy orchestration of malicious activities within compromised environments. SesameOp leverages Microsoft Visual Studio utilities, specifically .NET AppDomainManager injection, to evade detection and maintain persistence. The backdoor employs advanced obfuscation techniques, including payload compression and layered encryption, to secure communications. Researchers identified the infection chain, which includes a loader (Netapi64.dll) and a NET-based backdoor (OpenAIAgent.Netapi64), demonstrating threat actors' adaptability in exploiting emerging technologies. Microsoft and OpenAI collaborated to mitigate the threat by disabling an API key associated with the actor. SesameOp underscores the evolving tactics of cyber adversaries and the importance of continuous vigilance against novel attack vectors.</itunes:summary>
      <description>Articles discussed
Self-propagating supply chain attack hits 187 npm packages (https://www.bleepingcomputer.com/news/security/self-propagating-supply-chain-attack-hits-187-npm-packages/): A coordinated worm-style campaign dubbed 'Shai-Hulud' has compromised at least 187 npm packages, starting with the @ctrl/tinycolor package. This attack uses a self-propagating payload to infect other packages, exploiting vulnerabilities in package management systems. Researchers identified the attack's origin in npmjs.com and linked it to CrowdStrike's npm namespace. The malware employs TruffleHog, a legitimate secret scanner, to exfiltrate sensitive data like API keys and credentials. This incident follows high-profile supply chain attacks, highlighting the fragility of modern software supply chains and the need for enhanced security measures.
ENISA Will Operate the EU Cybersecurity Reserve. What This Means for Managed Security Service Providers (https://www.tripwire.com/state-of-security/enisa-operate-eu-cybersecurity-reserve-managed-security-service): The European Union is building a new line of defense. On August 26, 2025, the European Commission and the EU Agency for Cybersecurity (ENISA) signed a contribution agreement that hands ENISA the keys to the EU Cybersecurity Reserve. The deal comes with funding: €36 million over three years. ENISA's mission is straightforward, if not simple. It will administer, operate, and monitor the bloc’s emergency cyber response capabilities. The Cybersecurity Reserve is designed as a pool of pre-contracted services from trusted managed security providers. When a major incident hits, Member States, EU institutions, or even associated countries can call on those services. The Reserve offers a way to respond faster and with more weight than national resources alone. How the Cybersecurity Reserve Works ENISA will run the procurement process and will sign contracts with managed security service providers (MSSPs). These providers, vetted through public tenders, will stand ready to deploy incident response teams, forensic expertise, and recovery services. The aim is efficiency. Money spent should strengthen resilience, whether or not the alarm bells ring. The Money and the Mandate Contribution agreements like this aren’t new. ENISA has received similar top-up funding for special projects: the Cybersecurity Support Action, the Single Reporting
Defeating KASLR by Doing Nothing at All (https://googleprojectzero.blogspot.com/2025/11/defeating-kaslr-by-doing-nothing-at-all.html): Researchers Jann and Seth Jenkins discovered a vulnerability in the Linux kernel's linear mapping, which allows arbitrary reads without KASLR. They found that Pixel phones' bootloaders compress the kernel at a static physical address, making it possible to calculate a static kernel virtual address for any kernel .data entry. This enables attackers to access kernel symbols at predictable addresses, even with KASLR enabled. The impact is significant, as it allows for arbitrary read-write access to kernel memory, potentially bypassing defenses. This finding highlights the importance of addressing non-randomization in kernel memory management to prevent such vulnerabilities.
SesameOp: Novel backdoor uses OpenAI Assistants API for command and control (https://www.microsoft.com/en-us/security/blog/2025/11/03/sesameop-novel-backdoor-uses-openai-assistants-api-for-command-and-control/): Researchers from Microsoft’s Incident Response – Detection and Response Team (DART) discovered a sophisticated backdoor called SesameOp in July 2025. This backdoor exploits the OpenAI Assistants API for command-and-control (C2) communications, enabling stealthy orchestration of malicious activities within compromised environments. SesameOp leverages Microsoft Visual Studio utilities, specifically .NET AppDomainManager injection, to evade detection and maintain persistence. The backdoor employs advanced obfuscation techniques, including payload compression and layered encryption, to secure communications. Researchers identified the infection chain, which includes a loader (Netapi64.dll) and a NET-based backdoor (OpenAIAgent.Netapi64), demonstrating threat actors' adaptability in exploiting emerging technologies. Microsoft and OpenAI collaborated to mitigate the threat by disabling an API key associated with the actor. SesameOp underscores the evolving tactics of cyber adversaries and the importance of continuous vigilance against novel attack vectors.</description>
      <itunes:summary>Articles discussed
Self-propagating supply chain attack hits 187 npm packages (https://www.bleepingcomputer.com/news/security/self-propagating-supply-chain-attack-hits-187-npm-packages/): A coordinated worm-style campaign dubbed 'Shai-Hulud' has compromised at least 187 npm packages, starting with the @ctrl/tinycolor package. This attack uses a self-propagating payload to infect other packages, exploiting vulnerabilities in package management systems. Researchers identified the attack's origin in npmjs.com and linked it to CrowdStrike's npm namespace. The malware employs TruffleHog, a legitimate secret scanner, to exfiltrate sensitive data like API keys and credentials. This incident follows high-profile supply chain attacks, highlighting the fragility of modern software supply chains and the need for enhanced security measures.
ENISA Will Operate the EU Cybersecurity Reserve. What This Means for Managed Security Service Providers (https://www.tripwire.com/state-of-security/enisa-operate-eu-cybersecurity-reserve-managed-security-service): The European Union is building a new line of defense. On August 26, 2025, the European Commission and the EU Agency for Cybersecurity (ENISA) signed a contribution agreement that hands ENISA the keys to the EU Cybersecurity Reserve. The deal comes with funding: €36 million over three years. ENISA's mission is straightforward, if not simple. It will administer, operate, and monitor the bloc’s emergency cyber response capabilities. The Cybersecurity Reserve is designed as a pool of pre-contracted services from trusted managed security providers. When a major incident hits, Member States, EU institutions, or even associated countries can call on those services. The Reserve offers a way to respond faster and with more weight than national resources alone. How the Cybersecurity Reserve Works ENISA will run the procurement process and will sign contracts with managed security service providers (MSSPs). These providers, vetted through public tenders, will stand ready to deploy incident response teams, forensic expertise, and recovery services. The aim is efficiency. Money spent should strengthen resilience, whether or not the alarm bells ring. The Money and the Mandate Contribution agreements like this aren’t new. ENISA has received similar top-up funding for special projects: the Cybersecurity Support Action, the Single Reporting
Defeating KASLR by Doing Nothing at All (https://googleprojectzero.blogspot.com/2025/11/defeating-kaslr-by-doing-nothing-at-all.html): Researchers Jann and Seth Jenkins discovered a vulnerability in the Linux kernel's linear mapping, which allows arbitrary reads without KASLR. They found that Pixel phones' bootloaders compress the kernel at a static physical address, making it possible to calculate a static kernel virtual address for any kernel .data entry. This enables attackers to access kernel symbols at predictable addresses, even with KASLR enabled. The impact is significant, as it allows for arbitrary read-write access to kernel memory, potentially bypassing defenses. This finding highlights the importance of addressing non-randomization in kernel memory management to prevent such vulnerabilities.
SesameOp: Novel backdoor uses OpenAI Assistants API for command and control (https://www.microsoft.com/en-us/security/blog/2025/11/03/sesameop-novel-backdoor-uses-openai-assistants-api-for-command-and-control/): Researchers from Microsoft’s Incident Response – Detection and Response Team (DART) discovered a sophisticated backdoor called SesameOp in July 2025. This backdoor exploits the OpenAI Assistants API for command-and-control (C2) communications, enabling stealthy orchestration of malicious activities within compromised environments. SesameOp leverages Microsoft Visual Studio utilities, specifically .NET AppDomainManager injection, to evade detection and maintain persistence. The backdoor employs advanced obfuscation techniques, including payload compression and layered encryption, to secure communications. Researchers identified the infection chain, which includes a loader (Netapi64.dll) and a NET-based backdoor (OpenAIAgent.Netapi64), demonstrating threat actors' adaptability in exploiting emerging technologies. Microsoft and OpenAI collaborated to mitigate the threat by disabling an API key associated with the actor. SesameOp underscores the evolving tactics of cyber adversaries and the importance of continuous vigilance against novel attack vectors.</itunes:summary>
      <pubDate>Sat, 17 Jan 2026 15:58:59 GMT</pubDate>
      <enclosure url="https://raw.githubusercontent.com/Refugeek/podcast-feed/main/episodes-EScyber/20251104083555-self-propagating-supply-chain-attack-hits-187-npm-packages-enisa-will-operate-the-eu-cybersecurity-reserve-what-this-means-for-managed-security-service-providers.mp3" type="audio/mpeg" length="33006284"/>
      <itunes:duration>1650</itunes:duration>
    </item>
    <item>
      <title>20251104153837-why-context-is-the-new-currency-in-ai-from-rag-to-context-engineering-malware-injected-into-code-packages-that-get-2-billion-downloads-each-week</title>
      <description>Articles discussed
Why Context Is the New Currency in AI: From RAG to Context Engineering (https://towardsdatascience.com/why-context-is-the-new-currency-in-ai-from-rag-to-context-engineering/): The article discusses the growing importance of context in AI systems, highlighting a shift from traditional retrieval-based approaches to context engineering. Key actors include AI developers and researchers who are rethinking how information is managed and integrated within AI workflows. The method involves a comprehensive approach that considers user intent, instruction layering, and external data, moving beyond static buffers to dynamic, narrative-based structures. This approach aims to improve decision-making by ensuring information is delivered in a sequence that supports effective reasoning. The impact of this shift is significant, as it addresses the limitations of traditional systems that often fail to provide meaningful context, leading to inefficiencies and reduced utility. The novelty lies in the emphasis on compositional context rather than additive information, which can mitigate issues like &quot;attention dilution.&quot;
Malware Injected Into Code Packages That Get 2 Billion+ Downloads Each Week (https://www.techrepublic.com/article/news-malware-intercepts-crypto-injected-npm-package/): In a significant cybersecurity breach, unidentified hackers compromised 18 npm packages, which collectively handle over two billion downloads weekly. These packages, including popular ones like 'ansi-styles' and 'debug', were targeted through a phishing campaign that exploited a compromised npm package maintainer account. The malicious code injected into these packages intercepts crypto and Web3 activity, manipulates wallet interactions, and redirects funds to attacker-controlled accounts. This breach underscores the importance of security measures beyond one's own codebase, highlighting the risks associated with software dependencies. The compromised packages have since been removed by the npm registry, but the incident serves as a reminder of the evolving tactics cybercriminals employ.
OpenAI API moonlights as malware HQ in Microsoft’s latest discovery (https://go.theregister.com/feed/www.theregister.com/2025/11/04/openai_api_moonlights_as_malware/): Redmond uncovers SesameOp, a backdoor hiding its tracks by using OpenAI’s Assistants API as a command channel Hackers have found a new use for OpenAI's Assistants API – not to write poems or code, but to secretly control malware.…
Gotta fly: Lazarus targets the UAV sector (https://www.welivesecurity.com/en/eset-research/gotta-fly-lazarus-targets-uav-sector/): ESET Research has observed a new instance of the Operation DreamJob cyberespionage campaign conducted by Lazarus, a North Korea-aligned APT group. The campaign targeted several European companies active in the defense industry, with a focus on UAV technology, suggesting a link to North Korea’s drone program. The primary goal of the attackers was likely the theft of proprietary information and manufacturing know-how. The social-engineering technique used for initial access involved trojanizing open-source projects from GitHub and deploying ScoringMathTea, a RAT that offers full control over compromised machines. The introduction of new libraries designed for DLL proxying and the selection of new open-source projects for trojanization represent significant evolutions in the group’s toolset. This activity is attributed to Lazarus campaigns related to Operation DreamJob, based on the observed tactics and targets.</description>
      <itunes:summary>Articles discussed
Why Context Is the New Currency in AI: From RAG to Context Engineering (https://towardsdatascience.com/why-context-is-the-new-currency-in-ai-from-rag-to-context-engineering/): The article discusses the growing importance of context in AI systems, highlighting a shift from traditional retrieval-based approaches to context engineering. Key actors include AI developers and researchers who are rethinking how information is managed and integrated within AI workflows. The method involves a comprehensive approach that considers user intent, instruction layering, and external data, moving beyond static buffers to dynamic, narrative-based structures. This approach aims to improve decision-making by ensuring information is delivered in a sequence that supports effective reasoning. The impact of this shift is significant, as it addresses the limitations of traditional systems that often fail to provide meaningful context, leading to inefficiencies and reduced utility. The novelty lies in the emphasis on compositional context rather than additive information, which can mitigate issues like &quot;attention dilution.&quot;
Malware Injected Into Code Packages That Get 2 Billion+ Downloads Each Week (https://www.techrepublic.com/article/news-malware-intercepts-crypto-injected-npm-package/): In a significant cybersecurity breach, unidentified hackers compromised 18 npm packages, which collectively handle over two billion downloads weekly. These packages, including popular ones like 'ansi-styles' and 'debug', were targeted through a phishing campaign that exploited a compromised npm package maintainer account. The malicious code injected into these packages intercepts crypto and Web3 activity, manipulates wallet interactions, and redirects funds to attacker-controlled accounts. This breach underscores the importance of security measures beyond one's own codebase, highlighting the risks associated with software dependencies. The compromised packages have since been removed by the npm registry, but the incident serves as a reminder of the evolving tactics cybercriminals employ.
OpenAI API moonlights as malware HQ in Microsoft’s latest discovery (https://go.theregister.com/feed/www.theregister.com/2025/11/04/openai_api_moonlights_as_malware/): Redmond uncovers SesameOp, a backdoor hiding its tracks by using OpenAI’s Assistants API as a command channel Hackers have found a new use for OpenAI's Assistants API – not to write poems or code, but to secretly control malware.…
Gotta fly: Lazarus targets the UAV sector (https://www.welivesecurity.com/en/eset-research/gotta-fly-lazarus-targets-uav-sector/): ESET Research has observed a new instance of the Operation DreamJob cyberespionage campaign conducted by Lazarus, a North Korea-aligned APT group. The campaign targeted several European companies active in the defense industry, with a focus on UAV technology, suggesting a link to North Korea’s drone program. The primary goal of the attackers was likely the theft of proprietary information and manufacturing know-how. The social-engineering technique used for initial access involved trojanizing open-source projects from GitHub and deploying ScoringMathTea, a RAT that offers full control over compromised machines. The introduction of new libraries designed for DLL proxying and the selection of new open-source projects for trojanization represent significant evolutions in the group’s toolset. This activity is attributed to Lazarus campaigns related to Operation DreamJob, based on the observed tactics and targets.</itunes:summary>
      <description>Articles discussed
Why Context Is the New Currency in AI: From RAG to Context Engineering (https://towardsdatascience.com/why-context-is-the-new-currency-in-ai-from-rag-to-context-engineering/): The article discusses the growing importance of context in AI systems, highlighting a shift from traditional retrieval-based approaches to context engineering. Key actors include AI developers and researchers who are rethinking how information is managed and integrated within AI workflows. The method involves a comprehensive approach that considers user intent, instruction layering, and external data, moving beyond static buffers to dynamic, narrative-based structures. This approach aims to improve decision-making by ensuring information is delivered in a sequence that supports effective reasoning. The impact of this shift is significant, as it addresses the limitations of traditional systems that often fail to provide meaningful context, leading to inefficiencies and reduced utility. The novelty lies in the emphasis on compositional context rather than additive information, which can mitigate issues like &quot;attention dilution.&quot;
Malware Injected Into Code Packages That Get 2 Billion+ Downloads Each Week (https://www.techrepublic.com/article/news-malware-intercepts-crypto-injected-npm-package/): In a significant cybersecurity breach, unidentified hackers compromised 18 npm packages, which collectively handle over two billion downloads weekly. These packages, including popular ones like 'ansi-styles' and 'debug', were targeted through a phishing campaign that exploited a compromised npm package maintainer account. The malicious code injected into these packages intercepts crypto and Web3 activity, manipulates wallet interactions, and redirects funds to attacker-controlled accounts. This breach underscores the importance of security measures beyond one's own codebase, highlighting the risks associated with software dependencies. The compromised packages have since been removed by the npm registry, but the incident serves as a reminder of the evolving tactics cybercriminals employ.
OpenAI API moonlights as malware HQ in Microsoft’s latest discovery (https://go.theregister.com/feed/www.theregister.com/2025/11/04/openai_api_moonlights_as_malware/): Redmond uncovers SesameOp, a backdoor hiding its tracks by using OpenAI’s Assistants API as a command channel Hackers have found a new use for OpenAI's Assistants API – not to write poems or code, but to secretly control malware.…
Gotta fly: Lazarus targets the UAV sector (https://www.welivesecurity.com/en/eset-research/gotta-fly-lazarus-targets-uav-sector/): ESET Research has observed a new instance of the Operation DreamJob cyberespionage campaign conducted by Lazarus, a North Korea-aligned APT group. The campaign targeted several European companies active in the defense industry, with a focus on UAV technology, suggesting a link to North Korea’s drone program. The primary goal of the attackers was likely the theft of proprietary information and manufacturing know-how. The social-engineering technique used for initial access involved trojanizing open-source projects from GitHub and deploying ScoringMathTea, a RAT that offers full control over compromised machines. The introduction of new libraries designed for DLL proxying and the selection of new open-source projects for trojanization represent significant evolutions in the group’s toolset. This activity is attributed to Lazarus campaigns related to Operation DreamJob, based on the observed tactics and targets.</description>
      <itunes:summary>Articles discussed
Why Context Is the New Currency in AI: From RAG to Context Engineering (https://towardsdatascience.com/why-context-is-the-new-currency-in-ai-from-rag-to-context-engineering/): The article discusses the growing importance of context in AI systems, highlighting a shift from traditional retrieval-based approaches to context engineering. Key actors include AI developers and researchers who are rethinking how information is managed and integrated within AI workflows. The method involves a comprehensive approach that considers user intent, instruction layering, and external data, moving beyond static buffers to dynamic, narrative-based structures. This approach aims to improve decision-making by ensuring information is delivered in a sequence that supports effective reasoning. The impact of this shift is significant, as it addresses the limitations of traditional systems that often fail to provide meaningful context, leading to inefficiencies and reduced utility. The novelty lies in the emphasis on compositional context rather than additive information, which can mitigate issues like &quot;attention dilution.&quot;
Malware Injected Into Code Packages That Get 2 Billion+ Downloads Each Week (https://www.techrepublic.com/article/news-malware-intercepts-crypto-injected-npm-package/): In a significant cybersecurity breach, unidentified hackers compromised 18 npm packages, which collectively handle over two billion downloads weekly. These packages, including popular ones like 'ansi-styles' and 'debug', were targeted through a phishing campaign that exploited a compromised npm package maintainer account. The malicious code injected into these packages intercepts crypto and Web3 activity, manipulates wallet interactions, and redirects funds to attacker-controlled accounts. This breach underscores the importance of security measures beyond one's own codebase, highlighting the risks associated with software dependencies. The compromised packages have since been removed by the npm registry, but the incident serves as a reminder of the evolving tactics cybercriminals employ.
OpenAI API moonlights as malware HQ in Microsoft’s latest discovery (https://go.theregister.com/feed/www.theregister.com/2025/11/04/openai_api_moonlights_as_malware/): Redmond uncovers SesameOp, a backdoor hiding its tracks by using OpenAI’s Assistants API as a command channel Hackers have found a new use for OpenAI's Assistants API – not to write poems or code, but to secretly control malware.…
Gotta fly: Lazarus targets the UAV sector (https://www.welivesecurity.com/en/eset-research/gotta-fly-lazarus-targets-uav-sector/): ESET Research has observed a new instance of the Operation DreamJob cyberespionage campaign conducted by Lazarus, a North Korea-aligned APT group. The campaign targeted several European companies active in the defense industry, with a focus on UAV technology, suggesting a link to North Korea’s drone program. The primary goal of the attackers was likely the theft of proprietary information and manufacturing know-how. The social-engineering technique used for initial access involved trojanizing open-source projects from GitHub and deploying ScoringMathTea, a RAT that offers full control over compromised machines. The introduction of new libraries designed for DLL proxying and the selection of new open-source projects for trojanization represent significant evolutions in the group’s toolset. This activity is attributed to Lazarus campaigns related to Operation DreamJob, based on the observed tactics and targets.</itunes:summary>
      <pubDate>Sat, 17 Jan 2026 15:58:59 GMT</pubDate>
      <enclosure url="https://raw.githubusercontent.com/Refugeek/podcast-feed/main/episodes-EScyber/20251104153837-why-context-is-the-new-currency-in-ai-from-rag-to-context-engineering-malware-injected-into-code-packages-that-get-2-billion-downloads-each-week.mp3" type="audio/mpeg" length="38763884"/>
      <itunes:duration>1938</itunes:duration>
    </item>
    <item>
      <title>20251105193822-how-sakana-ais-new-evolutionary-algorithm-builds-powerful-ai-models-without-expensive-retraining-opencuas-open-source-computer-use-agents-rival-proprietary-models-from-openai-and-anthropic</title>
      <description>Articles discussed
How Sakana AI’s new evolutionary algorithm builds powerful AI models without expensive retraining (https://venturebeat.com/ai/how-sakana-ais-new-evolutionary-algorithm-builds-powerful-ai-models-without-expensive-retraining/): Sakana AI has developed a new model merging technique called M2N2, which enables the creation of powerful multi-skilled agents without the high cost and data needs of retraining. This method allows for the development of AI models that can perform multiple tasks efficiently. The technique is particularly useful in fields requiring diverse skill sets, such as healthcare and customer service. M2N2 leverages evolutionary algorithms to optimize model performance, making it a novel approach in AI development. The impact of this innovation is expected to be significant, as it could reduce costs and time associated with AI model development. This advancement marks a step forward in the field of AI, offering a more scalable and efficient solution for complex problem-solving tasks.
OpenCUA’s open source computer-use agents rival proprietary models from OpenAI and Anthropic (https://venturebeat.com/ai/opencuas-open-source-computer-use-agents-rival-proprietary-models-from-openai-and-anthropic/): OpenCUA is an open-source framework that provides data and training recipes for building powerful computer-use agents. These agents challenge proprietary systems like those from OpenAI and Anthropic. The framework is designed to rival proprietary models in terms of capability and performance. Its open-source nature allows for greater accessibility and potential for innovation. The impact of OpenCUA could be significant in advancing AI capabilities and democratizing access to advanced AI technologies. The novelty of this framework lies in its open-source approach, which contrasts with the closed-source models of major AI companies.
Google warns of new AI-powered malware families deployed in the wild (https://www.bleepingcomputer.com/news/security/google-warns-of-new-ai-powered-malware-families-deployed-in-the-wild/): Google's Threat Intelligence Group (GTIG) has identified a major shift this year, with adversaries leveraging artificial intelligence to deploy new malware families that integrate large language models (LLMs) during execution. This new approach enables dynamic altering mid-execution, which reaches new levels of operational versatility that are virtually impossible to achieve with traditional malware. Google calls the technique &quot;just-in-time&quot; self-modification and highlights the experimental PromptFlux malware dropper and the PromptSteal (a.k.a. LameHug) data miner deployed in Ukraine, as examples for dynamic script generation, code obfuscation, and creation of on-demand functions. PromptFlux is an experimental VBScript dropper that leverages Google's LLM Gemini in its latest version to generate obfuscated VBScript variants. It attempts persistence via Startup folder entries, and spreads laterally on removable drives and mapped network shares. &quot;The most novel component of PROMPTFLUX is its 'Thinking Robot' module, designed to periodically query Gemini to obtain new code for evading antivirus software,&quot; explains Google. The prompt is very specific and machine-parsable, according to the researchers, who see indications that the malware's creators aim to create an ever-evolving &quot;metamorphic script.&quot; PromptFlux &quot;StartThinkingRobot&quot; functionSource: Google Google could not attribute PromptFlux to a specific threat actor, but noted that the tactics, techniques, and procedures indicate
What to Do When Your Credit Risk Model Works Today, but Breaks Six Months Later (https://towardsdatascience.com/your-credit-risk-model-works-today-it-breaks-in-six-months/): A study reveals that gradient-boosted trees (XGBoost) fail to maintain ranking stability over time, leading to significant accuracy degradation in credit risk models. The research identifies that the geometry of optimization, rather than data or hyperparameters, is crucial for maintaining model stability. By adopting a Hamiltonian framework for neural network training, the study demonstrates improved temporal stability, with AUC scores dropping only 10.6 points over six years compared to XGBoost's 32 points. This approach preserves relative orderings, crucial for ranking tasks like credit risk assessment. The findings suggest that optimizing models on symplectic manifolds can enhance stability, offering broader applicability across systems requiring ranking, such as medical risk stratification and fraud detection.
Russian spies pack custom malware into hidden VMs on Windows machines (https://go.theregister.com/feed/www.theregister.com/2025/11/04/russian_spies_pack_custom_malware/): Russian spies, known as Curly COMrades, are exploiting Microsoft's Hyper-V hypervisor to create hidden virtual machines (VMs) on compromised Windows machines. This technique allows them to bypass endpoint detection and response (EDR) tools, providing long-term network access for surveillance and malware deployment. The attackers use a lightweight Alpine Linux-based VM to host custom malware, including CurlyShell and CurlCat, which are designed to evade detection and maintain persistence. This campaign, which began in July, has targeted judicial and government bodies in Georgia and an energy company in Moldova, with Bitdefender tracking the group since 2024. The sophistication of this approach highlights a growing trend of threat actors bypassing EDR solutions by leveraging legitimate virtualization technologies. To counter this, experts recommend a multi-layered defense strategy beyond endpoint detection.</description>
      <itunes:summary>Articles discussed
How Sakana AI’s new evolutionary algorithm builds powerful AI models without expensive retraining (https://venturebeat.com/ai/how-sakana-ais-new-evolutionary-algorithm-builds-powerful-ai-models-without-expensive-retraining/): Sakana AI has developed a new model merging technique called M2N2, which enables the creation of powerful multi-skilled agents without the high cost and data needs of retraining. This method allows for the development of AI models that can perform multiple tasks efficiently. The technique is particularly useful in fields requiring diverse skill sets, such as healthcare and customer service. M2N2 leverages evolutionary algorithms to optimize model performance, making it a novel approach in AI development. The impact of this innovation is expected to be significant, as it could reduce costs and time associated with AI model development. This advancement marks a step forward in the field of AI, offering a more scalable and efficient solution for complex problem-solving tasks.
OpenCUA’s open source computer-use agents rival proprietary models from OpenAI and Anthropic (https://venturebeat.com/ai/opencuas-open-source-computer-use-agents-rival-proprietary-models-from-openai-and-anthropic/): OpenCUA is an open-source framework that provides data and training recipes for building powerful computer-use agents. These agents challenge proprietary systems like those from OpenAI and Anthropic. The framework is designed to rival proprietary models in terms of capability and performance. Its open-source nature allows for greater accessibility and potential for innovation. The impact of OpenCUA could be significant in advancing AI capabilities and democratizing access to advanced AI technologies. The novelty of this framework lies in its open-source approach, which contrasts with the closed-source models of major AI companies.
Google warns of new AI-powered malware families deployed in the wild (https://www.bleepingcomputer.com/news/security/google-warns-of-new-ai-powered-malware-families-deployed-in-the-wild/): Google's Threat Intelligence Group (GTIG) has identified a major shift this year, with adversaries leveraging artificial intelligence to deploy new malware families that integrate large language models (LLMs) during execution. This new approach enables dynamic altering mid-execution, which reaches new levels of operational versatility that are virtually impossible to achieve with traditional malware. Google calls the technique &quot;just-in-time&quot; self-modification and highlights the experimental PromptFlux malware dropper and the PromptSteal (a.k.a. LameHug) data miner deployed in Ukraine, as examples for dynamic script generation, code obfuscation, and creation of on-demand functions. PromptFlux is an experimental VBScript dropper that leverages Google's LLM Gemini in its latest version to generate obfuscated VBScript variants. It attempts persistence via Startup folder entries, and spreads laterally on removable drives and mapped network shares. &quot;The most novel component of PROMPTFLUX is its 'Thinking Robot' module, designed to periodically query Gemini to obtain new code for evading antivirus software,&quot; explains Google. The prompt is very specific and machine-parsable, according to the researchers, who see indications that the malware's creators aim to create an ever-evolving &quot;metamorphic script.&quot; PromptFlux &quot;StartThinkingRobot&quot; functionSource: Google Google could not attribute PromptFlux to a specific threat actor, but noted that the tactics, techniques, and procedures indicate
What to Do When Your Credit Risk Model Works Today, but Breaks Six Months Later (https://towardsdatascience.com/your-credit-risk-model-works-today-it-breaks-in-six-months/): A study reveals that gradient-boosted trees (XGBoost) fail to maintain ranking stability over time, leading to significant accuracy degradation in credit risk models. The research identifies that the geometry of optimization, rather than data or hyperparameters, is crucial for maintaining model stability. By adopting a Hamiltonian framework for neural network training, the study demonstrates improved temporal stability, with AUC scores dropping only 10.6 points over six years compared to XGBoost's 32 points. This approach preserves relative orderings, crucial for ranking tasks like credit risk assessment. The findings suggest that optimizing models on symplectic manifolds can enhance stability, offering broader applicability across systems requiring ranking, such as medical risk stratification and fraud detection.
Russian spies pack custom malware into hidden VMs on Windows machines (https://go.theregister.com/feed/www.theregister.com/2025/11/04/russian_spies_pack_custom_malware/): Russian spies, known as Curly COMrades, are exploiting Microsoft's Hyper-V hypervisor to create hidden virtual machines (VMs) on compromised Windows machines. This technique allows them to bypass endpoint detection and response (EDR) tools, providing long-term network access for surveillance and malware deployment. The attackers use a lightweight Alpine Linux-based VM to host custom malware, including CurlyShell and CurlCat, which are designed to evade detection and maintain persistence. This campaign, which began in July, has targeted judicial and government bodies in Georgia and an energy company in Moldova, with Bitdefender tracking the group since 2024. The sophistication of this approach highlights a growing trend of threat actors bypassing EDR solutions by leveraging legitimate virtualization technologies. To counter this, experts recommend a multi-layered defense strategy beyond endpoint detection.</itunes:summary>
      <description>Articles discussed
How Sakana AI’s new evolutionary algorithm builds powerful AI models without expensive retraining (https://venturebeat.com/ai/how-sakana-ais-new-evolutionary-algorithm-builds-powerful-ai-models-without-expensive-retraining/): Sakana AI has developed a new model merging technique called M2N2, which enables the creation of powerful multi-skilled agents without the high cost and data needs of retraining. This method allows for the development of AI models that can perform multiple tasks efficiently. The technique is particularly useful in fields requiring diverse skill sets, such as healthcare and customer service. M2N2 leverages evolutionary algorithms to optimize model performance, making it a novel approach in AI development. The impact of this innovation is expected to be significant, as it could reduce costs and time associated with AI model development. This advancement marks a step forward in the field of AI, offering a more scalable and efficient solution for complex problem-solving tasks.
OpenCUA’s open source computer-use agents rival proprietary models from OpenAI and Anthropic (https://venturebeat.com/ai/opencuas-open-source-computer-use-agents-rival-proprietary-models-from-openai-and-anthropic/): OpenCUA is an open-source framework that provides data and training recipes for building powerful computer-use agents. These agents challenge proprietary systems like those from OpenAI and Anthropic. The framework is designed to rival proprietary models in terms of capability and performance. Its open-source nature allows for greater accessibility and potential for innovation. The impact of OpenCUA could be significant in advancing AI capabilities and democratizing access to advanced AI technologies. The novelty of this framework lies in its open-source approach, which contrasts with the closed-source models of major AI companies.
Google warns of new AI-powered malware families deployed in the wild (https://www.bleepingcomputer.com/news/security/google-warns-of-new-ai-powered-malware-families-deployed-in-the-wild/): Google's Threat Intelligence Group (GTIG) has identified a major shift this year, with adversaries leveraging artificial intelligence to deploy new malware families that integrate large language models (LLMs) during execution. This new approach enables dynamic altering mid-execution, which reaches new levels of operational versatility that are virtually impossible to achieve with traditional malware. Google calls the technique &quot;just-in-time&quot; self-modification and highlights the experimental PromptFlux malware dropper and the PromptSteal (a.k.a. LameHug) data miner deployed in Ukraine, as examples for dynamic script generation, code obfuscation, and creation of on-demand functions. PromptFlux is an experimental VBScript dropper that leverages Google's LLM Gemini in its latest version to generate obfuscated VBScript variants. It attempts persistence via Startup folder entries, and spreads laterally on removable drives and mapped network shares. &quot;The most novel component of PROMPTFLUX is its 'Thinking Robot' module, designed to periodically query Gemini to obtain new code for evading antivirus software,&quot; explains Google. The prompt is very specific and machine-parsable, according to the researchers, who see indications that the malware's creators aim to create an ever-evolving &quot;metamorphic script.&quot; PromptFlux &quot;StartThinkingRobot&quot; functionSource: Google Google could not attribute PromptFlux to a specific threat actor, but noted that the tactics, techniques, and procedures indicate
What to Do When Your Credit Risk Model Works Today, but Breaks Six Months Later (https://towardsdatascience.com/your-credit-risk-model-works-today-it-breaks-in-six-months/): A study reveals that gradient-boosted trees (XGBoost) fail to maintain ranking stability over time, leading to significant accuracy degradation in credit risk models. The research identifies that the geometry of optimization, rather than data or hyperparameters, is crucial for maintaining model stability. By adopting a Hamiltonian framework for neural network training, the study demonstrates improved temporal stability, with AUC scores dropping only 10.6 points over six years compared to XGBoost's 32 points. This approach preserves relative orderings, crucial for ranking tasks like credit risk assessment. The findings suggest that optimizing models on symplectic manifolds can enhance stability, offering broader applicability across systems requiring ranking, such as medical risk stratification and fraud detection.
Russian spies pack custom malware into hidden VMs on Windows machines (https://go.theregister.com/feed/www.theregister.com/2025/11/04/russian_spies_pack_custom_malware/): Russian spies, known as Curly COMrades, are exploiting Microsoft's Hyper-V hypervisor to create hidden virtual machines (VMs) on compromised Windows machines. This technique allows them to bypass endpoint detection and response (EDR) tools, providing long-term network access for surveillance and malware deployment. The attackers use a lightweight Alpine Linux-based VM to host custom malware, including CurlyShell and CurlCat, which are designed to evade detection and maintain persistence. This campaign, which began in July, has targeted judicial and government bodies in Georgia and an energy company in Moldova, with Bitdefender tracking the group since 2024. The sophistication of this approach highlights a growing trend of threat actors bypassing EDR solutions by leveraging legitimate virtualization technologies. To counter this, experts recommend a multi-layered defense strategy beyond endpoint detection.</description>
      <itunes:summary>Articles discussed
How Sakana AI’s new evolutionary algorithm builds powerful AI models without expensive retraining (https://venturebeat.com/ai/how-sakana-ais-new-evolutionary-algorithm-builds-powerful-ai-models-without-expensive-retraining/): Sakana AI has developed a new model merging technique called M2N2, which enables the creation of powerful multi-skilled agents without the high cost and data needs of retraining. This method allows for the development of AI models that can perform multiple tasks efficiently. The technique is particularly useful in fields requiring diverse skill sets, such as healthcare and customer service. M2N2 leverages evolutionary algorithms to optimize model performance, making it a novel approach in AI development. The impact of this innovation is expected to be significant, as it could reduce costs and time associated with AI model development. This advancement marks a step forward in the field of AI, offering a more scalable and efficient solution for complex problem-solving tasks.
OpenCUA’s open source computer-use agents rival proprietary models from OpenAI and Anthropic (https://venturebeat.com/ai/opencuas-open-source-computer-use-agents-rival-proprietary-models-from-openai-and-anthropic/): OpenCUA is an open-source framework that provides data and training recipes for building powerful computer-use agents. These agents challenge proprietary systems like those from OpenAI and Anthropic. The framework is designed to rival proprietary models in terms of capability and performance. Its open-source nature allows for greater accessibility and potential for innovation. The impact of OpenCUA could be significant in advancing AI capabilities and democratizing access to advanced AI technologies. The novelty of this framework lies in its open-source approach, which contrasts with the closed-source models of major AI companies.
Google warns of new AI-powered malware families deployed in the wild (https://www.bleepingcomputer.com/news/security/google-warns-of-new-ai-powered-malware-families-deployed-in-the-wild/): Google's Threat Intelligence Group (GTIG) has identified a major shift this year, with adversaries leveraging artificial intelligence to deploy new malware families that integrate large language models (LLMs) during execution. This new approach enables dynamic altering mid-execution, which reaches new levels of operational versatility that are virtually impossible to achieve with traditional malware. Google calls the technique &quot;just-in-time&quot; self-modification and highlights the experimental PromptFlux malware dropper and the PromptSteal (a.k.a. LameHug) data miner deployed in Ukraine, as examples for dynamic script generation, code obfuscation, and creation of on-demand functions. PromptFlux is an experimental VBScript dropper that leverages Google's LLM Gemini in its latest version to generate obfuscated VBScript variants. It attempts persistence via Startup folder entries, and spreads laterally on removable drives and mapped network shares. &quot;The most novel component of PROMPTFLUX is its 'Thinking Robot' module, designed to periodically query Gemini to obtain new code for evading antivirus software,&quot; explains Google. The prompt is very specific and machine-parsable, according to the researchers, who see indications that the malware's creators aim to create an ever-evolving &quot;metamorphic script.&quot; PromptFlux &quot;StartThinkingRobot&quot; functionSource: Google Google could not attribute PromptFlux to a specific threat actor, but noted that the tactics, techniques, and procedures indicate
What to Do When Your Credit Risk Model Works Today, but Breaks Six Months Later (https://towardsdatascience.com/your-credit-risk-model-works-today-it-breaks-in-six-months/): A study reveals that gradient-boosted trees (XGBoost) fail to maintain ranking stability over time, leading to significant accuracy degradation in credit risk models. The research identifies that the geometry of optimization, rather than data or hyperparameters, is crucial for maintaining model stability. By adopting a Hamiltonian framework for neural network training, the study demonstrates improved temporal stability, with AUC scores dropping only 10.6 points over six years compared to XGBoost's 32 points. This approach preserves relative orderings, crucial for ranking tasks like credit risk assessment. The findings suggest that optimizing models on symplectic manifolds can enhance stability, offering broader applicability across systems requiring ranking, such as medical risk stratification and fraud detection.
Russian spies pack custom malware into hidden VMs on Windows machines (https://go.theregister.com/feed/www.theregister.com/2025/11/04/russian_spies_pack_custom_malware/): Russian spies, known as Curly COMrades, are exploiting Microsoft's Hyper-V hypervisor to create hidden virtual machines (VMs) on compromised Windows machines. This technique allows them to bypass endpoint detection and response (EDR) tools, providing long-term network access for surveillance and malware deployment. The attackers use a lightweight Alpine Linux-based VM to host custom malware, including CurlyShell and CurlCat, which are designed to evade detection and maintain persistence. This campaign, which began in July, has targeted judicial and government bodies in Georgia and an energy company in Moldova, with Bitdefender tracking the group since 2024. The sophistication of this approach highlights a growing trend of threat actors bypassing EDR solutions by leveraging legitimate virtualization technologies. To counter this, experts recommend a multi-layered defense strategy beyond endpoint detection.</itunes:summary>
      <pubDate>Sat, 17 Jan 2026 15:58:59 GMT</pubDate>
      <enclosure url="https://raw.githubusercontent.com/Refugeek/podcast-feed/main/episodes-EScyber/20251105193822-how-sakana-ais-new-evolutionary-algorithm-builds-powerful-ai-models-without-expensive-retraining-opencuas-open-source-computer-use-agents-rival-proprietary-models-from-openai-and-anthropic.mp3" type="audio/mpeg" length="28814924"/>
      <itunes:duration>1440</itunes:duration>
    </item>
    <item>
      <title>20251106022824-japan-s-active-cyberdefense-law-a-new-era-in-cybersecurity-strategy-agentic-ais-ooda-loop-problem</title>
      <description>Articles discussed
Japan's Active Cyberdefense Law: A New Era in Cybersecurity Strategy (https://www.tripwire.com/state-of-security/japans-active-cyberdefense-law-new-era-cybersecurity-strategy): On May 16th, 2025, Japan enacted the Japan Active Cyberdefense Law, empowering its law enforcement and military agencies to conduct pre-emptive cyber operations. The law grants government agencies the ability to monitor foreign internet traffic, conduct pre-emptive countermeasures, launch joint cyber operations, and mandate businesses to report cyberattacks. The law is a reaction to criticism of Japan's digital defenses, particularly after the 2022 &quot;Blair Shock&quot; incident, and is a reflection of a global trend towards more aggressive cybersecurity strategies. The law draws a line at domestic surveillance, exempting private communications from monitoring. The law's implementation will require Japan to build operational capacity within its cyber units, develop clear rules of engagement, and deepen ties with international allies and private sector partners.
Agentic AI’s OODA Loop Problem (https://www.schneier.com/blog/archives/2025/10/agentic-ais-ooda-loop-problem.html): The OODA loop—for observe, orient, decide, act—is a framework to understand decision-making in adversarial situations. We apply the same framework to artificial intelligence agents, who have to make their decisions with untrustworthy observations and orientation. To solve this problem, we need new systems of input, processing, and output integrity. Many decades ago, U.S. Air Force Colonel John Boyd introduced the concept of the “OODA loop,” for Observe, Orient, Decide, and Act. These are the four steps of real-time continuous decision-making. Boyd developed it for fighter pilots, but it’s long been applied in artificial intelligence (AI) and robotics. An AI agent, like a pilot, executes the loop over and over, accomplishing its goals iteratively within an ever-changing environment. This is Anthropic’s definition: “Agents are models using tools in a loop.”...
We Didn’t Invent Attention — We Just Rediscovered It (https://towardsdatascience.com/we-didnt-invent-attention-we-just-rediscovered-it/): Attention mechanisms, a fundamental aspect of information processing, have been observed to emerge independently across biological, chemical, and artificial systems. This convergence suggests that certain mathematical structures represent convergent solutions to fundamental optimization problems. Biological systems, such as the optic tectum in vertebrates and C. elegans, exhibit attention-like mechanisms that have evolved over hundreds of millions of years. Chemical systems, like the formose reaction, demonstrate selective amplification principles that power attention mechanisms without programming or training. This finding challenges traditional metaphors of attention as selection and highlights the importance of amplification and normalization in information processing. The implications of these findings extend beyond AI, suggesting that attention mechanisms may be a universal principle underlying efficient information processing in all computational systems.
Securing critical infrastructure: Why Europe’s risk-based regulations matter (https://www.microsoft.com/en-us/security/blog/2025/11/05/securing-critical-infrastructure-why-europes-risk-based-regulations-matter/): The European Union has enacted two significant cybersecurity regulations: the Network and Information Systems Directive 2 (NIS2) and the Digital Operational Resilience Act (DORA). These laws aim to bolster cybersecurity standards across critical sectors, including energy, telecommunications, and financial services, by establishing stringent requirements for risk management, incident reporting, and governance oversight. NIS2 mandates a uniform cybersecurity framework across EU entities, while DORA focuses on enhancing the digital resilience of financial institutions. These regulations shift the CISO role to a more strategic position, requiring broader oversight of IT, operational technology, IoT, AI, and supply chain components. Compliance with NIS2 and DORA emphasizes a risk-based approach, prioritizing controls that effectively mitigate identified threats, such as multifactor authentication and cryptography. The legislation underscores the importance of board oversight in cyber governance, holding directors accountable for implementing and maintaining robust cybersecurity measures.</description>
      <itunes:summary>Articles discussed
Japan's Active Cyberdefense Law: A New Era in Cybersecurity Strategy (https://www.tripwire.com/state-of-security/japans-active-cyberdefense-law-new-era-cybersecurity-strategy): On May 16th, 2025, Japan enacted the Japan Active Cyberdefense Law, empowering its law enforcement and military agencies to conduct pre-emptive cyber operations. The law grants government agencies the ability to monitor foreign internet traffic, conduct pre-emptive countermeasures, launch joint cyber operations, and mandate businesses to report cyberattacks. The law is a reaction to criticism of Japan's digital defenses, particularly after the 2022 &quot;Blair Shock&quot; incident, and is a reflection of a global trend towards more aggressive cybersecurity strategies. The law draws a line at domestic surveillance, exempting private communications from monitoring. The law's implementation will require Japan to build operational capacity within its cyber units, develop clear rules of engagement, and deepen ties with international allies and private sector partners.
Agentic AI’s OODA Loop Problem (https://www.schneier.com/blog/archives/2025/10/agentic-ais-ooda-loop-problem.html): The OODA loop—for observe, orient, decide, act—is a framework to understand decision-making in adversarial situations. We apply the same framework to artificial intelligence agents, who have to make their decisions with untrustworthy observations and orientation. To solve this problem, we need new systems of input, processing, and output integrity. Many decades ago, U.S. Air Force Colonel John Boyd introduced the concept of the “OODA loop,” for Observe, Orient, Decide, and Act. These are the four steps of real-time continuous decision-making. Boyd developed it for fighter pilots, but it’s long been applied in artificial intelligence (AI) and robotics. An AI agent, like a pilot, executes the loop over and over, accomplishing its goals iteratively within an ever-changing environment. This is Anthropic’s definition: “Agents are models using tools in a loop.”...
We Didn’t Invent Attention — We Just Rediscovered It (https://towardsdatascience.com/we-didnt-invent-attention-we-just-rediscovered-it/): Attention mechanisms, a fundamental aspect of information processing, have been observed to emerge independently across biological, chemical, and artificial systems. This convergence suggests that certain mathematical structures represent convergent solutions to fundamental optimization problems. Biological systems, such as the optic tectum in vertebrates and C. elegans, exhibit attention-like mechanisms that have evolved over hundreds of millions of years. Chemical systems, like the formose reaction, demonstrate selective amplification principles that power attention mechanisms without programming or training. This finding challenges traditional metaphors of attention as selection and highlights the importance of amplification and normalization in information processing. The implications of these findings extend beyond AI, suggesting that attention mechanisms may be a universal principle underlying efficient information processing in all computational systems.
Securing critical infrastructure: Why Europe’s risk-based regulations matter (https://www.microsoft.com/en-us/security/blog/2025/11/05/securing-critical-infrastructure-why-europes-risk-based-regulations-matter/): The European Union has enacted two significant cybersecurity regulations: the Network and Information Systems Directive 2 (NIS2) and the Digital Operational Resilience Act (DORA). These laws aim to bolster cybersecurity standards across critical sectors, including energy, telecommunications, and financial services, by establishing stringent requirements for risk management, incident reporting, and governance oversight. NIS2 mandates a uniform cybersecurity framework across EU entities, while DORA focuses on enhancing the digital resilience of financial institutions. These regulations shift the CISO role to a more strategic position, requiring broader oversight of IT, operational technology, IoT, AI, and supply chain components. Compliance with NIS2 and DORA emphasizes a risk-based approach, prioritizing controls that effectively mitigate identified threats, such as multifactor authentication and cryptography. The legislation underscores the importance of board oversight in cyber governance, holding directors accountable for implementing and maintaining robust cybersecurity measures.</itunes:summary>
      <description>Articles discussed
Japan's Active Cyberdefense Law: A New Era in Cybersecurity Strategy (https://www.tripwire.com/state-of-security/japans-active-cyberdefense-law-new-era-cybersecurity-strategy): On May 16th, 2025, Japan enacted the Japan Active Cyberdefense Law, empowering its law enforcement and military agencies to conduct pre-emptive cyber operations. The law grants government agencies the ability to monitor foreign internet traffic, conduct pre-emptive countermeasures, launch joint cyber operations, and mandate businesses to report cyberattacks. The law is a reaction to criticism of Japan's digital defenses, particularly after the 2022 &quot;Blair Shock&quot; incident, and is a reflection of a global trend towards more aggressive cybersecurity strategies. The law draws a line at domestic surveillance, exempting private communications from monitoring. The law's implementation will require Japan to build operational capacity within its cyber units, develop clear rules of engagement, and deepen ties with international allies and private sector partners.
Agentic AI’s OODA Loop Problem (https://www.schneier.com/blog/archives/2025/10/agentic-ais-ooda-loop-problem.html): The OODA loop—for observe, orient, decide, act—is a framework to understand decision-making in adversarial situations. We apply the same framework to artificial intelligence agents, who have to make their decisions with untrustworthy observations and orientation. To solve this problem, we need new systems of input, processing, and output integrity. Many decades ago, U.S. Air Force Colonel John Boyd introduced the concept of the “OODA loop,” for Observe, Orient, Decide, and Act. These are the four steps of real-time continuous decision-making. Boyd developed it for fighter pilots, but it’s long been applied in artificial intelligence (AI) and robotics. An AI agent, like a pilot, executes the loop over and over, accomplishing its goals iteratively within an ever-changing environment. This is Anthropic’s definition: “Agents are models using tools in a loop.”...
We Didn’t Invent Attention — We Just Rediscovered It (https://towardsdatascience.com/we-didnt-invent-attention-we-just-rediscovered-it/): Attention mechanisms, a fundamental aspect of information processing, have been observed to emerge independently across biological, chemical, and artificial systems. This convergence suggests that certain mathematical structures represent convergent solutions to fundamental optimization problems. Biological systems, such as the optic tectum in vertebrates and C. elegans, exhibit attention-like mechanisms that have evolved over hundreds of millions of years. Chemical systems, like the formose reaction, demonstrate selective amplification principles that power attention mechanisms without programming or training. This finding challenges traditional metaphors of attention as selection and highlights the importance of amplification and normalization in information processing. The implications of these findings extend beyond AI, suggesting that attention mechanisms may be a universal principle underlying efficient information processing in all computational systems.
Securing critical infrastructure: Why Europe’s risk-based regulations matter (https://www.microsoft.com/en-us/security/blog/2025/11/05/securing-critical-infrastructure-why-europes-risk-based-regulations-matter/): The European Union has enacted two significant cybersecurity regulations: the Network and Information Systems Directive 2 (NIS2) and the Digital Operational Resilience Act (DORA). These laws aim to bolster cybersecurity standards across critical sectors, including energy, telecommunications, and financial services, by establishing stringent requirements for risk management, incident reporting, and governance oversight. NIS2 mandates a uniform cybersecurity framework across EU entities, while DORA focuses on enhancing the digital resilience of financial institutions. These regulations shift the CISO role to a more strategic position, requiring broader oversight of IT, operational technology, IoT, AI, and supply chain components. Compliance with NIS2 and DORA emphasizes a risk-based approach, prioritizing controls that effectively mitigate identified threats, such as multifactor authentication and cryptography. The legislation underscores the importance of board oversight in cyber governance, holding directors accountable for implementing and maintaining robust cybersecurity measures.</description>
      <itunes:summary>Articles discussed
Japan's Active Cyberdefense Law: A New Era in Cybersecurity Strategy (https://www.tripwire.com/state-of-security/japans-active-cyberdefense-law-new-era-cybersecurity-strategy): On May 16th, 2025, Japan enacted the Japan Active Cyberdefense Law, empowering its law enforcement and military agencies to conduct pre-emptive cyber operations. The law grants government agencies the ability to monitor foreign internet traffic, conduct pre-emptive countermeasures, launch joint cyber operations, and mandate businesses to report cyberattacks. The law is a reaction to criticism of Japan's digital defenses, particularly after the 2022 &quot;Blair Shock&quot; incident, and is a reflection of a global trend towards more aggressive cybersecurity strategies. The law draws a line at domestic surveillance, exempting private communications from monitoring. The law's implementation will require Japan to build operational capacity within its cyber units, develop clear rules of engagement, and deepen ties with international allies and private sector partners.
Agentic AI’s OODA Loop Problem (https://www.schneier.com/blog/archives/2025/10/agentic-ais-ooda-loop-problem.html): The OODA loop—for observe, orient, decide, act—is a framework to understand decision-making in adversarial situations. We apply the same framework to artificial intelligence agents, who have to make their decisions with untrustworthy observations and orientation. To solve this problem, we need new systems of input, processing, and output integrity. Many decades ago, U.S. Air Force Colonel John Boyd introduced the concept of the “OODA loop,” for Observe, Orient, Decide, and Act. These are the four steps of real-time continuous decision-making. Boyd developed it for fighter pilots, but it’s long been applied in artificial intelligence (AI) and robotics. An AI agent, like a pilot, executes the loop over and over, accomplishing its goals iteratively within an ever-changing environment. This is Anthropic’s definition: “Agents are models using tools in a loop.”...
We Didn’t Invent Attention — We Just Rediscovered It (https://towardsdatascience.com/we-didnt-invent-attention-we-just-rediscovered-it/): Attention mechanisms, a fundamental aspect of information processing, have been observed to emerge independently across biological, chemical, and artificial systems. This convergence suggests that certain mathematical structures represent convergent solutions to fundamental optimization problems. Biological systems, such as the optic tectum in vertebrates and C. elegans, exhibit attention-like mechanisms that have evolved over hundreds of millions of years. Chemical systems, like the formose reaction, demonstrate selective amplification principles that power attention mechanisms without programming or training. This finding challenges traditional metaphors of attention as selection and highlights the importance of amplification and normalization in information processing. The implications of these findings extend beyond AI, suggesting that attention mechanisms may be a universal principle underlying efficient information processing in all computational systems.
Securing critical infrastructure: Why Europe’s risk-based regulations matter (https://www.microsoft.com/en-us/security/blog/2025/11/05/securing-critical-infrastructure-why-europes-risk-based-regulations-matter/): The European Union has enacted two significant cybersecurity regulations: the Network and Information Systems Directive 2 (NIS2) and the Digital Operational Resilience Act (DORA). These laws aim to bolster cybersecurity standards across critical sectors, including energy, telecommunications, and financial services, by establishing stringent requirements for risk management, incident reporting, and governance oversight. NIS2 mandates a uniform cybersecurity framework across EU entities, while DORA focuses on enhancing the digital resilience of financial institutions. These regulations shift the CISO role to a more strategic position, requiring broader oversight of IT, operational technology, IoT, AI, and supply chain components. Compliance with NIS2 and DORA emphasizes a risk-based approach, prioritizing controls that effectively mitigate identified threats, such as multifactor authentication and cryptography. The legislation underscores the importance of board oversight in cyber governance, holding directors accountable for implementing and maintaining robust cybersecurity measures.</itunes:summary>
      <pubDate>Sat, 17 Jan 2026 15:58:59 GMT</pubDate>
      <enclosure url="https://raw.githubusercontent.com/Refugeek/podcast-feed/main/episodes-EScyber/20251106022824-japan-s-active-cyberdefense-law-a-new-era-in-cybersecurity-strategy-agentic-ais-ooda-loop-problem.mp3" type="audio/mpeg" length="37617164"/>
      <itunes:duration>1880</itunes:duration>
    </item>
    <item>
      <title>20251107214816-self-improving-language-models-are-becoming-reality-with-mit-s-updated-seal-technique-we-keep-talking-about-ai-agents-but-do-we-ever-know-what-they-are</title>
      <description>Articles discussed
Self-improving language models are becoming reality with MIT's updated SEAL technique (https://venturebeat.com/ai/self-improving-language-models-are-becoming-reality-with-mits-updated-seal): Researchers at the Massachusetts Institute of Technology (MIT) are gaining renewed attention for developing and open sourcing a technique that allows large language models (LLMs) — like those underpinning ChatGPT and most modern AI chatbots — to improve themselves by generating synthetic data to fine-tune upon. The technique, known as SEAL (Self-Adapting LLMs), was first described in a paper published back in June and covered by VentureBeat at the time.A significantly expanded and updated version of the paper was released last month, as well as open source code posted on Github (under an MIT License, allowing for commercial and enterprise usage), and is making new waves among AI power users on the social network X this week.SEAL allows LLMs to autonomously generate and apply their own fine-tuning strategies. Unlike conventional models that rely on fixed external data and human-crafted optimization pipelines, SEAL enables models to evolve by producing their own synthetic training data and corresponding optimization directives.The development comes from a team affiliated with MIT’s Improbable AI
We keep talking about AI agents, but do we ever know what they are? (https://venturebeat.com/ai/we-keep-talking-about-ai-agents-but-do-we-ever-know-what-they-are): Imagine you do two things on a Monday morning.First, you ask a chatbot to summarize your new emails. Next, you ask an AI tool to figure out why your top competitor grew so fast last quarter. The AI silently gets to work. It scours financial reports, news articles and social media sentiment. It cross-references that data with your internal sales numbers, drafts a strategy outlining three potential reasons for the competitor's success and schedules a 30-minute meeting with your team to present its findings.We're calling both of these &quot;AI agents,&quot; but they represent worlds of difference in intelligence, capability and the level of trust we place in them. This ambiguity creates a fog that makes it difficult to build, evaluate, and safely govern these powerful new tools. If we can't agree on what we're building, how can we know when we've succeeded?This post won't try to sell you on yet another definitive framework. Instead, think of it as a survey of the current landscape of agent autonomy, a map to help us all navigate the terrain together.What are we even talking about? Defining an &quot;AI agent&quot;Befo
Whisper Leak: A novel side-channel attack on remote language models (https://www.microsoft.com/en-us/security/blog/2025/11/07/whisper-leak-a-novel-side-channel-cyberattack-on-remote-language-models/): Microsoft has identified a novel side-channel attack on remote language models, dubbed &quot;Whisper Leak,&quot; which exploits network packet sizes and timings to infer conversation topics despite end-to-end encryption. This attack allows adversaries to observe encrypted traffic and deduce sensitive information, posing risks in oppressive regimes or sensitive sectors like healthcare and law. The attack leverages the autoregressive nature of language models, which generate responses in sequential steps, making it possible to infer topics based on packet size variations. Microsoft collaborated with multiple vendors to mitigate these risks, implementing privacy enhancements in their language model frameworks. The research highlights the importance of robust anonymization and encryption techniques to safeguard user data in AI-driven interactions.</description>
      <itunes:summary>Articles discussed
Self-improving language models are becoming reality with MIT's updated SEAL technique (https://venturebeat.com/ai/self-improving-language-models-are-becoming-reality-with-mits-updated-seal): Researchers at the Massachusetts Institute of Technology (MIT) are gaining renewed attention for developing and open sourcing a technique that allows large language models (LLMs) — like those underpinning ChatGPT and most modern AI chatbots — to improve themselves by generating synthetic data to fine-tune upon. The technique, known as SEAL (Self-Adapting LLMs), was first described in a paper published back in June and covered by VentureBeat at the time.A significantly expanded and updated version of the paper was released last month, as well as open source code posted on Github (under an MIT License, allowing for commercial and enterprise usage), and is making new waves among AI power users on the social network X this week.SEAL allows LLMs to autonomously generate and apply their own fine-tuning strategies. Unlike conventional models that rely on fixed external data and human-crafted optimization pipelines, SEAL enables models to evolve by producing their own synthetic training data and corresponding optimization directives.The development comes from a team affiliated with MIT’s Improbable AI
We keep talking about AI agents, but do we ever know what they are? (https://venturebeat.com/ai/we-keep-talking-about-ai-agents-but-do-we-ever-know-what-they-are): Imagine you do two things on a Monday morning.First, you ask a chatbot to summarize your new emails. Next, you ask an AI tool to figure out why your top competitor grew so fast last quarter. The AI silently gets to work. It scours financial reports, news articles and social media sentiment. It cross-references that data with your internal sales numbers, drafts a strategy outlining three potential reasons for the competitor's success and schedules a 30-minute meeting with your team to present its findings.We're calling both of these &quot;AI agents,&quot; but they represent worlds of difference in intelligence, capability and the level of trust we place in them. This ambiguity creates a fog that makes it difficult to build, evaluate, and safely govern these powerful new tools. If we can't agree on what we're building, how can we know when we've succeeded?This post won't try to sell you on yet another definitive framework. Instead, think of it as a survey of the current landscape of agent autonomy, a map to help us all navigate the terrain together.What are we even talking about? Defining an &quot;AI agent&quot;Befo
Whisper Leak: A novel side-channel attack on remote language models (https://www.microsoft.com/en-us/security/blog/2025/11/07/whisper-leak-a-novel-side-channel-cyberattack-on-remote-language-models/): Microsoft has identified a novel side-channel attack on remote language models, dubbed &quot;Whisper Leak,&quot; which exploits network packet sizes and timings to infer conversation topics despite end-to-end encryption. This attack allows adversaries to observe encrypted traffic and deduce sensitive information, posing risks in oppressive regimes or sensitive sectors like healthcare and law. The attack leverages the autoregressive nature of language models, which generate responses in sequential steps, making it possible to infer topics based on packet size variations. Microsoft collaborated with multiple vendors to mitigate these risks, implementing privacy enhancements in their language model frameworks. The research highlights the importance of robust anonymization and encryption techniques to safeguard user data in AI-driven interactions.</itunes:summary>
      <description>Articles discussed
Self-improving language models are becoming reality with MIT's updated SEAL technique (https://venturebeat.com/ai/self-improving-language-models-are-becoming-reality-with-mits-updated-seal): Researchers at the Massachusetts Institute of Technology (MIT) are gaining renewed attention for developing and open sourcing a technique that allows large language models (LLMs) — like those underpinning ChatGPT and most modern AI chatbots — to improve themselves by generating synthetic data to fine-tune upon. The technique, known as SEAL (Self-Adapting LLMs), was first described in a paper published back in June and covered by VentureBeat at the time.A significantly expanded and updated version of the paper was released last month, as well as open source code posted on Github (under an MIT License, allowing for commercial and enterprise usage), and is making new waves among AI power users on the social network X this week.SEAL allows LLMs to autonomously generate and apply their own fine-tuning strategies. Unlike conventional models that rely on fixed external data and human-crafted optimization pipelines, SEAL enables models to evolve by producing their own synthetic training data and corresponding optimization directives.The development comes from a team affiliated with MIT’s Improbable AI
We keep talking about AI agents, but do we ever know what they are? (https://venturebeat.com/ai/we-keep-talking-about-ai-agents-but-do-we-ever-know-what-they-are): Imagine you do two things on a Monday morning.First, you ask a chatbot to summarize your new emails. Next, you ask an AI tool to figure out why your top competitor grew so fast last quarter. The AI silently gets to work. It scours financial reports, news articles and social media sentiment. It cross-references that data with your internal sales numbers, drafts a strategy outlining three potential reasons for the competitor's success and schedules a 30-minute meeting with your team to present its findings.We're calling both of these &quot;AI agents,&quot; but they represent worlds of difference in intelligence, capability and the level of trust we place in them. This ambiguity creates a fog that makes it difficult to build, evaluate, and safely govern these powerful new tools. If we can't agree on what we're building, how can we know when we've succeeded?This post won't try to sell you on yet another definitive framework. Instead, think of it as a survey of the current landscape of agent autonomy, a map to help us all navigate the terrain together.What are we even talking about? Defining an &quot;AI agent&quot;Befo
Whisper Leak: A novel side-channel attack on remote language models (https://www.microsoft.com/en-us/security/blog/2025/11/07/whisper-leak-a-novel-side-channel-cyberattack-on-remote-language-models/): Microsoft has identified a novel side-channel attack on remote language models, dubbed &quot;Whisper Leak,&quot; which exploits network packet sizes and timings to infer conversation topics despite end-to-end encryption. This attack allows adversaries to observe encrypted traffic and deduce sensitive information, posing risks in oppressive regimes or sensitive sectors like healthcare and law. The attack leverages the autoregressive nature of language models, which generate responses in sequential steps, making it possible to infer topics based on packet size variations. Microsoft collaborated with multiple vendors to mitigate these risks, implementing privacy enhancements in their language model frameworks. The research highlights the importance of robust anonymization and encryption techniques to safeguard user data in AI-driven interactions.</description>
      <itunes:summary>Articles discussed
Self-improving language models are becoming reality with MIT's updated SEAL technique (https://venturebeat.com/ai/self-improving-language-models-are-becoming-reality-with-mits-updated-seal): Researchers at the Massachusetts Institute of Technology (MIT) are gaining renewed attention for developing and open sourcing a technique that allows large language models (LLMs) — like those underpinning ChatGPT and most modern AI chatbots — to improve themselves by generating synthetic data to fine-tune upon. The technique, known as SEAL (Self-Adapting LLMs), was first described in a paper published back in June and covered by VentureBeat at the time.A significantly expanded and updated version of the paper was released last month, as well as open source code posted on Github (under an MIT License, allowing for commercial and enterprise usage), and is making new waves among AI power users on the social network X this week.SEAL allows LLMs to autonomously generate and apply their own fine-tuning strategies. Unlike conventional models that rely on fixed external data and human-crafted optimization pipelines, SEAL enables models to evolve by producing their own synthetic training data and corresponding optimization directives.The development comes from a team affiliated with MIT’s Improbable AI
We keep talking about AI agents, but do we ever know what they are? (https://venturebeat.com/ai/we-keep-talking-about-ai-agents-but-do-we-ever-know-what-they-are): Imagine you do two things on a Monday morning.First, you ask a chatbot to summarize your new emails. Next, you ask an AI tool to figure out why your top competitor grew so fast last quarter. The AI silently gets to work. It scours financial reports, news articles and social media sentiment. It cross-references that data with your internal sales numbers, drafts a strategy outlining three potential reasons for the competitor's success and schedules a 30-minute meeting with your team to present its findings.We're calling both of these &quot;AI agents,&quot; but they represent worlds of difference in intelligence, capability and the level of trust we place in them. This ambiguity creates a fog that makes it difficult to build, evaluate, and safely govern these powerful new tools. If we can't agree on what we're building, how can we know when we've succeeded?This post won't try to sell you on yet another definitive framework. Instead, think of it as a survey of the current landscape of agent autonomy, a map to help us all navigate the terrain together.What are we even talking about? Defining an &quot;AI agent&quot;Befo
Whisper Leak: A novel side-channel attack on remote language models (https://www.microsoft.com/en-us/security/blog/2025/11/07/whisper-leak-a-novel-side-channel-cyberattack-on-remote-language-models/): Microsoft has identified a novel side-channel attack on remote language models, dubbed &quot;Whisper Leak,&quot; which exploits network packet sizes and timings to infer conversation topics despite end-to-end encryption. This attack allows adversaries to observe encrypted traffic and deduce sensitive information, posing risks in oppressive regimes or sensitive sectors like healthcare and law. The attack leverages the autoregressive nature of language models, which generate responses in sequential steps, making it possible to infer topics based on packet size variations. Microsoft collaborated with multiple vendors to mitigate these risks, implementing privacy enhancements in their language model frameworks. The research highlights the importance of robust anonymization and encryption techniques to safeguard user data in AI-driven interactions.</itunes:summary>
      <pubDate>Sat, 17 Jan 2026 15:58:59 GMT</pubDate>
      <enclosure url="https://raw.githubusercontent.com/Refugeek/podcast-feed/main/episodes-EScyber/20251107214816-self-improving-language-models-are-becoming-reality-with-mit-s-updated-seal-technique-we-keep-talking-about-ai-agents-but-do-we-ever-know-what-they-are.mp3" type="audio/mpeg" length="35502284"/>
      <itunes:duration>1775</itunes:duration>
    </item>
    <item>
      <title>20251110021621-to-scale-agentic-ai-notion-tore-down-its-tech-stack-and-started-fresh-openai-announces-apps-sdk-allowing-chatgpt-to-launch-and-run-third-party-apps-like-zillow-canva-spotify</title>
      <description>Articles discussed
To scale agentic AI, Notion tore down its tech stack and started fresh (https://venturebeat.com/ai/to-scale-agentic-ai-notion-tore-down-its-tech-stack-and-started-fresh): Many organizations would be hesitant to overhaul their tech stack and start from scratch. Not Notion. For the 3.0 version of its productivity software (released in September), the company didn’t hesitate to rebuild from the ground up; they recognized that it was necessary, in fact, to support agentic AI at enterprise scale. Whereas traditional AI-powered workflows involve explicit, step-by-step instructions based on few-shot learning, AI agents powered by advanced reasoning models are thoughtful about tool definition, can identify and comprehend what tools they have at their disposal and plan next steps. “Rather than trying to retrofit into what we were building, we wanted to play to the strengths of reasoning models,” Sarah Sachs, Notion’s head of AI modeling, told VentureBeat. “We've rebuilt a new architecture because workflows are different from agents.”Re-orchestrating so models can work autonomouslyNotion has been adopted by 94% of Forbes AI 50 companies, has 100 million total users and counts among its customers OpenAI, Cursor, Figma, Ramp and Vercel. In a rapidly evolving AI lan
OpenAI announces Apps SDK allowing ChatGPT to launch and run third party apps like Zillow, Canva, Spotify (https://venturebeat.com/ai/openai-announces-apps-sdk-allowing-chatgpt-to-launch-and-run-third-party): OpenAI's annual conference for third-party developers, DevDay, kicked off with a bang today as co-founder and CEO Sam Altman announced a new &quot;Apps SDK&quot; that makes it &quot;possible to build apps inside of ChatGPT,&quot; including paid apps, which companies can charge users for using OpenAI's recently unveiled Agentic Commerce Protocol (ACP). In other words, instead of launching apps one-by-one on your phone, computer, or on the web — now you can do all that without ever leaving ChatGPT. This feature allows the user to log-into their accounts on those external apps and bring all their information back into ChatGPT, and use the apps very similarly to how they already do outside of the chatbot, but now with the ability to ask ChatGPT to perform certain actions, analyze content, or go beyond what each app could offer on its own. You can direct Canva to make you slides based on a text description, ask Zillow for home listings in a certain area fitting certain requirements, or as Coursera about a specific lesson's content while it plays on video, all from within ChatGPT — with many other apps also already offe
LLM-Powered Time-Series Analysis (https://towardsdatascience.com/llm-powered-time-series-analysis/): Large Language Models (LLMs) are revolutionizing time-series modeling by enabling advanced forecasting through prompt engineering. This approach allows analysts to efficiently set up, tune, and validate models using structured prompts, enhancing workflows for ARIMA, Prophet, and deep learning architectures like LSTMs and transformers. The method is grounded in research and real-world examples, offering practical prompts and tools for model development, validation, and interpretation. This innovation not only speeds up the modeling process but also ensures more accurate and reliable forecasts.
Previously unknown Landfall spyware used in 0-day attacks on Samsung phones (https://go.theregister.com/feed/www.theregister.com/2025/11/07/landfall_spyware_samsung_0days/): A previously unknown Android spyware family called LANDFALL exploited a zero-day in Samsung Galaxy devices for nearly a year, installing surveillance code capable of recording calls, tracking locations, and harvesting photos and logs before Samsung finally patched it in April. The surveillance campaign likely began in July 2024 and abused CVE-2025-21042, a critical bug in Samsung's image-processing library that affects Galaxy devices running Android versions 13, 14, 15, and 16, according to Palo Alto Networks Unit 42 researchers who discovered the commercial-grade spyware and revealed details of the espionage attacks. &quot;This was a precision espionage campaign, targeting specific Samsung Galaxy devices in the Middle East, with likely victims in Iraq, Iran, Turkey, and Morocco,&quot; Itay Cohen, a senior principal researcher at Unit 42, told The Register. The use of zero-day exploits, custom infrastructure, and modular payload design all indicate an espionage-motivated operation. According to the cyber sleuths, exploiting CVE-2025-21042 likely involved sending a maliciously crafted image to the victim's device via a messaging application in a &quot;zero-click&quot; attack, meaning that infecting targeted phones didn't require any user interaction.
How to Build Your Own Agentic AI System Using CrewAI (https://towardsdatascience.com/how-to-build-your-own-agentic-ai-system-using-crewai/): Agentic AI systems, which autonomously plan and execute complex tasks, have emerged as a transformative technology. These systems, distinguished by their autonomy, enable organizations to achieve 20% to 30% faster workflow cycles. Multi-Agent Systems (MAS) orchestrate Agent functionalities into dynamic workflows, enhancing performance by reducing noise and handling increased knowledge domains. CrewAI is an open-source Python framework that facilitates the development of production-ready AI agent teams. It emphasizes role-based multi-agent collaborations, offering less flexibility for complex agentic architectures compared to other frameworks like LangChain and LlamaIndex. Despite its relatively younger status, CrewAI is gaining traction since July 2025 due to its ease of implementation. The framework allows developers to create AI agent teams with specific roles and tasks, using tools to accomplish objectives. An example involves developing a Social Media Marketing Crew that generates blog posts and campaign messages tailored to user interests. This approach leverages CrewAI's capabilities to create a cohesive team of AI agents, each with defined roles and tools, to achieve specific marketing goals.</description>
      <itunes:summary>Articles discussed
To scale agentic AI, Notion tore down its tech stack and started fresh (https://venturebeat.com/ai/to-scale-agentic-ai-notion-tore-down-its-tech-stack-and-started-fresh): Many organizations would be hesitant to overhaul their tech stack and start from scratch. Not Notion. For the 3.0 version of its productivity software (released in September), the company didn’t hesitate to rebuild from the ground up; they recognized that it was necessary, in fact, to support agentic AI at enterprise scale. Whereas traditional AI-powered workflows involve explicit, step-by-step instructions based on few-shot learning, AI agents powered by advanced reasoning models are thoughtful about tool definition, can identify and comprehend what tools they have at their disposal and plan next steps. “Rather than trying to retrofit into what we were building, we wanted to play to the strengths of reasoning models,” Sarah Sachs, Notion’s head of AI modeling, told VentureBeat. “We've rebuilt a new architecture because workflows are different from agents.”Re-orchestrating so models can work autonomouslyNotion has been adopted by 94% of Forbes AI 50 companies, has 100 million total users and counts among its customers OpenAI, Cursor, Figma, Ramp and Vercel. In a rapidly evolving AI lan
OpenAI announces Apps SDK allowing ChatGPT to launch and run third party apps like Zillow, Canva, Spotify (https://venturebeat.com/ai/openai-announces-apps-sdk-allowing-chatgpt-to-launch-and-run-third-party): OpenAI's annual conference for third-party developers, DevDay, kicked off with a bang today as co-founder and CEO Sam Altman announced a new &quot;Apps SDK&quot; that makes it &quot;possible to build apps inside of ChatGPT,&quot; including paid apps, which companies can charge users for using OpenAI's recently unveiled Agentic Commerce Protocol (ACP). In other words, instead of launching apps one-by-one on your phone, computer, or on the web — now you can do all that without ever leaving ChatGPT. This feature allows the user to log-into their accounts on those external apps and bring all their information back into ChatGPT, and use the apps very similarly to how they already do outside of the chatbot, but now with the ability to ask ChatGPT to perform certain actions, analyze content, or go beyond what each app could offer on its own. You can direct Canva to make you slides based on a text description, ask Zillow for home listings in a certain area fitting certain requirements, or as Coursera about a specific lesson's content while it plays on video, all from within ChatGPT — with many other apps also already offe
LLM-Powered Time-Series Analysis (https://towardsdatascience.com/llm-powered-time-series-analysis/): Large Language Models (LLMs) are revolutionizing time-series modeling by enabling advanced forecasting through prompt engineering. This approach allows analysts to efficiently set up, tune, and validate models using structured prompts, enhancing workflows for ARIMA, Prophet, and deep learning architectures like LSTMs and transformers. The method is grounded in research and real-world examples, offering practical prompts and tools for model development, validation, and interpretation. This innovation not only speeds up the modeling process but also ensures more accurate and reliable forecasts.
Previously unknown Landfall spyware used in 0-day attacks on Samsung phones (https://go.theregister.com/feed/www.theregister.com/2025/11/07/landfall_spyware_samsung_0days/): A previously unknown Android spyware family called LANDFALL exploited a zero-day in Samsung Galaxy devices for nearly a year, installing surveillance code capable of recording calls, tracking locations, and harvesting photos and logs before Samsung finally patched it in April. The surveillance campaign likely began in July 2024 and abused CVE-2025-21042, a critical bug in Samsung's image-processing library that affects Galaxy devices running Android versions 13, 14, 15, and 16, according to Palo Alto Networks Unit 42 researchers who discovered the commercial-grade spyware and revealed details of the espionage attacks. &quot;This was a precision espionage campaign, targeting specific Samsung Galaxy devices in the Middle East, with likely victims in Iraq, Iran, Turkey, and Morocco,&quot; Itay Cohen, a senior principal researcher at Unit 42, told The Register. The use of zero-day exploits, custom infrastructure, and modular payload design all indicate an espionage-motivated operation. According to the cyber sleuths, exploiting CVE-2025-21042 likely involved sending a maliciously crafted image to the victim's device via a messaging application in a &quot;zero-click&quot; attack, meaning that infecting targeted phones didn't require any user interaction.
How to Build Your Own Agentic AI System Using CrewAI (https://towardsdatascience.com/how-to-build-your-own-agentic-ai-system-using-crewai/): Agentic AI systems, which autonomously plan and execute complex tasks, have emerged as a transformative technology. These systems, distinguished by their autonomy, enable organizations to achieve 20% to 30% faster workflow cycles. Multi-Agent Systems (MAS) orchestrate Agent functionalities into dynamic workflows, enhancing performance by reducing noise and handling increased knowledge domains. CrewAI is an open-source Python framework that facilitates the development of production-ready AI agent teams. It emphasizes role-based multi-agent collaborations, offering less flexibility for complex agentic architectures compared to other frameworks like LangChain and LlamaIndex. Despite its relatively younger status, CrewAI is gaining traction since July 2025 due to its ease of implementation. The framework allows developers to create AI agent teams with specific roles and tasks, using tools to accomplish objectives. An example involves developing a Social Media Marketing Crew that generates blog posts and campaign messages tailored to user interests. This approach leverages CrewAI's capabilities to create a cohesive team of AI agents, each with defined roles and tools, to achieve specific marketing goals.</itunes:summary>
      <description>Articles discussed
To scale agentic AI, Notion tore down its tech stack and started fresh (https://venturebeat.com/ai/to-scale-agentic-ai-notion-tore-down-its-tech-stack-and-started-fresh): Many organizations would be hesitant to overhaul their tech stack and start from scratch. Not Notion. For the 3.0 version of its productivity software (released in September), the company didn’t hesitate to rebuild from the ground up; they recognized that it was necessary, in fact, to support agentic AI at enterprise scale. Whereas traditional AI-powered workflows involve explicit, step-by-step instructions based on few-shot learning, AI agents powered by advanced reasoning models are thoughtful about tool definition, can identify and comprehend what tools they have at their disposal and plan next steps. “Rather than trying to retrofit into what we were building, we wanted to play to the strengths of reasoning models,” Sarah Sachs, Notion’s head of AI modeling, told VentureBeat. “We've rebuilt a new architecture because workflows are different from agents.”Re-orchestrating so models can work autonomouslyNotion has been adopted by 94% of Forbes AI 50 companies, has 100 million total users and counts among its customers OpenAI, Cursor, Figma, Ramp and Vercel. In a rapidly evolving AI lan
OpenAI announces Apps SDK allowing ChatGPT to launch and run third party apps like Zillow, Canva, Spotify (https://venturebeat.com/ai/openai-announces-apps-sdk-allowing-chatgpt-to-launch-and-run-third-party): OpenAI's annual conference for third-party developers, DevDay, kicked off with a bang today as co-founder and CEO Sam Altman announced a new &quot;Apps SDK&quot; that makes it &quot;possible to build apps inside of ChatGPT,&quot; including paid apps, which companies can charge users for using OpenAI's recently unveiled Agentic Commerce Protocol (ACP). In other words, instead of launching apps one-by-one on your phone, computer, or on the web — now you can do all that without ever leaving ChatGPT. This feature allows the user to log-into their accounts on those external apps and bring all their information back into ChatGPT, and use the apps very similarly to how they already do outside of the chatbot, but now with the ability to ask ChatGPT to perform certain actions, analyze content, or go beyond what each app could offer on its own. You can direct Canva to make you slides based on a text description, ask Zillow for home listings in a certain area fitting certain requirements, or as Coursera about a specific lesson's content while it plays on video, all from within ChatGPT — with many other apps also already offe
LLM-Powered Time-Series Analysis (https://towardsdatascience.com/llm-powered-time-series-analysis/): Large Language Models (LLMs) are revolutionizing time-series modeling by enabling advanced forecasting through prompt engineering. This approach allows analysts to efficiently set up, tune, and validate models using structured prompts, enhancing workflows for ARIMA, Prophet, and deep learning architectures like LSTMs and transformers. The method is grounded in research and real-world examples, offering practical prompts and tools for model development, validation, and interpretation. This innovation not only speeds up the modeling process but also ensures more accurate and reliable forecasts.
Previously unknown Landfall spyware used in 0-day attacks on Samsung phones (https://go.theregister.com/feed/www.theregister.com/2025/11/07/landfall_spyware_samsung_0days/): A previously unknown Android spyware family called LANDFALL exploited a zero-day in Samsung Galaxy devices for nearly a year, installing surveillance code capable of recording calls, tracking locations, and harvesting photos and logs before Samsung finally patched it in April. The surveillance campaign likely began in July 2024 and abused CVE-2025-21042, a critical bug in Samsung's image-processing library that affects Galaxy devices running Android versions 13, 14, 15, and 16, according to Palo Alto Networks Unit 42 researchers who discovered the commercial-grade spyware and revealed details of the espionage attacks. &quot;This was a precision espionage campaign, targeting specific Samsung Galaxy devices in the Middle East, with likely victims in Iraq, Iran, Turkey, and Morocco,&quot; Itay Cohen, a senior principal researcher at Unit 42, told The Register. The use of zero-day exploits, custom infrastructure, and modular payload design all indicate an espionage-motivated operation. According to the cyber sleuths, exploiting CVE-2025-21042 likely involved sending a maliciously crafted image to the victim's device via a messaging application in a &quot;zero-click&quot; attack, meaning that infecting targeted phones didn't require any user interaction.
How to Build Your Own Agentic AI System Using CrewAI (https://towardsdatascience.com/how-to-build-your-own-agentic-ai-system-using-crewai/): Agentic AI systems, which autonomously plan and execute complex tasks, have emerged as a transformative technology. These systems, distinguished by their autonomy, enable organizations to achieve 20% to 30% faster workflow cycles. Multi-Agent Systems (MAS) orchestrate Agent functionalities into dynamic workflows, enhancing performance by reducing noise and handling increased knowledge domains. CrewAI is an open-source Python framework that facilitates the development of production-ready AI agent teams. It emphasizes role-based multi-agent collaborations, offering less flexibility for complex agentic architectures compared to other frameworks like LangChain and LlamaIndex. Despite its relatively younger status, CrewAI is gaining traction since July 2025 due to its ease of implementation. The framework allows developers to create AI agent teams with specific roles and tasks, using tools to accomplish objectives. An example involves developing a Social Media Marketing Crew that generates blog posts and campaign messages tailored to user interests. This approach leverages CrewAI's capabilities to create a cohesive team of AI agents, each with defined roles and tools, to achieve specific marketing goals.</description>
      <itunes:summary>Articles discussed
To scale agentic AI, Notion tore down its tech stack and started fresh (https://venturebeat.com/ai/to-scale-agentic-ai-notion-tore-down-its-tech-stack-and-started-fresh): Many organizations would be hesitant to overhaul their tech stack and start from scratch. Not Notion. For the 3.0 version of its productivity software (released in September), the company didn’t hesitate to rebuild from the ground up; they recognized that it was necessary, in fact, to support agentic AI at enterprise scale. Whereas traditional AI-powered workflows involve explicit, step-by-step instructions based on few-shot learning, AI agents powered by advanced reasoning models are thoughtful about tool definition, can identify and comprehend what tools they have at their disposal and plan next steps. “Rather than trying to retrofit into what we were building, we wanted to play to the strengths of reasoning models,” Sarah Sachs, Notion’s head of AI modeling, told VentureBeat. “We've rebuilt a new architecture because workflows are different from agents.”Re-orchestrating so models can work autonomouslyNotion has been adopted by 94% of Forbes AI 50 companies, has 100 million total users and counts among its customers OpenAI, Cursor, Figma, Ramp and Vercel. In a rapidly evolving AI lan
OpenAI announces Apps SDK allowing ChatGPT to launch and run third party apps like Zillow, Canva, Spotify (https://venturebeat.com/ai/openai-announces-apps-sdk-allowing-chatgpt-to-launch-and-run-third-party): OpenAI's annual conference for third-party developers, DevDay, kicked off with a bang today as co-founder and CEO Sam Altman announced a new &quot;Apps SDK&quot; that makes it &quot;possible to build apps inside of ChatGPT,&quot; including paid apps, which companies can charge users for using OpenAI's recently unveiled Agentic Commerce Protocol (ACP). In other words, instead of launching apps one-by-one on your phone, computer, or on the web — now you can do all that without ever leaving ChatGPT. This feature allows the user to log-into their accounts on those external apps and bring all their information back into ChatGPT, and use the apps very similarly to how they already do outside of the chatbot, but now with the ability to ask ChatGPT to perform certain actions, analyze content, or go beyond what each app could offer on its own. You can direct Canva to make you slides based on a text description, ask Zillow for home listings in a certain area fitting certain requirements, or as Coursera about a specific lesson's content while it plays on video, all from within ChatGPT — with many other apps also already offe
LLM-Powered Time-Series Analysis (https://towardsdatascience.com/llm-powered-time-series-analysis/): Large Language Models (LLMs) are revolutionizing time-series modeling by enabling advanced forecasting through prompt engineering. This approach allows analysts to efficiently set up, tune, and validate models using structured prompts, enhancing workflows for ARIMA, Prophet, and deep learning architectures like LSTMs and transformers. The method is grounded in research and real-world examples, offering practical prompts and tools for model development, validation, and interpretation. This innovation not only speeds up the modeling process but also ensures more accurate and reliable forecasts.
Previously unknown Landfall spyware used in 0-day attacks on Samsung phones (https://go.theregister.com/feed/www.theregister.com/2025/11/07/landfall_spyware_samsung_0days/): A previously unknown Android spyware family called LANDFALL exploited a zero-day in Samsung Galaxy devices for nearly a year, installing surveillance code capable of recording calls, tracking locations, and harvesting photos and logs before Samsung finally patched it in April. The surveillance campaign likely began in July 2024 and abused CVE-2025-21042, a critical bug in Samsung's image-processing library that affects Galaxy devices running Android versions 13, 14, 15, and 16, according to Palo Alto Networks Unit 42 researchers who discovered the commercial-grade spyware and revealed details of the espionage attacks. &quot;This was a precision espionage campaign, targeting specific Samsung Galaxy devices in the Middle East, with likely victims in Iraq, Iran, Turkey, and Morocco,&quot; Itay Cohen, a senior principal researcher at Unit 42, told The Register. The use of zero-day exploits, custom infrastructure, and modular payload design all indicate an espionage-motivated operation. According to the cyber sleuths, exploiting CVE-2025-21042 likely involved sending a maliciously crafted image to the victim's device via a messaging application in a &quot;zero-click&quot; attack, meaning that infecting targeted phones didn't require any user interaction.
How to Build Your Own Agentic AI System Using CrewAI (https://towardsdatascience.com/how-to-build-your-own-agentic-ai-system-using-crewai/): Agentic AI systems, which autonomously plan and execute complex tasks, have emerged as a transformative technology. These systems, distinguished by their autonomy, enable organizations to achieve 20% to 30% faster workflow cycles. Multi-Agent Systems (MAS) orchestrate Agent functionalities into dynamic workflows, enhancing performance by reducing noise and handling increased knowledge domains. CrewAI is an open-source Python framework that facilitates the development of production-ready AI agent teams. It emphasizes role-based multi-agent collaborations, offering less flexibility for complex agentic architectures compared to other frameworks like LangChain and LlamaIndex. Despite its relatively younger status, CrewAI is gaining traction since July 2025 due to its ease of implementation. The framework allows developers to create AI agent teams with specific roles and tasks, using tools to accomplish objectives. An example involves developing a Social Media Marketing Crew that generates blog posts and campaign messages tailored to user interests. This approach leverages CrewAI's capabilities to create a cohesive team of AI agents, each with defined roles and tools, to achieve specific marketing goals.</itunes:summary>
      <pubDate>Sat, 17 Jan 2026 15:58:59 GMT</pubDate>
      <enclosure url="https://raw.githubusercontent.com/Refugeek/podcast-feed/main/episodes-EScyber/20251110021621-to-scale-agentic-ai-notion-tore-down-its-tech-stack-and-started-fresh-openai-announces-apps-sdk-allowing-chatgpt-to-launch-and-run-third-party-apps-like-zillow-canva-spotify.mp3" type="audio/mpeg" length="40890284"/>
      <itunes:duration>2044</itunes:duration>
    </item>
    <item>
      <title>20251111091548-databricks-set-to-accelerate-agentic-ai-by-up-to-100x-with-mooncake-technology-no-etl-pipelines-for-analytics-and-ai-you-thought-it-was-over-authentication-coercion-keeps-evolving</title>
      <description>Articles discussed
Databricks set to accelerate agentic AI by up to 100x with ‘Mooncake’ technology — no ETL pipelines for analytics and AI (https://venturebeat.com/data-infrastructure/databricks-set-to-accelerate-agentic-ai-by-up-to-100x-with-mooncake): Many enterprises running PostgreSQL databases for their applications face the same expensive reality. When they need to analyze that operational data or feed it to AI models, they build ETL (Extract, Transform, Load) data pipelines to move it into analytical systems. Those pipelines require dedicated data engineering teams, break frequently and create delays measured in hours or days between when data is written to a database and when it becomes available for analytics.For companies with large numbers of PostgreSQL instances, this infrastructure tax is massive. More critically, it wasn't designed for a world where AI agents generate and deploy applications at machine speed, creating new tables, events and workflows faster than any data engineering team can keep up.Databricks is making a bet that this architecture is fundamentally broken. The company is acquiring Mooncake, an early-stage startup focused on bridging PostgreSQL with lakehouse formats, to eliminate the need for ETL pipelines entirely. Financial terms of the deal are not being publicly disclosed. The technology promises to make oper
You Thought It Was Over? Authentication Coercion Keeps Evolving (https://unit42.paloaltonetworks.com/authentication-coercion/): Authentication coercion is a new threat that exploits Windows' inherent authentication protocols to force machines to authenticate to attacker-controlled systems. This attack method is particularly concerning because it can result in complete domain compromise, allowing attackers to steal sensitive data, deploy malware, and establish persistent access undetected for extended periods. The attack leverages a Windows feature that enables computers to execute procedures on remote machines, manipulating this feature to force machines, including critical Tier 0 assets, to authenticate to attacker-controlled systems. This attack exploits the design of legitimate authentication protocols in Microsoft Windows environments and requires no special permissions. Security researchers have documented the use of coercion tools such as PetitPotam (CVE-2021-36942) in actual attacks. Microsoft has issued security advisories acknowledging the exploitation potential of this CVE. This attack method is particularly concerning because it can result in complete domain compromise, allowing attackers to steal sensitive data, deploy malware, and establish persistent access undetected for extended periods.
LLM side-channel attack could allow snoops to guess what you're talking about (https://go.theregister.com/feed/www.theregister.com/2025/11/11/llm_sidechannel_attack_microsoft_researcher/): Microsoft researchers have developed a side-channel attack called Whisper Leak, which allows adversaries to infer the topics of encrypted LLM queries by analyzing packet size and timing patterns in streaming responses. This attack exploits the incremental nature of streaming models, making them susceptible to interception and analysis of network traffic. The researchers tested Whisper Leak against several providers, including Alibaba Qwen, Anthropic's Claude, Amazon Nova, DeepSeek, Lambda Labs, and Google's Gemini, achieving over 98% accuracy in distinguishing sensitive topics from normal traffic. While some providers have implemented mitigations, others have declined to fix the flaw. The attack highlights the risks posed by side-channel vulnerabilities in LLMs and underscores the need for improved security measures to protect sensitive information.</description>
      <itunes:summary>Articles discussed
Databricks set to accelerate agentic AI by up to 100x with ‘Mooncake’ technology — no ETL pipelines for analytics and AI (https://venturebeat.com/data-infrastructure/databricks-set-to-accelerate-agentic-ai-by-up-to-100x-with-mooncake): Many enterprises running PostgreSQL databases for their applications face the same expensive reality. When they need to analyze that operational data or feed it to AI models, they build ETL (Extract, Transform, Load) data pipelines to move it into analytical systems. Those pipelines require dedicated data engineering teams, break frequently and create delays measured in hours or days between when data is written to a database and when it becomes available for analytics.For companies with large numbers of PostgreSQL instances, this infrastructure tax is massive. More critically, it wasn't designed for a world where AI agents generate and deploy applications at machine speed, creating new tables, events and workflows faster than any data engineering team can keep up.Databricks is making a bet that this architecture is fundamentally broken. The company is acquiring Mooncake, an early-stage startup focused on bridging PostgreSQL with lakehouse formats, to eliminate the need for ETL pipelines entirely. Financial terms of the deal are not being publicly disclosed. The technology promises to make oper
You Thought It Was Over? Authentication Coercion Keeps Evolving (https://unit42.paloaltonetworks.com/authentication-coercion/): Authentication coercion is a new threat that exploits Windows' inherent authentication protocols to force machines to authenticate to attacker-controlled systems. This attack method is particularly concerning because it can result in complete domain compromise, allowing attackers to steal sensitive data, deploy malware, and establish persistent access undetected for extended periods. The attack leverages a Windows feature that enables computers to execute procedures on remote machines, manipulating this feature to force machines, including critical Tier 0 assets, to authenticate to attacker-controlled systems. This attack exploits the design of legitimate authentication protocols in Microsoft Windows environments and requires no special permissions. Security researchers have documented the use of coercion tools such as PetitPotam (CVE-2021-36942) in actual attacks. Microsoft has issued security advisories acknowledging the exploitation potential of this CVE. This attack method is particularly concerning because it can result in complete domain compromise, allowing attackers to steal sensitive data, deploy malware, and establish persistent access undetected for extended periods.
LLM side-channel attack could allow snoops to guess what you're talking about (https://go.theregister.com/feed/www.theregister.com/2025/11/11/llm_sidechannel_attack_microsoft_researcher/): Microsoft researchers have developed a side-channel attack called Whisper Leak, which allows adversaries to infer the topics of encrypted LLM queries by analyzing packet size and timing patterns in streaming responses. This attack exploits the incremental nature of streaming models, making them susceptible to interception and analysis of network traffic. The researchers tested Whisper Leak against several providers, including Alibaba Qwen, Anthropic's Claude, Amazon Nova, DeepSeek, Lambda Labs, and Google's Gemini, achieving over 98% accuracy in distinguishing sensitive topics from normal traffic. While some providers have implemented mitigations, others have declined to fix the flaw. The attack highlights the risks posed by side-channel vulnerabilities in LLMs and underscores the need for improved security measures to protect sensitive information.</itunes:summary>
      <description>Articles discussed
Databricks set to accelerate agentic AI by up to 100x with ‘Mooncake’ technology — no ETL pipelines for analytics and AI (https://venturebeat.com/data-infrastructure/databricks-set-to-accelerate-agentic-ai-by-up-to-100x-with-mooncake): Many enterprises running PostgreSQL databases for their applications face the same expensive reality. When they need to analyze that operational data or feed it to AI models, they build ETL (Extract, Transform, Load) data pipelines to move it into analytical systems. Those pipelines require dedicated data engineering teams, break frequently and create delays measured in hours or days between when data is written to a database and when it becomes available for analytics.For companies with large numbers of PostgreSQL instances, this infrastructure tax is massive. More critically, it wasn't designed for a world where AI agents generate and deploy applications at machine speed, creating new tables, events and workflows faster than any data engineering team can keep up.Databricks is making a bet that this architecture is fundamentally broken. The company is acquiring Mooncake, an early-stage startup focused on bridging PostgreSQL with lakehouse formats, to eliminate the need for ETL pipelines entirely. Financial terms of the deal are not being publicly disclosed. The technology promises to make oper
You Thought It Was Over? Authentication Coercion Keeps Evolving (https://unit42.paloaltonetworks.com/authentication-coercion/): Authentication coercion is a new threat that exploits Windows' inherent authentication protocols to force machines to authenticate to attacker-controlled systems. This attack method is particularly concerning because it can result in complete domain compromise, allowing attackers to steal sensitive data, deploy malware, and establish persistent access undetected for extended periods. The attack leverages a Windows feature that enables computers to execute procedures on remote machines, manipulating this feature to force machines, including critical Tier 0 assets, to authenticate to attacker-controlled systems. This attack exploits the design of legitimate authentication protocols in Microsoft Windows environments and requires no special permissions. Security researchers have documented the use of coercion tools such as PetitPotam (CVE-2021-36942) in actual attacks. Microsoft has issued security advisories acknowledging the exploitation potential of this CVE. This attack method is particularly concerning because it can result in complete domain compromise, allowing attackers to steal sensitive data, deploy malware, and establish persistent access undetected for extended periods.
LLM side-channel attack could allow snoops to guess what you're talking about (https://go.theregister.com/feed/www.theregister.com/2025/11/11/llm_sidechannel_attack_microsoft_researcher/): Microsoft researchers have developed a side-channel attack called Whisper Leak, which allows adversaries to infer the topics of encrypted LLM queries by analyzing packet size and timing patterns in streaming responses. This attack exploits the incremental nature of streaming models, making them susceptible to interception and analysis of network traffic. The researchers tested Whisper Leak against several providers, including Alibaba Qwen, Anthropic's Claude, Amazon Nova, DeepSeek, Lambda Labs, and Google's Gemini, achieving over 98% accuracy in distinguishing sensitive topics from normal traffic. While some providers have implemented mitigations, others have declined to fix the flaw. The attack highlights the risks posed by side-channel vulnerabilities in LLMs and underscores the need for improved security measures to protect sensitive information.</description>
      <itunes:summary>Articles discussed
Databricks set to accelerate agentic AI by up to 100x with ‘Mooncake’ technology — no ETL pipelines for analytics and AI (https://venturebeat.com/data-infrastructure/databricks-set-to-accelerate-agentic-ai-by-up-to-100x-with-mooncake): Many enterprises running PostgreSQL databases for their applications face the same expensive reality. When they need to analyze that operational data or feed it to AI models, they build ETL (Extract, Transform, Load) data pipelines to move it into analytical systems. Those pipelines require dedicated data engineering teams, break frequently and create delays measured in hours or days between when data is written to a database and when it becomes available for analytics.For companies with large numbers of PostgreSQL instances, this infrastructure tax is massive. More critically, it wasn't designed for a world where AI agents generate and deploy applications at machine speed, creating new tables, events and workflows faster than any data engineering team can keep up.Databricks is making a bet that this architecture is fundamentally broken. The company is acquiring Mooncake, an early-stage startup focused on bridging PostgreSQL with lakehouse formats, to eliminate the need for ETL pipelines entirely. Financial terms of the deal are not being publicly disclosed. The technology promises to make oper
You Thought It Was Over? Authentication Coercion Keeps Evolving (https://unit42.paloaltonetworks.com/authentication-coercion/): Authentication coercion is a new threat that exploits Windows' inherent authentication protocols to force machines to authenticate to attacker-controlled systems. This attack method is particularly concerning because it can result in complete domain compromise, allowing attackers to steal sensitive data, deploy malware, and establish persistent access undetected for extended periods. The attack leverages a Windows feature that enables computers to execute procedures on remote machines, manipulating this feature to force machines, including critical Tier 0 assets, to authenticate to attacker-controlled systems. This attack exploits the design of legitimate authentication protocols in Microsoft Windows environments and requires no special permissions. Security researchers have documented the use of coercion tools such as PetitPotam (CVE-2021-36942) in actual attacks. Microsoft has issued security advisories acknowledging the exploitation potential of this CVE. This attack method is particularly concerning because it can result in complete domain compromise, allowing attackers to steal sensitive data, deploy malware, and establish persistent access undetected for extended periods.
LLM side-channel attack could allow snoops to guess what you're talking about (https://go.theregister.com/feed/www.theregister.com/2025/11/11/llm_sidechannel_attack_microsoft_researcher/): Microsoft researchers have developed a side-channel attack called Whisper Leak, which allows adversaries to infer the topics of encrypted LLM queries by analyzing packet size and timing patterns in streaming responses. This attack exploits the incremental nature of streaming models, making them susceptible to interception and analysis of network traffic. The researchers tested Whisper Leak against several providers, including Alibaba Qwen, Anthropic's Claude, Amazon Nova, DeepSeek, Lambda Labs, and Google's Gemini, achieving over 98% accuracy in distinguishing sensitive topics from normal traffic. While some providers have implemented mitigations, others have declined to fix the flaw. The attack highlights the risks posed by side-channel vulnerabilities in LLMs and underscores the need for improved security measures to protect sensitive information.</itunes:summary>
      <pubDate>Sat, 17 Jan 2026 15:58:59 GMT</pubDate>
      <enclosure url="https://raw.githubusercontent.com/Refugeek/podcast-feed/main/episodes-EScyber/20251111091548-databricks-set-to-accelerate-agentic-ai-by-up-to-100x-with-mooncake-technology-no-etl-pipelines-for-analytics-and-ai-you-thought-it-was-over-authentication-coercion-keeps-evolving.mp3" type="audio/mpeg" length="29729804"/>
      <itunes:duration>1486</itunes:duration>
    </item>
    <item>
      <title>20251115121020-patch-now-samsung-zero-day-lets-attackers-take-over-your-phone-nhs-supplier-ends-probe-into-ransomware-attack-that-contributed-to-patient-death</title>
      <description>Articles discussed
Patch now: Samsung zero-day lets attackers take over your phone (https://www.malwarebytes.com/blog/news/2025/11/patch-now-samsung-zero-day-lets-attackers-take-over-your-phone): A critical vulnerability in Samsung mobile devices has exposed users to sophisticated cyberattacks. On November 10, 2025, the US Cybersecurity and Infrastructure Security Agency (CISA) added CVE-2025-21042 to its Known Exploited Vulnerabilities (KEV) catalog, signaling active exploitation. This vulnerability, an out-of-bounds write flaw in Samsung's image processing library, allows remote attackers to execute arbitrary code, potentially gaining complete control over the device without user interaction. Exploited as a remote code execution (RCE) zero-day, attackers deployed LANDFALL spyware on Galaxy devices in the Middle East using malformed Digital Negative (DNG) image files sent via WhatsApp. This attack chain is a &quot;zero-click&quot; attack, requiring no user action to compromise the device. Samsung patched the issue in April 2025, but CISA's warning highlights ongoing exploits, emphasizing the need for urgent patching and enhanced security measures.
NHS supplier ends probe into ransomware attack that contributed to patient death (https://go.theregister.com/feed/www.theregister.com/2025/11/13/synnovis_qilin_investigation/): Synnovis, a major NHS supplier, has concluded its 18-month investigation into a 2024 ransomware attack that disrupted pathology services across London. The Qilin ransomware gang, believed to be of Russian origin, claimed responsibility for the attack, which led to the cancellation of thousands of appointments and operations. The investigation revealed that the compromised data was unstructured and fragmented, posing a significant challenge for forensic teams. Security firm CaseMatrix estimated that over 900,000 NHS patient records were leaked, though Synnovis has not confirmed this figure. Notably, the disruption contributed to a patient's death, marking a rare instance of a ransomware incident linked to a fatality. Synnovis has not paid a ransom, citing ethical principles and rejecting funding for cybercriminal activities. The company has replaced all affected infrastructure and will notify NHS organizations by November 21, though individual patient notifications may take longer.
Anthropic claims of Claude AI-automated cyberattacks met with doubt (https://www.bleepingcomputer.com/news/security/anthropic-claims-of-claude-ai-automated-cyberattacks-met-with-doubt/): Anthropic reports that a Chinese state-sponsored threat group, tracked as GTG-1002, carried out a cyber-espionage operation that was largely automated through the abuse of the company's Claude Code AI model. [...]
China hates crypto and scams, but is now outraged USA acquired bitcoin from a scammer (https://go.theregister.com/feed/www.theregister.com/2025/11/12/cverc_prince_grou_scam_bitcoin/): A new theory from the agency that brought us ‘America hacked itself to blame Beijing’ China’s National Computer Virus Emergency Response Center (CVERC) has alleged a nation-state entity, probably the USA, was behind a 2020 attack on a bitcoin mining operation and by doing so has gone into bat for entities that Beijing usually blasts.…
EU's reforms of GDPR, AI slated by privacy activists for 'playing into Big Tech’s hands' (https://go.theregister.com/feed/www.theregister.com/2025/11/11/eu_leaked_gdpr_ai_reforms/): The European Commission plans to overhaul digital privacy legislation with the &quot;Digital Omnibus&quot; package, introducing amendments to AI regulation, cybersecurity, data protection, and privacy. Critics, including privacy group Noyb, argue these changes could undermine existing rules, benefiting Big Tech at the expense of user privacy. Proposed amendments include a loophole in GDPR that might allow companies to use personal data more freely for commercial purposes, weaken data access rights, and weaken protections for sensitive data. The EC claims these reforms aim to alleviate administrative burdens on small businesses, but critics see it as a distraction from the real issues. The proposals could influence U.S. tech policy, similar to how GDPR inspired the California Consumer Privacy Act. AI reforms are also under scrutiny, with concerns about granting AI systems special exemptions that could privilege risky technologies over others.
How to Build Agents with GPT-5 (https://towardsdatascience.com/how-to-build-agents-with-gpt-5/): OpenAI's GPT-5 model is being utilized to develop AI agents capable of answering user queries by accessing internal knowledge bases and utilizing various tools. This approach enhances the model's ability to automate workflows and improve data management processes. The method involves integrating GPT-5 with tools from OpenAI's ecosystem, such as vector storage and custom tools, to enable efficient data retrieval and user interaction. The impact of this development is significant, as it positions GPT-5 as a fundamental component in modern AI applications, driving innovation in information access and processing. Additionally, the novelty lies in the seamless integration of diverse tools within the GPT-5 framework, offering a flexible and powerful solution for complex AI tasks.</description>
      <itunes:summary>Articles discussed
Patch now: Samsung zero-day lets attackers take over your phone (https://www.malwarebytes.com/blog/news/2025/11/patch-now-samsung-zero-day-lets-attackers-take-over-your-phone): A critical vulnerability in Samsung mobile devices has exposed users to sophisticated cyberattacks. On November 10, 2025, the US Cybersecurity and Infrastructure Security Agency (CISA) added CVE-2025-21042 to its Known Exploited Vulnerabilities (KEV) catalog, signaling active exploitation. This vulnerability, an out-of-bounds write flaw in Samsung's image processing library, allows remote attackers to execute arbitrary code, potentially gaining complete control over the device without user interaction. Exploited as a remote code execution (RCE) zero-day, attackers deployed LANDFALL spyware on Galaxy devices in the Middle East using malformed Digital Negative (DNG) image files sent via WhatsApp. This attack chain is a &quot;zero-click&quot; attack, requiring no user action to compromise the device. Samsung patched the issue in April 2025, but CISA's warning highlights ongoing exploits, emphasizing the need for urgent patching and enhanced security measures.
NHS supplier ends probe into ransomware attack that contributed to patient death (https://go.theregister.com/feed/www.theregister.com/2025/11/13/synnovis_qilin_investigation/): Synnovis, a major NHS supplier, has concluded its 18-month investigation into a 2024 ransomware attack that disrupted pathology services across London. The Qilin ransomware gang, believed to be of Russian origin, claimed responsibility for the attack, which led to the cancellation of thousands of appointments and operations. The investigation revealed that the compromised data was unstructured and fragmented, posing a significant challenge for forensic teams. Security firm CaseMatrix estimated that over 900,000 NHS patient records were leaked, though Synnovis has not confirmed this figure. Notably, the disruption contributed to a patient's death, marking a rare instance of a ransomware incident linked to a fatality. Synnovis has not paid a ransom, citing ethical principles and rejecting funding for cybercriminal activities. The company has replaced all affected infrastructure and will notify NHS organizations by November 21, though individual patient notifications may take longer.
Anthropic claims of Claude AI-automated cyberattacks met with doubt (https://www.bleepingcomputer.com/news/security/anthropic-claims-of-claude-ai-automated-cyberattacks-met-with-doubt/): Anthropic reports that a Chinese state-sponsored threat group, tracked as GTG-1002, carried out a cyber-espionage operation that was largely automated through the abuse of the company's Claude Code AI model. [...]
China hates crypto and scams, but is now outraged USA acquired bitcoin from a scammer (https://go.theregister.com/feed/www.theregister.com/2025/11/12/cverc_prince_grou_scam_bitcoin/): A new theory from the agency that brought us ‘America hacked itself to blame Beijing’ China’s National Computer Virus Emergency Response Center (CVERC) has alleged a nation-state entity, probably the USA, was behind a 2020 attack on a bitcoin mining operation and by doing so has gone into bat for entities that Beijing usually blasts.…
EU's reforms of GDPR, AI slated by privacy activists for 'playing into Big Tech’s hands' (https://go.theregister.com/feed/www.theregister.com/2025/11/11/eu_leaked_gdpr_ai_reforms/): The European Commission plans to overhaul digital privacy legislation with the &quot;Digital Omnibus&quot; package, introducing amendments to AI regulation, cybersecurity, data protection, and privacy. Critics, including privacy group Noyb, argue these changes could undermine existing rules, benefiting Big Tech at the expense of user privacy. Proposed amendments include a loophole in GDPR that might allow companies to use personal data more freely for commercial purposes, weaken data access rights, and weaken protections for sensitive data. The EC claims these reforms aim to alleviate administrative burdens on small businesses, but critics see it as a distraction from the real issues. The proposals could influence U.S. tech policy, similar to how GDPR inspired the California Consumer Privacy Act. AI reforms are also under scrutiny, with concerns about granting AI systems special exemptions that could privilege risky technologies over others.
How to Build Agents with GPT-5 (https://towardsdatascience.com/how-to-build-agents-with-gpt-5/): OpenAI's GPT-5 model is being utilized to develop AI agents capable of answering user queries by accessing internal knowledge bases and utilizing various tools. This approach enhances the model's ability to automate workflows and improve data management processes. The method involves integrating GPT-5 with tools from OpenAI's ecosystem, such as vector storage and custom tools, to enable efficient data retrieval and user interaction. The impact of this development is significant, as it positions GPT-5 as a fundamental component in modern AI applications, driving innovation in information access and processing. Additionally, the novelty lies in the seamless integration of diverse tools within the GPT-5 framework, offering a flexible and powerful solution for complex AI tasks.</itunes:summary>
      <description>Articles discussed
Patch now: Samsung zero-day lets attackers take over your phone (https://www.malwarebytes.com/blog/news/2025/11/patch-now-samsung-zero-day-lets-attackers-take-over-your-phone): A critical vulnerability in Samsung mobile devices has exposed users to sophisticated cyberattacks. On November 10, 2025, the US Cybersecurity and Infrastructure Security Agency (CISA) added CVE-2025-21042 to its Known Exploited Vulnerabilities (KEV) catalog, signaling active exploitation. This vulnerability, an out-of-bounds write flaw in Samsung's image processing library, allows remote attackers to execute arbitrary code, potentially gaining complete control over the device without user interaction. Exploited as a remote code execution (RCE) zero-day, attackers deployed LANDFALL spyware on Galaxy devices in the Middle East using malformed Digital Negative (DNG) image files sent via WhatsApp. This attack chain is a &quot;zero-click&quot; attack, requiring no user action to compromise the device. Samsung patched the issue in April 2025, but CISA's warning highlights ongoing exploits, emphasizing the need for urgent patching and enhanced security measures.
NHS supplier ends probe into ransomware attack that contributed to patient death (https://go.theregister.com/feed/www.theregister.com/2025/11/13/synnovis_qilin_investigation/): Synnovis, a major NHS supplier, has concluded its 18-month investigation into a 2024 ransomware attack that disrupted pathology services across London. The Qilin ransomware gang, believed to be of Russian origin, claimed responsibility for the attack, which led to the cancellation of thousands of appointments and operations. The investigation revealed that the compromised data was unstructured and fragmented, posing a significant challenge for forensic teams. Security firm CaseMatrix estimated that over 900,000 NHS patient records were leaked, though Synnovis has not confirmed this figure. Notably, the disruption contributed to a patient's death, marking a rare instance of a ransomware incident linked to a fatality. Synnovis has not paid a ransom, citing ethical principles and rejecting funding for cybercriminal activities. The company has replaced all affected infrastructure and will notify NHS organizations by November 21, though individual patient notifications may take longer.
Anthropic claims of Claude AI-automated cyberattacks met with doubt (https://www.bleepingcomputer.com/news/security/anthropic-claims-of-claude-ai-automated-cyberattacks-met-with-doubt/): Anthropic reports that a Chinese state-sponsored threat group, tracked as GTG-1002, carried out a cyber-espionage operation that was largely automated through the abuse of the company's Claude Code AI model. [...]
China hates crypto and scams, but is now outraged USA acquired bitcoin from a scammer (https://go.theregister.com/feed/www.theregister.com/2025/11/12/cverc_prince_grou_scam_bitcoin/): A new theory from the agency that brought us ‘America hacked itself to blame Beijing’ China’s National Computer Virus Emergency Response Center (CVERC) has alleged a nation-state entity, probably the USA, was behind a 2020 attack on a bitcoin mining operation and by doing so has gone into bat for entities that Beijing usually blasts.…
EU's reforms of GDPR, AI slated by privacy activists for 'playing into Big Tech’s hands' (https://go.theregister.com/feed/www.theregister.com/2025/11/11/eu_leaked_gdpr_ai_reforms/): The European Commission plans to overhaul digital privacy legislation with the &quot;Digital Omnibus&quot; package, introducing amendments to AI regulation, cybersecurity, data protection, and privacy. Critics, including privacy group Noyb, argue these changes could undermine existing rules, benefiting Big Tech at the expense of user privacy. Proposed amendments include a loophole in GDPR that might allow companies to use personal data more freely for commercial purposes, weaken data access rights, and weaken protections for sensitive data. The EC claims these reforms aim to alleviate administrative burdens on small businesses, but critics see it as a distraction from the real issues. The proposals could influence U.S. tech policy, similar to how GDPR inspired the California Consumer Privacy Act. AI reforms are also under scrutiny, with concerns about granting AI systems special exemptions that could privilege risky technologies over others.
How to Build Agents with GPT-5 (https://towardsdatascience.com/how-to-build-agents-with-gpt-5/): OpenAI's GPT-5 model is being utilized to develop AI agents capable of answering user queries by accessing internal knowledge bases and utilizing various tools. This approach enhances the model's ability to automate workflows and improve data management processes. The method involves integrating GPT-5 with tools from OpenAI's ecosystem, such as vector storage and custom tools, to enable efficient data retrieval and user interaction. The impact of this development is significant, as it positions GPT-5 as a fundamental component in modern AI applications, driving innovation in information access and processing. Additionally, the novelty lies in the seamless integration of diverse tools within the GPT-5 framework, offering a flexible and powerful solution for complex AI tasks.</description>
      <itunes:summary>Articles discussed
Patch now: Samsung zero-day lets attackers take over your phone (https://www.malwarebytes.com/blog/news/2025/11/patch-now-samsung-zero-day-lets-attackers-take-over-your-phone): A critical vulnerability in Samsung mobile devices has exposed users to sophisticated cyberattacks. On November 10, 2025, the US Cybersecurity and Infrastructure Security Agency (CISA) added CVE-2025-21042 to its Known Exploited Vulnerabilities (KEV) catalog, signaling active exploitation. This vulnerability, an out-of-bounds write flaw in Samsung's image processing library, allows remote attackers to execute arbitrary code, potentially gaining complete control over the device without user interaction. Exploited as a remote code execution (RCE) zero-day, attackers deployed LANDFALL spyware on Galaxy devices in the Middle East using malformed Digital Negative (DNG) image files sent via WhatsApp. This attack chain is a &quot;zero-click&quot; attack, requiring no user action to compromise the device. Samsung patched the issue in April 2025, but CISA's warning highlights ongoing exploits, emphasizing the need for urgent patching and enhanced security measures.
NHS supplier ends probe into ransomware attack that contributed to patient death (https://go.theregister.com/feed/www.theregister.com/2025/11/13/synnovis_qilin_investigation/): Synnovis, a major NHS supplier, has concluded its 18-month investigation into a 2024 ransomware attack that disrupted pathology services across London. The Qilin ransomware gang, believed to be of Russian origin, claimed responsibility for the attack, which led to the cancellation of thousands of appointments and operations. The investigation revealed that the compromised data was unstructured and fragmented, posing a significant challenge for forensic teams. Security firm CaseMatrix estimated that over 900,000 NHS patient records were leaked, though Synnovis has not confirmed this figure. Notably, the disruption contributed to a patient's death, marking a rare instance of a ransomware incident linked to a fatality. Synnovis has not paid a ransom, citing ethical principles and rejecting funding for cybercriminal activities. The company has replaced all affected infrastructure and will notify NHS organizations by November 21, though individual patient notifications may take longer.
Anthropic claims of Claude AI-automated cyberattacks met with doubt (https://www.bleepingcomputer.com/news/security/anthropic-claims-of-claude-ai-automated-cyberattacks-met-with-doubt/): Anthropic reports that a Chinese state-sponsored threat group, tracked as GTG-1002, carried out a cyber-espionage operation that was largely automated through the abuse of the company's Claude Code AI model. [...]
China hates crypto and scams, but is now outraged USA acquired bitcoin from a scammer (https://go.theregister.com/feed/www.theregister.com/2025/11/12/cverc_prince_grou_scam_bitcoin/): A new theory from the agency that brought us ‘America hacked itself to blame Beijing’ China’s National Computer Virus Emergency Response Center (CVERC) has alleged a nation-state entity, probably the USA, was behind a 2020 attack on a bitcoin mining operation and by doing so has gone into bat for entities that Beijing usually blasts.…
EU's reforms of GDPR, AI slated by privacy activists for 'playing into Big Tech’s hands' (https://go.theregister.com/feed/www.theregister.com/2025/11/11/eu_leaked_gdpr_ai_reforms/): The European Commission plans to overhaul digital privacy legislation with the &quot;Digital Omnibus&quot; package, introducing amendments to AI regulation, cybersecurity, data protection, and privacy. Critics, including privacy group Noyb, argue these changes could undermine existing rules, benefiting Big Tech at the expense of user privacy. Proposed amendments include a loophole in GDPR that might allow companies to use personal data more freely for commercial purposes, weaken data access rights, and weaken protections for sensitive data. The EC claims these reforms aim to alleviate administrative burdens on small businesses, but critics see it as a distraction from the real issues. The proposals could influence U.S. tech policy, similar to how GDPR inspired the California Consumer Privacy Act. AI reforms are also under scrutiny, with concerns about granting AI systems special exemptions that could privilege risky technologies over others.
How to Build Agents with GPT-5 (https://towardsdatascience.com/how-to-build-agents-with-gpt-5/): OpenAI's GPT-5 model is being utilized to develop AI agents capable of answering user queries by accessing internal knowledge bases and utilizing various tools. This approach enhances the model's ability to automate workflows and improve data management processes. The method involves integrating GPT-5 with tools from OpenAI's ecosystem, such as vector storage and custom tools, to enable efficient data retrieval and user interaction. The impact of this development is significant, as it positions GPT-5 as a fundamental component in modern AI applications, driving innovation in information access and processing. Additionally, the novelty lies in the seamless integration of diverse tools within the GPT-5 framework, offering a flexible and powerful solution for complex AI tasks.</itunes:summary>
      <pubDate>Sat, 17 Jan 2026 15:58:59 GMT</pubDate>
      <enclosure url="https://raw.githubusercontent.com/Refugeek/podcast-feed/main/episodes-EScyber/20251115121020-patch-now-samsung-zero-day-lets-attackers-take-over-your-phone-nhs-supplier-ends-probe-into-ransomware-attack-that-contributed-to-patient-death.mp3" type="audio/mpeg" length="41913164"/>
      <itunes:duration>2095</itunes:duration>
    </item>
    <item>
      <title>20251127133255-i-measured-neural-network-training-every-5-steps-for-10000-iterations-collaborative-research-by-microsoft-and-nvidia-on-real-time-immunity</title>
      <description>Articles discussed
I Measured Neural Network Training Every 5 Steps for 10,000 Iterations (https://towardsdatascience.com/i-measured-neural-network-training-every-5-steps-for-10000-iterations/): Image by Pixabay.com The post I Measured Neural Network Training Every 5 Steps for 10,000 Iterations appeared first on Towards Data Science.
Collaborative research by Microsoft and NVIDIA on real-time immunity (https://techcommunity.microsoft.com/blog/microsoft-security-blog/collaborative-research-by-microsoft-and-nvidia-on-real-time-immunity/4470164): Read about Microsoft and NVIDIA joint research on real-time immunity. The post Collaborative research by Microsoft and NVIDIA on real-time immunity appeared first on Microsoft Security Blog.
Collaborative research by Microsoft and NVIDIA on real-time immunity (https://techcommunity.microsoft.com/blog/microsoft-security-blog/collaborative-research-by-microsoft-and-nvidia-on-real-time-immunity/4470164): Microsoft and NVIDIA have developed a real-time AI-powered cybersecurity system that uses Adversarial Learning to detect and respond to AI-driven cyber threats. The system leverages NVIDIA's GPU-accelerated computing to deliver scalable, adaptive protection against evolving cyber threats. The system achieves real-time security by scaling transformer-based architectures and optimizing them for ultra-low-latency inference at massive scale. This is achieved through joint engineering efforts, including NVIDIA's custom GPU kernel and tokenizer improvements, which gave Microsoft the headroom needed to deliver real-time protection at enterprise scale. The system has demonstrated significant performance boosts compared to standard GPU solutions, reducing forward-pass latency from 9.45 ms to 3.39 ms. This represents a 2.8× speedup and contributed 6.06 ms of the total 10.13 ms end-to-end latency reduction reported in the performance breakdown above. The system's innovations include adversarial learning pipeline, model distillation and architecture, and security-specific input segmentation. These enhancements laid the foundation for high-precision detection and enabling AI models that can generalize across diverse threat variants. Overall, the NVIDIA inference optimization led to significant performance boosts compared to standard GPU solutions, reducing forward-pass latency from 9.45 ms to 3.39 ms. This represents a 2.8× speedup and contributed 6.06 ms of the total 10.13 ms end-to-end
Why CrewAI’s Manager-Worker Architecture Fails — and How to Fix It (https://towardsdatascience.com/why-crewais-manager-worker-architecture-fails-and-how-to-fix-it/): A real-world analysis of why CrewAI’s hierarchical orchestration misfires—and a practical fix you can implement today. The post Why CrewAI’s Manager-Worker Architecture Fails — and How to Fix It appeared first on Towards Data Science.
HashJack attack shows AI browsers can be fooled with a simple ‘#’ (https://go.theregister.com/feed/www.theregister.com/2025/11/25/hashjack_attack_ai_browser_hashtag/): Cato Networks has identified a new attack method called HashJack, which exploits AI browsers to execute malicious prompts by hiding them within legitimate URLs. This technique, which involves appending a &quot;#&quot; to a URL and embedding malicious instructions after it, bypasses traditional network and server defenses, making it particularly dangerous as it can manipulate AI browser assistants like Copilot in Edge, Gemini in Chrome, and Comet from Perplexity AI. HashJack can lead to severe outcomes such as data exfiltration, phishing, misinformation, and even medical harm. The attack highlights the vulnerabilities of AI browsers, which are increasingly integrated into mainstream web browsing experiences. While Google and Microsoft acknowledged the threat, they have yet to implement fixes, underscoring the need for comprehensive security measures beyond traditional defenses. The discovery emphasizes the importance of layered defenses, including AI governance and monitoring client-side interactions, as AI browsers become more prevalent.
Tor switches to new Counter Galois Onion relay encryption algorithm (https://www.bleepingcomputer.com/news/security/tor-switches-to-new-counter-galois-onion-relay-encryption-algorithm/): Tor has announced the implementation of Counter Galois Onion (CGO), a new encryption algorithm designed to enhance the security and resilience of its network against modern traffic-interception attacks. Developed by cryptography researchers Jean Paul Degabriele, Alessandro Melloni, Jean-Pierre Münch, and Martijn Stam, CGO addresses several vulnerabilities in the previous Tor1 algorithm, including malleable relay encryption, partial forward secrecy, and weak authentication methods. The new system employs a Rugged Pseudorandom Permutation (RPRP) construction called UIV+, offering improved tagging protection, forward secrecy, and authentication while reducing bandwidth overhead and ensuring circuit integrity. CGO is currently being integrated into the Tor browser and its Rust-based client, Arti, with the feature marked as experimental. Users of the Tor Browser will automatically benefit from the upgrade once the system is fully deployed, though a specific timeline for this transition has not been provided.
Amazon security boss: Hostile countries use cyber targeting for physical military strikes (https://go.theregister.com/feed/www.theregister.com/2025/11/19/amazon_cso_warfare_cyber_kinetic/): Cyber-kinetic warfare has emerged as a new operational model, where cyber operations are used to scope out targets before launching physical military strikes. This has been observed in the actions of Iran's Imperial Kitten and MuddyWater cyber threat groups, which used digital reconnaissance to prepare for physical attacks. These groups have compromised maritime vessels' Automatic Identification Systems (AIS) platforms, accessed CCTV cameras, and conducted targeted searches for AIS location data. The IRGC's cyber arm began conducting targeted searches for AIS location data for a specific shipping vessel in January 2024, leading to a missile strike by Houthi forces against that ship on February 1, 2024. This highlights the unmistakable correlation between cyber reconnaissance and kinetic strikes. Additionally, Russia has been reported to have hacked into surveillance cameras to coordinate its attack on Kyiv, and China has been seen compromising critical infrastructure in Guam.</description>
      <itunes:summary>Articles discussed
I Measured Neural Network Training Every 5 Steps for 10,000 Iterations (https://towardsdatascience.com/i-measured-neural-network-training-every-5-steps-for-10000-iterations/): Image by Pixabay.com The post I Measured Neural Network Training Every 5 Steps for 10,000 Iterations appeared first on Towards Data Science.
Collaborative research by Microsoft and NVIDIA on real-time immunity (https://techcommunity.microsoft.com/blog/microsoft-security-blog/collaborative-research-by-microsoft-and-nvidia-on-real-time-immunity/4470164): Read about Microsoft and NVIDIA joint research on real-time immunity. The post Collaborative research by Microsoft and NVIDIA on real-time immunity appeared first on Microsoft Security Blog.
Collaborative research by Microsoft and NVIDIA on real-time immunity (https://techcommunity.microsoft.com/blog/microsoft-security-blog/collaborative-research-by-microsoft-and-nvidia-on-real-time-immunity/4470164): Microsoft and NVIDIA have developed a real-time AI-powered cybersecurity system that uses Adversarial Learning to detect and respond to AI-driven cyber threats. The system leverages NVIDIA's GPU-accelerated computing to deliver scalable, adaptive protection against evolving cyber threats. The system achieves real-time security by scaling transformer-based architectures and optimizing them for ultra-low-latency inference at massive scale. This is achieved through joint engineering efforts, including NVIDIA's custom GPU kernel and tokenizer improvements, which gave Microsoft the headroom needed to deliver real-time protection at enterprise scale. The system has demonstrated significant performance boosts compared to standard GPU solutions, reducing forward-pass latency from 9.45 ms to 3.39 ms. This represents a 2.8× speedup and contributed 6.06 ms of the total 10.13 ms end-to-end latency reduction reported in the performance breakdown above. The system's innovations include adversarial learning pipeline, model distillation and architecture, and security-specific input segmentation. These enhancements laid the foundation for high-precision detection and enabling AI models that can generalize across diverse threat variants. Overall, the NVIDIA inference optimization led to significant performance boosts compared to standard GPU solutions, reducing forward-pass latency from 9.45 ms to 3.39 ms. This represents a 2.8× speedup and contributed 6.06 ms of the total 10.13 ms end-to-end
Why CrewAI’s Manager-Worker Architecture Fails — and How to Fix It (https://towardsdatascience.com/why-crewais-manager-worker-architecture-fails-and-how-to-fix-it/): A real-world analysis of why CrewAI’s hierarchical orchestration misfires—and a practical fix you can implement today. The post Why CrewAI’s Manager-Worker Architecture Fails — and How to Fix It appeared first on Towards Data Science.
HashJack attack shows AI browsers can be fooled with a simple ‘#’ (https://go.theregister.com/feed/www.theregister.com/2025/11/25/hashjack_attack_ai_browser_hashtag/): Cato Networks has identified a new attack method called HashJack, which exploits AI browsers to execute malicious prompts by hiding them within legitimate URLs. This technique, which involves appending a &quot;#&quot; to a URL and embedding malicious instructions after it, bypasses traditional network and server defenses, making it particularly dangerous as it can manipulate AI browser assistants like Copilot in Edge, Gemini in Chrome, and Comet from Perplexity AI. HashJack can lead to severe outcomes such as data exfiltration, phishing, misinformation, and even medical harm. The attack highlights the vulnerabilities of AI browsers, which are increasingly integrated into mainstream web browsing experiences. While Google and Microsoft acknowledged the threat, they have yet to implement fixes, underscoring the need for comprehensive security measures beyond traditional defenses. The discovery emphasizes the importance of layered defenses, including AI governance and monitoring client-side interactions, as AI browsers become more prevalent.
Tor switches to new Counter Galois Onion relay encryption algorithm (https://www.bleepingcomputer.com/news/security/tor-switches-to-new-counter-galois-onion-relay-encryption-algorithm/): Tor has announced the implementation of Counter Galois Onion (CGO), a new encryption algorithm designed to enhance the security and resilience of its network against modern traffic-interception attacks. Developed by cryptography researchers Jean Paul Degabriele, Alessandro Melloni, Jean-Pierre Münch, and Martijn Stam, CGO addresses several vulnerabilities in the previous Tor1 algorithm, including malleable relay encryption, partial forward secrecy, and weak authentication methods. The new system employs a Rugged Pseudorandom Permutation (RPRP) construction called UIV+, offering improved tagging protection, forward secrecy, and authentication while reducing bandwidth overhead and ensuring circuit integrity. CGO is currently being integrated into the Tor browser and its Rust-based client, Arti, with the feature marked as experimental. Users of the Tor Browser will automatically benefit from the upgrade once the system is fully deployed, though a specific timeline for this transition has not been provided.
Amazon security boss: Hostile countries use cyber targeting for physical military strikes (https://go.theregister.com/feed/www.theregister.com/2025/11/19/amazon_cso_warfare_cyber_kinetic/): Cyber-kinetic warfare has emerged as a new operational model, where cyber operations are used to scope out targets before launching physical military strikes. This has been observed in the actions of Iran's Imperial Kitten and MuddyWater cyber threat groups, which used digital reconnaissance to prepare for physical attacks. These groups have compromised maritime vessels' Automatic Identification Systems (AIS) platforms, accessed CCTV cameras, and conducted targeted searches for AIS location data. The IRGC's cyber arm began conducting targeted searches for AIS location data for a specific shipping vessel in January 2024, leading to a missile strike by Houthi forces against that ship on February 1, 2024. This highlights the unmistakable correlation between cyber reconnaissance and kinetic strikes. Additionally, Russia has been reported to have hacked into surveillance cameras to coordinate its attack on Kyiv, and China has been seen compromising critical infrastructure in Guam.</itunes:summary>
      <description>Articles discussed
I Measured Neural Network Training Every 5 Steps for 10,000 Iterations (https://towardsdatascience.com/i-measured-neural-network-training-every-5-steps-for-10000-iterations/): Image by Pixabay.com The post I Measured Neural Network Training Every 5 Steps for 10,000 Iterations appeared first on Towards Data Science.
Collaborative research by Microsoft and NVIDIA on real-time immunity (https://techcommunity.microsoft.com/blog/microsoft-security-blog/collaborative-research-by-microsoft-and-nvidia-on-real-time-immunity/4470164): Read about Microsoft and NVIDIA joint research on real-time immunity. The post Collaborative research by Microsoft and NVIDIA on real-time immunity appeared first on Microsoft Security Blog.
Collaborative research by Microsoft and NVIDIA on real-time immunity (https://techcommunity.microsoft.com/blog/microsoft-security-blog/collaborative-research-by-microsoft-and-nvidia-on-real-time-immunity/4470164): Microsoft and NVIDIA have developed a real-time AI-powered cybersecurity system that uses Adversarial Learning to detect and respond to AI-driven cyber threats. The system leverages NVIDIA's GPU-accelerated computing to deliver scalable, adaptive protection against evolving cyber threats. The system achieves real-time security by scaling transformer-based architectures and optimizing them for ultra-low-latency inference at massive scale. This is achieved through joint engineering efforts, including NVIDIA's custom GPU kernel and tokenizer improvements, which gave Microsoft the headroom needed to deliver real-time protection at enterprise scale. The system has demonstrated significant performance boosts compared to standard GPU solutions, reducing forward-pass latency from 9.45 ms to 3.39 ms. This represents a 2.8× speedup and contributed 6.06 ms of the total 10.13 ms end-to-end latency reduction reported in the performance breakdown above. The system's innovations include adversarial learning pipeline, model distillation and architecture, and security-specific input segmentation. These enhancements laid the foundation for high-precision detection and enabling AI models that can generalize across diverse threat variants. Overall, the NVIDIA inference optimization led to significant performance boosts compared to standard GPU solutions, reducing forward-pass latency from 9.45 ms to 3.39 ms. This represents a 2.8× speedup and contributed 6.06 ms of the total 10.13 ms end-to-end
Why CrewAI’s Manager-Worker Architecture Fails — and How to Fix It (https://towardsdatascience.com/why-crewais-manager-worker-architecture-fails-and-how-to-fix-it/): A real-world analysis of why CrewAI’s hierarchical orchestration misfires—and a practical fix you can implement today. The post Why CrewAI’s Manager-Worker Architecture Fails — and How to Fix It appeared first on Towards Data Science.
HashJack attack shows AI browsers can be fooled with a simple ‘#’ (https://go.theregister.com/feed/www.theregister.com/2025/11/25/hashjack_attack_ai_browser_hashtag/): Cato Networks has identified a new attack method called HashJack, which exploits AI browsers to execute malicious prompts by hiding them within legitimate URLs. This technique, which involves appending a &quot;#&quot; to a URL and embedding malicious instructions after it, bypasses traditional network and server defenses, making it particularly dangerous as it can manipulate AI browser assistants like Copilot in Edge, Gemini in Chrome, and Comet from Perplexity AI. HashJack can lead to severe outcomes such as data exfiltration, phishing, misinformation, and even medical harm. The attack highlights the vulnerabilities of AI browsers, which are increasingly integrated into mainstream web browsing experiences. While Google and Microsoft acknowledged the threat, they have yet to implement fixes, underscoring the need for comprehensive security measures beyond traditional defenses. The discovery emphasizes the importance of layered defenses, including AI governance and monitoring client-side interactions, as AI browsers become more prevalent.
Tor switches to new Counter Galois Onion relay encryption algorithm (https://www.bleepingcomputer.com/news/security/tor-switches-to-new-counter-galois-onion-relay-encryption-algorithm/): Tor has announced the implementation of Counter Galois Onion (CGO), a new encryption algorithm designed to enhance the security and resilience of its network against modern traffic-interception attacks. Developed by cryptography researchers Jean Paul Degabriele, Alessandro Melloni, Jean-Pierre Münch, and Martijn Stam, CGO addresses several vulnerabilities in the previous Tor1 algorithm, including malleable relay encryption, partial forward secrecy, and weak authentication methods. The new system employs a Rugged Pseudorandom Permutation (RPRP) construction called UIV+, offering improved tagging protection, forward secrecy, and authentication while reducing bandwidth overhead and ensuring circuit integrity. CGO is currently being integrated into the Tor browser and its Rust-based client, Arti, with the feature marked as experimental. Users of the Tor Browser will automatically benefit from the upgrade once the system is fully deployed, though a specific timeline for this transition has not been provided.
Amazon security boss: Hostile countries use cyber targeting for physical military strikes (https://go.theregister.com/feed/www.theregister.com/2025/11/19/amazon_cso_warfare_cyber_kinetic/): Cyber-kinetic warfare has emerged as a new operational model, where cyber operations are used to scope out targets before launching physical military strikes. This has been observed in the actions of Iran's Imperial Kitten and MuddyWater cyber threat groups, which used digital reconnaissance to prepare for physical attacks. These groups have compromised maritime vessels' Automatic Identification Systems (AIS) platforms, accessed CCTV cameras, and conducted targeted searches for AIS location data. The IRGC's cyber arm began conducting targeted searches for AIS location data for a specific shipping vessel in January 2024, leading to a missile strike by Houthi forces against that ship on February 1, 2024. This highlights the unmistakable correlation between cyber reconnaissance and kinetic strikes. Additionally, Russia has been reported to have hacked into surveillance cameras to coordinate its attack on Kyiv, and China has been seen compromising critical infrastructure in Guam.</description>
      <itunes:summary>Articles discussed
I Measured Neural Network Training Every 5 Steps for 10,000 Iterations (https://towardsdatascience.com/i-measured-neural-network-training-every-5-steps-for-10000-iterations/): Image by Pixabay.com The post I Measured Neural Network Training Every 5 Steps for 10,000 Iterations appeared first on Towards Data Science.
Collaborative research by Microsoft and NVIDIA on real-time immunity (https://techcommunity.microsoft.com/blog/microsoft-security-blog/collaborative-research-by-microsoft-and-nvidia-on-real-time-immunity/4470164): Read about Microsoft and NVIDIA joint research on real-time immunity. The post Collaborative research by Microsoft and NVIDIA on real-time immunity appeared first on Microsoft Security Blog.
Collaborative research by Microsoft and NVIDIA on real-time immunity (https://techcommunity.microsoft.com/blog/microsoft-security-blog/collaborative-research-by-microsoft-and-nvidia-on-real-time-immunity/4470164): Microsoft and NVIDIA have developed a real-time AI-powered cybersecurity system that uses Adversarial Learning to detect and respond to AI-driven cyber threats. The system leverages NVIDIA's GPU-accelerated computing to deliver scalable, adaptive protection against evolving cyber threats. The system achieves real-time security by scaling transformer-based architectures and optimizing them for ultra-low-latency inference at massive scale. This is achieved through joint engineering efforts, including NVIDIA's custom GPU kernel and tokenizer improvements, which gave Microsoft the headroom needed to deliver real-time protection at enterprise scale. The system has demonstrated significant performance boosts compared to standard GPU solutions, reducing forward-pass latency from 9.45 ms to 3.39 ms. This represents a 2.8× speedup and contributed 6.06 ms of the total 10.13 ms end-to-end latency reduction reported in the performance breakdown above. The system's innovations include adversarial learning pipeline, model distillation and architecture, and security-specific input segmentation. These enhancements laid the foundation for high-precision detection and enabling AI models that can generalize across diverse threat variants. Overall, the NVIDIA inference optimization led to significant performance boosts compared to standard GPU solutions, reducing forward-pass latency from 9.45 ms to 3.39 ms. This represents a 2.8× speedup and contributed 6.06 ms of the total 10.13 ms end-to-end
Why CrewAI’s Manager-Worker Architecture Fails — and How to Fix It (https://towardsdatascience.com/why-crewais-manager-worker-architecture-fails-and-how-to-fix-it/): A real-world analysis of why CrewAI’s hierarchical orchestration misfires—and a practical fix you can implement today. The post Why CrewAI’s Manager-Worker Architecture Fails — and How to Fix It appeared first on Towards Data Science.
HashJack attack shows AI browsers can be fooled with a simple ‘#’ (https://go.theregister.com/feed/www.theregister.com/2025/11/25/hashjack_attack_ai_browser_hashtag/): Cato Networks has identified a new attack method called HashJack, which exploits AI browsers to execute malicious prompts by hiding them within legitimate URLs. This technique, which involves appending a &quot;#&quot; to a URL and embedding malicious instructions after it, bypasses traditional network and server defenses, making it particularly dangerous as it can manipulate AI browser assistants like Copilot in Edge, Gemini in Chrome, and Comet from Perplexity AI. HashJack can lead to severe outcomes such as data exfiltration, phishing, misinformation, and even medical harm. The attack highlights the vulnerabilities of AI browsers, which are increasingly integrated into mainstream web browsing experiences. While Google and Microsoft acknowledged the threat, they have yet to implement fixes, underscoring the need for comprehensive security measures beyond traditional defenses. The discovery emphasizes the importance of layered defenses, including AI governance and monitoring client-side interactions, as AI browsers become more prevalent.
Tor switches to new Counter Galois Onion relay encryption algorithm (https://www.bleepingcomputer.com/news/security/tor-switches-to-new-counter-galois-onion-relay-encryption-algorithm/): Tor has announced the implementation of Counter Galois Onion (CGO), a new encryption algorithm designed to enhance the security and resilience of its network against modern traffic-interception attacks. Developed by cryptography researchers Jean Paul Degabriele, Alessandro Melloni, Jean-Pierre Münch, and Martijn Stam, CGO addresses several vulnerabilities in the previous Tor1 algorithm, including malleable relay encryption, partial forward secrecy, and weak authentication methods. The new system employs a Rugged Pseudorandom Permutation (RPRP) construction called UIV+, offering improved tagging protection, forward secrecy, and authentication while reducing bandwidth overhead and ensuring circuit integrity. CGO is currently being integrated into the Tor browser and its Rust-based client, Arti, with the feature marked as experimental. Users of the Tor Browser will automatically benefit from the upgrade once the system is fully deployed, though a specific timeline for this transition has not been provided.
Amazon security boss: Hostile countries use cyber targeting for physical military strikes (https://go.theregister.com/feed/www.theregister.com/2025/11/19/amazon_cso_warfare_cyber_kinetic/): Cyber-kinetic warfare has emerged as a new operational model, where cyber operations are used to scope out targets before launching physical military strikes. This has been observed in the actions of Iran's Imperial Kitten and MuddyWater cyber threat groups, which used digital reconnaissance to prepare for physical attacks. These groups have compromised maritime vessels' Automatic Identification Systems (AIS) platforms, accessed CCTV cameras, and conducted targeted searches for AIS location data. The IRGC's cyber arm began conducting targeted searches for AIS location data for a specific shipping vessel in January 2024, leading to a missile strike by Houthi forces against that ship on February 1, 2024. This highlights the unmistakable correlation between cyber reconnaissance and kinetic strikes. Additionally, Russia has been reported to have hacked into surveillance cameras to coordinate its attack on Kyiv, and China has been seen compromising critical infrastructure in Guam.</itunes:summary>
      <pubDate>Sat, 17 Jan 2026 15:58:59 GMT</pubDate>
      <enclosure url="https://raw.githubusercontent.com/Refugeek/podcast-feed/main/episodes-EScyber/20251127133255-i-measured-neural-network-training-every-5-steps-for-10000-iterations-collaborative-research-by-microsoft-and-nvidia-on-real-time-immunity.mp3" type="audio/mpeg" length="32187404"/>
      <itunes:duration>1609</itunes:duration>
    </item>
    <item>
      <title>20251205095605-whispering-poetry-at-ai-can-make-it-break-its-own-rules-critical-react-next-js-flaw-lets-hackers-execute-code-on-servers</title>
      <description>Articles discussed
Whispering poetry at AI can make it break its own rules (https://www.malwarebytes.com/blog/news/2025/12/whispering-poetry-at-ai-can-make-it-break-its-own-rules): Researchers at Icaro Lab, Sapienza University, and DEXAI tested whether giving AI instructions as poetry would make it harder to detect different types of dangerous content. The researchers found that poetic elements such as metaphor, rhythm, and unconventional framing might disrupt pattern-matching heuristics that the AI’s guardrails rely on to spot harmful content. The tests covered models across nine providers, including all the usual suspects: Google, OpenAI, Anthropic, Deepseek, and Meta. One way the researchers calculated the scores was by measuring the attack success rate (ASR) across each provider’s models. The researchers found that DeepSeek (an open-source model developed by researchers in China) was the least safe, with a 62% ASR. Google was the second least safe. Anthropic, which produces Claude, was the safest, with a 6.95 ASR. OpenAI, which makes ChatGPT, was the second most safe.
Critical React, Next.js flaw lets hackers execute code on servers (https://www.bleepingcomputer.com/news/security/critical-react2shell-flaw-in-react-nextjs-lets-hackers-run-javascript-code/): ## Critical React, Next.js Flaw Lets Hackers Execute Code on Servers A critical vulnerability, dubbed 'React2Shell', in the React Server Components (RSC) 'Flight' protocol allows remote code execution without authentication in React and Next.js applications. The security issue stems from insecure deserialization. It received a severity score of 10/10 and has been assigned the identifiers CVE-2025-55182 for React and CVE-2025-66478 (CVE rejected in the National Vulnerability Database) for Next.js. Security researcher Lachlan Davidson discovered the flaw and reported it to React on November 29. He found that an attacker could achieve remote code execution (RCE) by sending a specially crafted HTTP request to React Server Function endpoints. &quot;Even if your app does not implement any React Server Function endpoints, it may still be vulnerable if your app supports React Server Components [RCS],&quot; warns the security advisory from React. The following packages in their default configuration are impacted: react-server-dom-parcel react-server-dom-turbopack and react-server-dom-webpack React is an open-source JavaScript library for building user interfaces. It's maintained by Meta and widely adopted by organizations of all sizes for front-end web development. Next.js, maintained by Vercel, is a framework built on top of React that adds server-side rendering, routing, and API endpoints. Both solutions are
'Exploitation is imminent' as 39 percent of cloud environs have max-severity React hole (https://go.theregister.com/feed/www.theregister.com/2025/12/03/exploitation_is_imminent_react_vulnerability/): A critical vulnerability in the JavaScript library React has been identified, affecting 39% of cloud environments. This flaw, tracked as CVE-2025-55182, allows unauthenticated, remote attackers to execute malicious code on vulnerable instances. The React team disclosed the vulnerability, recommending immediate upgrades to versions 19.0.1, 19.1.2, and 19.2.1 to mitigate the risk. Vercel, the maintainer of Next.js, also issued a patch and alert for the affected framework. Researchers reported the flaw to Meta, which collaborated with React to swiftly release an emergency patch. The vulnerability's high severity and ease of exploitation make immediate patching critical, with potential for widespread attention and public disclosure of technical details.
Prompt Injection Through Poetry (https://www.schneier.com/blog/archives/2025/11/prompt-injection-through-poetry.html): Researchers have demonstrated that poetic prompts can effectively jailbreak large language models (LLMs), achieving high attack success rates (ASR) across various models. This method, termed &quot;poetic jailbreaking,&quot; involves converting harmful prompts into verse, which bypasses contemporary safety mechanisms and highlights limitations in current alignment methods and evaluation protocols. The study utilized a curated dataset of 20 handcrafted adversarial poems in English and Italian, along with an additional 1,200 prompts from the MLCommons AILuminate Safety Benchmark, to test the effectiveness of poetic reframing in inducing aligned models to bypass refusal heuristics. The findings indicate that poetic framing can substantially outperform non-poetic baselines, with an average jailbreak success rate of 62% for hand-crafted poems and approximately 43% for meta-prompt conversions. This research underscores the need for improved safety mechanisms in LLMs to mitigate the risks posed by adversarial poetry.
North Korea lures engineers to rent identities in fake IT worker scheme (https://www.bleepingcomputer.com/news/security/north-korea-lures-engineers-to-rent-identities-in-fake-it-worker-scheme/): North Korean state-sponsored hackers, known as the Lazarus Group, have been using social engineering tactics to recruit engineers to rent their identities for illicit fundraising. The group uses stolen identities and AI, including deepfake videos, to infiltrate Western companies for espionage and revenue generation. The group uses a front-man engineer to pose as a figurehead in the operation to get a remote job at a targeted company. The front-man engineer would receive a percentage of the salary, between 20% and 35% for the duration of the contract. The compromised engineer takes all the risk as they rented their identity and will be the only one responsible for any damage done. The compromised engineer would have to let DPRK agents use their computer to hide the North Korean’s location and their traces.
Why We’ve Been Optimizing the Wrong Thing in LLMs for Years (https://towardsdatascience.com/why-weve-been-optimizing-the-wrong-thing-in-llms-for-years/): Researchers at Meta FAIR have introduced a novel approach to training Large Language Models (LLMs) called Multi-Token Prediction (MTP). This method leverages the latent capacity of LLMs to process multiple future tokens simultaneously, enhancing performance and inference speed. By dividing the model into a shared trunk and independent heads, MTP predicts future tokens concurrently, reducing inference time by up to three times. The architecture addresses memory bottlenecks through a sequential forward/backward pass strategy, maintaining similar batch sizes as standard models. Experimental results show MTP significantly outperforms traditional Next-Token Prediction (NTP) methods across various benchmarks. The study also explores design choices, such as parameter parity and head topology, revealing that parallel heads outperform causal heads in certain scenarios.
Malicious LLMs empower inexperienced hackers with advanced tools (https://www.bleepingcomputer.com/news/security/malicious-llms-empower-inexperienced-hackers-with-advanced-tools/): Unrestricted large language models (LLMs) like WormGPT 4 and KawaiiGPT are improving their capabilities to generate malicious code, delivering functional scripts for ransomware encryptors and lateral movement. [...]</description>
      <itunes:summary>Articles discussed
Whispering poetry at AI can make it break its own rules (https://www.malwarebytes.com/blog/news/2025/12/whispering-poetry-at-ai-can-make-it-break-its-own-rules): Researchers at Icaro Lab, Sapienza University, and DEXAI tested whether giving AI instructions as poetry would make it harder to detect different types of dangerous content. The researchers found that poetic elements such as metaphor, rhythm, and unconventional framing might disrupt pattern-matching heuristics that the AI’s guardrails rely on to spot harmful content. The tests covered models across nine providers, including all the usual suspects: Google, OpenAI, Anthropic, Deepseek, and Meta. One way the researchers calculated the scores was by measuring the attack success rate (ASR) across each provider’s models. The researchers found that DeepSeek (an open-source model developed by researchers in China) was the least safe, with a 62% ASR. Google was the second least safe. Anthropic, which produces Claude, was the safest, with a 6.95 ASR. OpenAI, which makes ChatGPT, was the second most safe.
Critical React, Next.js flaw lets hackers execute code on servers (https://www.bleepingcomputer.com/news/security/critical-react2shell-flaw-in-react-nextjs-lets-hackers-run-javascript-code/): ## Critical React, Next.js Flaw Lets Hackers Execute Code on Servers A critical vulnerability, dubbed 'React2Shell', in the React Server Components (RSC) 'Flight' protocol allows remote code execution without authentication in React and Next.js applications. The security issue stems from insecure deserialization. It received a severity score of 10/10 and has been assigned the identifiers CVE-2025-55182 for React and CVE-2025-66478 (CVE rejected in the National Vulnerability Database) for Next.js. Security researcher Lachlan Davidson discovered the flaw and reported it to React on November 29. He found that an attacker could achieve remote code execution (RCE) by sending a specially crafted HTTP request to React Server Function endpoints. &quot;Even if your app does not implement any React Server Function endpoints, it may still be vulnerable if your app supports React Server Components [RCS],&quot; warns the security advisory from React. The following packages in their default configuration are impacted: react-server-dom-parcel react-server-dom-turbopack and react-server-dom-webpack React is an open-source JavaScript library for building user interfaces. It's maintained by Meta and widely adopted by organizations of all sizes for front-end web development. Next.js, maintained by Vercel, is a framework built on top of React that adds server-side rendering, routing, and API endpoints. Both solutions are
'Exploitation is imminent' as 39 percent of cloud environs have max-severity React hole (https://go.theregister.com/feed/www.theregister.com/2025/12/03/exploitation_is_imminent_react_vulnerability/): A critical vulnerability in the JavaScript library React has been identified, affecting 39% of cloud environments. This flaw, tracked as CVE-2025-55182, allows unauthenticated, remote attackers to execute malicious code on vulnerable instances. The React team disclosed the vulnerability, recommending immediate upgrades to versions 19.0.1, 19.1.2, and 19.2.1 to mitigate the risk. Vercel, the maintainer of Next.js, also issued a patch and alert for the affected framework. Researchers reported the flaw to Meta, which collaborated with React to swiftly release an emergency patch. The vulnerability's high severity and ease of exploitation make immediate patching critical, with potential for widespread attention and public disclosure of technical details.
Prompt Injection Through Poetry (https://www.schneier.com/blog/archives/2025/11/prompt-injection-through-poetry.html): Researchers have demonstrated that poetic prompts can effectively jailbreak large language models (LLMs), achieving high attack success rates (ASR) across various models. This method, termed &quot;poetic jailbreaking,&quot; involves converting harmful prompts into verse, which bypasses contemporary safety mechanisms and highlights limitations in current alignment methods and evaluation protocols. The study utilized a curated dataset of 20 handcrafted adversarial poems in English and Italian, along with an additional 1,200 prompts from the MLCommons AILuminate Safety Benchmark, to test the effectiveness of poetic reframing in inducing aligned models to bypass refusal heuristics. The findings indicate that poetic framing can substantially outperform non-poetic baselines, with an average jailbreak success rate of 62% for hand-crafted poems and approximately 43% for meta-prompt conversions. This research underscores the need for improved safety mechanisms in LLMs to mitigate the risks posed by adversarial poetry.
North Korea lures engineers to rent identities in fake IT worker scheme (https://www.bleepingcomputer.com/news/security/north-korea-lures-engineers-to-rent-identities-in-fake-it-worker-scheme/): North Korean state-sponsored hackers, known as the Lazarus Group, have been using social engineering tactics to recruit engineers to rent their identities for illicit fundraising. The group uses stolen identities and AI, including deepfake videos, to infiltrate Western companies for espionage and revenue generation. The group uses a front-man engineer to pose as a figurehead in the operation to get a remote job at a targeted company. The front-man engineer would receive a percentage of the salary, between 20% and 35% for the duration of the contract. The compromised engineer takes all the risk as they rented their identity and will be the only one responsible for any damage done. The compromised engineer would have to let DPRK agents use their computer to hide the North Korean’s location and their traces.
Why We’ve Been Optimizing the Wrong Thing in LLMs for Years (https://towardsdatascience.com/why-weve-been-optimizing-the-wrong-thing-in-llms-for-years/): Researchers at Meta FAIR have introduced a novel approach to training Large Language Models (LLMs) called Multi-Token Prediction (MTP). This method leverages the latent capacity of LLMs to process multiple future tokens simultaneously, enhancing performance and inference speed. By dividing the model into a shared trunk and independent heads, MTP predicts future tokens concurrently, reducing inference time by up to three times. The architecture addresses memory bottlenecks through a sequential forward/backward pass strategy, maintaining similar batch sizes as standard models. Experimental results show MTP significantly outperforms traditional Next-Token Prediction (NTP) methods across various benchmarks. The study also explores design choices, such as parameter parity and head topology, revealing that parallel heads outperform causal heads in certain scenarios.
Malicious LLMs empower inexperienced hackers with advanced tools (https://www.bleepingcomputer.com/news/security/malicious-llms-empower-inexperienced-hackers-with-advanced-tools/): Unrestricted large language models (LLMs) like WormGPT 4 and KawaiiGPT are improving their capabilities to generate malicious code, delivering functional scripts for ransomware encryptors and lateral movement. [...]</itunes:summary>
      <description>Articles discussed
Whispering poetry at AI can make it break its own rules (https://www.malwarebytes.com/blog/news/2025/12/whispering-poetry-at-ai-can-make-it-break-its-own-rules): Researchers at Icaro Lab, Sapienza University, and DEXAI tested whether giving AI instructions as poetry would make it harder to detect different types of dangerous content. The researchers found that poetic elements such as metaphor, rhythm, and unconventional framing might disrupt pattern-matching heuristics that the AI’s guardrails rely on to spot harmful content. The tests covered models across nine providers, including all the usual suspects: Google, OpenAI, Anthropic, Deepseek, and Meta. One way the researchers calculated the scores was by measuring the attack success rate (ASR) across each provider’s models. The researchers found that DeepSeek (an open-source model developed by researchers in China) was the least safe, with a 62% ASR. Google was the second least safe. Anthropic, which produces Claude, was the safest, with a 6.95 ASR. OpenAI, which makes ChatGPT, was the second most safe.
Critical React, Next.js flaw lets hackers execute code on servers (https://www.bleepingcomputer.com/news/security/critical-react2shell-flaw-in-react-nextjs-lets-hackers-run-javascript-code/): ## Critical React, Next.js Flaw Lets Hackers Execute Code on Servers A critical vulnerability, dubbed 'React2Shell', in the React Server Components (RSC) 'Flight' protocol allows remote code execution without authentication in React and Next.js applications. The security issue stems from insecure deserialization. It received a severity score of 10/10 and has been assigned the identifiers CVE-2025-55182 for React and CVE-2025-66478 (CVE rejected in the National Vulnerability Database) for Next.js. Security researcher Lachlan Davidson discovered the flaw and reported it to React on November 29. He found that an attacker could achieve remote code execution (RCE) by sending a specially crafted HTTP request to React Server Function endpoints. &quot;Even if your app does not implement any React Server Function endpoints, it may still be vulnerable if your app supports React Server Components [RCS],&quot; warns the security advisory from React. The following packages in their default configuration are impacted: react-server-dom-parcel react-server-dom-turbopack and react-server-dom-webpack React is an open-source JavaScript library for building user interfaces. It's maintained by Meta and widely adopted by organizations of all sizes for front-end web development. Next.js, maintained by Vercel, is a framework built on top of React that adds server-side rendering, routing, and API endpoints. Both solutions are
'Exploitation is imminent' as 39 percent of cloud environs have max-severity React hole (https://go.theregister.com/feed/www.theregister.com/2025/12/03/exploitation_is_imminent_react_vulnerability/): A critical vulnerability in the JavaScript library React has been identified, affecting 39% of cloud environments. This flaw, tracked as CVE-2025-55182, allows unauthenticated, remote attackers to execute malicious code on vulnerable instances. The React team disclosed the vulnerability, recommending immediate upgrades to versions 19.0.1, 19.1.2, and 19.2.1 to mitigate the risk. Vercel, the maintainer of Next.js, also issued a patch and alert for the affected framework. Researchers reported the flaw to Meta, which collaborated with React to swiftly release an emergency patch. The vulnerability's high severity and ease of exploitation make immediate patching critical, with potential for widespread attention and public disclosure of technical details.
Prompt Injection Through Poetry (https://www.schneier.com/blog/archives/2025/11/prompt-injection-through-poetry.html): Researchers have demonstrated that poetic prompts can effectively jailbreak large language models (LLMs), achieving high attack success rates (ASR) across various models. This method, termed &quot;poetic jailbreaking,&quot; involves converting harmful prompts into verse, which bypasses contemporary safety mechanisms and highlights limitations in current alignment methods and evaluation protocols. The study utilized a curated dataset of 20 handcrafted adversarial poems in English and Italian, along with an additional 1,200 prompts from the MLCommons AILuminate Safety Benchmark, to test the effectiveness of poetic reframing in inducing aligned models to bypass refusal heuristics. The findings indicate that poetic framing can substantially outperform non-poetic baselines, with an average jailbreak success rate of 62% for hand-crafted poems and approximately 43% for meta-prompt conversions. This research underscores the need for improved safety mechanisms in LLMs to mitigate the risks posed by adversarial poetry.
North Korea lures engineers to rent identities in fake IT worker scheme (https://www.bleepingcomputer.com/news/security/north-korea-lures-engineers-to-rent-identities-in-fake-it-worker-scheme/): North Korean state-sponsored hackers, known as the Lazarus Group, have been using social engineering tactics to recruit engineers to rent their identities for illicit fundraising. The group uses stolen identities and AI, including deepfake videos, to infiltrate Western companies for espionage and revenue generation. The group uses a front-man engineer to pose as a figurehead in the operation to get a remote job at a targeted company. The front-man engineer would receive a percentage of the salary, between 20% and 35% for the duration of the contract. The compromised engineer takes all the risk as they rented their identity and will be the only one responsible for any damage done. The compromised engineer would have to let DPRK agents use their computer to hide the North Korean’s location and their traces.
Why We’ve Been Optimizing the Wrong Thing in LLMs for Years (https://towardsdatascience.com/why-weve-been-optimizing-the-wrong-thing-in-llms-for-years/): Researchers at Meta FAIR have introduced a novel approach to training Large Language Models (LLMs) called Multi-Token Prediction (MTP). This method leverages the latent capacity of LLMs to process multiple future tokens simultaneously, enhancing performance and inference speed. By dividing the model into a shared trunk and independent heads, MTP predicts future tokens concurrently, reducing inference time by up to three times. The architecture addresses memory bottlenecks through a sequential forward/backward pass strategy, maintaining similar batch sizes as standard models. Experimental results show MTP significantly outperforms traditional Next-Token Prediction (NTP) methods across various benchmarks. The study also explores design choices, such as parameter parity and head topology, revealing that parallel heads outperform causal heads in certain scenarios.
Malicious LLMs empower inexperienced hackers with advanced tools (https://www.bleepingcomputer.com/news/security/malicious-llms-empower-inexperienced-hackers-with-advanced-tools/): Unrestricted large language models (LLMs) like WormGPT 4 and KawaiiGPT are improving their capabilities to generate malicious code, delivering functional scripts for ransomware encryptors and lateral movement. [...]</description>
      <itunes:summary>Articles discussed
Whispering poetry at AI can make it break its own rules (https://www.malwarebytes.com/blog/news/2025/12/whispering-poetry-at-ai-can-make-it-break-its-own-rules): Researchers at Icaro Lab, Sapienza University, and DEXAI tested whether giving AI instructions as poetry would make it harder to detect different types of dangerous content. The researchers found that poetic elements such as metaphor, rhythm, and unconventional framing might disrupt pattern-matching heuristics that the AI’s guardrails rely on to spot harmful content. The tests covered models across nine providers, including all the usual suspects: Google, OpenAI, Anthropic, Deepseek, and Meta. One way the researchers calculated the scores was by measuring the attack success rate (ASR) across each provider’s models. The researchers found that DeepSeek (an open-source model developed by researchers in China) was the least safe, with a 62% ASR. Google was the second least safe. Anthropic, which produces Claude, was the safest, with a 6.95 ASR. OpenAI, which makes ChatGPT, was the second most safe.
Critical React, Next.js flaw lets hackers execute code on servers (https://www.bleepingcomputer.com/news/security/critical-react2shell-flaw-in-react-nextjs-lets-hackers-run-javascript-code/): ## Critical React, Next.js Flaw Lets Hackers Execute Code on Servers A critical vulnerability, dubbed 'React2Shell', in the React Server Components (RSC) 'Flight' protocol allows remote code execution without authentication in React and Next.js applications. The security issue stems from insecure deserialization. It received a severity score of 10/10 and has been assigned the identifiers CVE-2025-55182 for React and CVE-2025-66478 (CVE rejected in the National Vulnerability Database) for Next.js. Security researcher Lachlan Davidson discovered the flaw and reported it to React on November 29. He found that an attacker could achieve remote code execution (RCE) by sending a specially crafted HTTP request to React Server Function endpoints. &quot;Even if your app does not implement any React Server Function endpoints, it may still be vulnerable if your app supports React Server Components [RCS],&quot; warns the security advisory from React. The following packages in their default configuration are impacted: react-server-dom-parcel react-server-dom-turbopack and react-server-dom-webpack React is an open-source JavaScript library for building user interfaces. It's maintained by Meta and widely adopted by organizations of all sizes for front-end web development. Next.js, maintained by Vercel, is a framework built on top of React that adds server-side rendering, routing, and API endpoints. Both solutions are
'Exploitation is imminent' as 39 percent of cloud environs have max-severity React hole (https://go.theregister.com/feed/www.theregister.com/2025/12/03/exploitation_is_imminent_react_vulnerability/): A critical vulnerability in the JavaScript library React has been identified, affecting 39% of cloud environments. This flaw, tracked as CVE-2025-55182, allows unauthenticated, remote attackers to execute malicious code on vulnerable instances. The React team disclosed the vulnerability, recommending immediate upgrades to versions 19.0.1, 19.1.2, and 19.2.1 to mitigate the risk. Vercel, the maintainer of Next.js, also issued a patch and alert for the affected framework. Researchers reported the flaw to Meta, which collaborated with React to swiftly release an emergency patch. The vulnerability's high severity and ease of exploitation make immediate patching critical, with potential for widespread attention and public disclosure of technical details.
Prompt Injection Through Poetry (https://www.schneier.com/blog/archives/2025/11/prompt-injection-through-poetry.html): Researchers have demonstrated that poetic prompts can effectively jailbreak large language models (LLMs), achieving high attack success rates (ASR) across various models. This method, termed &quot;poetic jailbreaking,&quot; involves converting harmful prompts into verse, which bypasses contemporary safety mechanisms and highlights limitations in current alignment methods and evaluation protocols. The study utilized a curated dataset of 20 handcrafted adversarial poems in English and Italian, along with an additional 1,200 prompts from the MLCommons AILuminate Safety Benchmark, to test the effectiveness of poetic reframing in inducing aligned models to bypass refusal heuristics. The findings indicate that poetic framing can substantially outperform non-poetic baselines, with an average jailbreak success rate of 62% for hand-crafted poems and approximately 43% for meta-prompt conversions. This research underscores the need for improved safety mechanisms in LLMs to mitigate the risks posed by adversarial poetry.
North Korea lures engineers to rent identities in fake IT worker scheme (https://www.bleepingcomputer.com/news/security/north-korea-lures-engineers-to-rent-identities-in-fake-it-worker-scheme/): North Korean state-sponsored hackers, known as the Lazarus Group, have been using social engineering tactics to recruit engineers to rent their identities for illicit fundraising. The group uses stolen identities and AI, including deepfake videos, to infiltrate Western companies for espionage and revenue generation. The group uses a front-man engineer to pose as a figurehead in the operation to get a remote job at a targeted company. The front-man engineer would receive a percentage of the salary, between 20% and 35% for the duration of the contract. The compromised engineer takes all the risk as they rented their identity and will be the only one responsible for any damage done. The compromised engineer would have to let DPRK agents use their computer to hide the North Korean’s location and their traces.
Why We’ve Been Optimizing the Wrong Thing in LLMs for Years (https://towardsdatascience.com/why-weve-been-optimizing-the-wrong-thing-in-llms-for-years/): Researchers at Meta FAIR have introduced a novel approach to training Large Language Models (LLMs) called Multi-Token Prediction (MTP). This method leverages the latent capacity of LLMs to process multiple future tokens simultaneously, enhancing performance and inference speed. By dividing the model into a shared trunk and independent heads, MTP predicts future tokens concurrently, reducing inference time by up to three times. The architecture addresses memory bottlenecks through a sequential forward/backward pass strategy, maintaining similar batch sizes as standard models. Experimental results show MTP significantly outperforms traditional Next-Token Prediction (NTP) methods across various benchmarks. The study also explores design choices, such as parameter parity and head topology, revealing that parallel heads outperform causal heads in certain scenarios.
Malicious LLMs empower inexperienced hackers with advanced tools (https://www.bleepingcomputer.com/news/security/malicious-llms-empower-inexperienced-hackers-with-advanced-tools/): Unrestricted large language models (LLMs) like WormGPT 4 and KawaiiGPT are improving their capabilities to generate malicious code, delivering functional scripts for ransomware encryptors and lateral movement. [...]</itunes:summary>
      <pubDate>Sat, 17 Jan 2026 15:58:59 GMT</pubDate>
      <enclosure url="https://raw.githubusercontent.com/Refugeek/podcast-feed/main/episodes-EScyber/20251205095605-whispering-poetry-at-ai-can-make-it-break-its-own-rules-critical-react-next-js-flaw-lets-hackers-execute-code-on-servers.mp3" type="audio/mpeg" length="36637964"/>
      <itunes:duration>1831</itunes:duration>
    </item>
    <item>
      <title>20251211061509-north-korean-hackers-exploit-react2shell-flaw-in-etherrat-malware-attacks-shai-hulud-2-0-guidance-for-detecting-investigating-and-defending-against-the-supply-chain-attack</title>
      <description>Articles discussed
North Korean hackers exploit React2Shell flaw in EtherRAT malware attacks (https://www.bleepingcomputer.com/news/security/north-korean-hackers-exploit-react2shell-flaw-in-etherrat-malware-attacks/): North Korean hackers exploited a critical vulnerability in the React Server Components (RSC) &quot;Flight&quot; protocol, known as React2Shell, to deploy EtherRAT malware. This attack, tracked as CVE-2025-55182, enables remote code execution via crafted HTTP requests, affecting numerous cloud environments running React/Next.js. Researchers at Sysdig discovered EtherRAT, which employs Ethereum smart contracts for command-and-control (C2) communication, multi-layered Linux persistence, and on-the-fly payload rewriting. The malware's advanced features include aggressive persistence mechanisms across five layers on Linux systems and the ability to self-update by sending its source code to an API endpoint. This self-updating capability helps evade static detection and may introduce mission-specific functionality. The attack chain begins with exploiting React2Shell to execute a base64-encoded shell command, followed by downloading and executing a malicious shell script. Automated exploitation led to breaches of at least 30 organizations across multiple sectors, resulting in credential theft, cryptomining, and the deployment of commodity backdoors.
Shai-Hulud 2.0: Guidance for detecting, investigating, and defending against the supply chain attack (https://www.microsoft.com/en-us/security/blog/2025/12/09/shai-hulud-2-0-guidance-for-detecting-investigating-and-defending-against-the-supply-chain-attack/): The Shai-Hulud 2.0 attack is a significant cloud-native ecosystem compromise that targets developer environments, CI/CD pipelines, and cloud-connected workloads. Attackers maliciously modified hundreds of publicly available packages, exploiting preinstall scripts to execute malicious code before security checks. This campaign builds on earlier supply chain compromises by introducing automation, faster propagation, and a broader target set, compromising maintainer accounts from widely used projects like Zapier and Postman. Stolen credentials are exfiltrated to public attacker-controlled repositories, posing a significant risk to organizations. To defend against threats like Shai-Hulud 2.0, organizations benefit from a layered protection approach from Microsoft Defender, which provides security coverage from code to runtime. This defense-in-depth approach is especially valuable when facing supply chain-driven attacks that might introduce malicious dependencies. Microsoft Defender recommends reviewing Key Vault assets, rotating exposed credentials, isolating affected CI/CD agents, and prioritizing high-risk attack paths to reduce further exposure.
Prompt injection is a problem that may never be fixed, warns NCSC (https://www.malwarebytes.com/blog/news/2025/12/prompt-injection-is-a-problem-that-may-never-be-fixed-warns-ncsc): Prompt injection, a persistent AI security issue, allows attackers to manipulate AI models by embedding malicious instructions within user input, bypassing safety mechanisms known as guardrails. This vulnerability poses significant risks, particularly as Large Language Models (LLMs) increasingly operate autonomously, potentially leading to data breaches or fraud. The National Cyber Security Centre (NCSC) warns that prompt injection may never be fully resolved, unlike SQL injection, which has been effectively managed over the years. The NCSC emphasizes the need for careful design and implementation of AI systems to prevent such vulnerabilities, advising developers to limit AI agents' capabilities and monitor interactions closely. The potential impact of prompt injection is likened to past cybersecurity breaches, highlighting the urgent need for industry-wide improvements in AI security practices.
Microsoft won’t fix .NET RCE bug affecting slew of enterprise apps, researchers say (https://go.theregister.com/feed/www.theregister.com/2025/12/10/microsoft_wont_fix_net_rce/): Security researchers have revealed a .NET security flaw affecting a host of enterprise-grade products, which Microsoft refuses to fix. Piotr Bazydło, principal vulnerability researcher at watchTowr, unveiled the findings at Black Hat Europe, claiming that several vendor and in-house solutions could be vulnerable to remote code execution (RCE) attacks due to errors in the way applications built on Microsoft's .NET framework handle Simple Object Access Protocol (SOAP) messages. The researcher identified the SoapHttpClientProtocol class as the primary culprit, explaining that it can be exploited in different ways to achieve an attacker's goals. The class inherits from HttpWebClientProtocol, as do other client proxy types, but watchTowr highlighted SoapHttpClientProtocol as it is by far the most common across codebases. Bazydło said in a blog post shared with The Register before publication: &quot;Its name and the official documentation both paint a simple picture: it should handle SOAP messages transported over HTTP. Straightforward. Predictable. Safe. Reality is less cooperative.&quot; The class is responsible for setting the target URL of the SOAP service and defining a SOAP method, but the vulnerability arises when attackers can manipulate that target URL and the way SoapHttpClientProtocol creates clients. Despite being designed to handle HTTP requests, SoapHttpClientProtocol uses a
Google Chrome’s New AI Security Aims to Stop Hackers Cold (https://www.techrepublic.com/article/news-google-chrome-ai-security/): Google has announced a significant security overhaul for Chrome's AI-powered browsing features, designed to block indirect prompt injection attacks. This development addresses vulnerabilities flagged by security researchers, with Chrome controlling approximately 65% of the global browser market. The new system introduces the User Alignment Critic, an AI security guard that scrutinizes proposed actions without seeing actual web content, preventing manipulation. Google also implements Agent Origin Sets to restrict AI agents' access to certain web sections, requiring explicit user approval for sensitive actions. The company offers a $20,000 bounty for demonstrating successful breaches of these security measures, employing automated red-teaming systems to proactively identify weaknesses. These enhancements could set industry standards for AI agent protection, influencing how other companies approach AI safety.
Beyond the bomb: When adversaries bring their own virtual machine for persistence (https://redcanary.com/blog/threat-intelligence/email-bombing-virtual-machine/): Adversaries have been using spam bombing campaigns to gain initial access to compromised networks. In 2025, Red Canary Intelligence uncovered an interesting tactic: after a spam bombing attack, an adversary introduced their own virtual machine (VM) into a compromised environment and established persistence. This was the first time Red Canary Intelligence had detected an adversary bringing their own QEMU VM into an environment under the guise of a technical support call following a spam bombing attack. The adversary wasted no time after flooding the inbox, initiating a call to the organization posing as a technical support representative. Their offer was simple: help alleviate the deluge. After gaining the user’s confidence, the adversary leveraged remote assistance software Quick Assist. This legitimate, built-in Windows application allows a trusted person to take control of another computer remotely. While it can be used for support—like many remote monitoring and management (RMM) tools these days—it can be misused by threat actors to establish initial access and deploy malicious payloads. The adversary’s actions began with the execution of a Visual Basic Script (VBScript), Update.vbs. Due to Quick Assist’s nature of piggybacking on the user’s Explorer session, many of these initial actions appeared to originate from explorer.exe
Uncovering Hidden Forensic Evidence in Windows: The Mystery of AutoLogger-Diagtrack-Listener.etl (https://feeds.fortinet.com/~/932574884/0/fortinet/blog/threat-research~Uncovering-Hidden-Forensic-Evidence-in-Windows-The-Mystery-of-AutoLoggerDiagtrackListeneretl): A recent FortiGuard IR services (FGIR) incident response engagement revealed that the threat actor used anti-forensic techniques to cover their tracks and avoid their malware getting into the hands of researchers. While analyzing a disk image of a compromised Windows Server 2016 system, FGIR was able to identify historical evidence of deleted malware and tools used by the threat actor, inside an obscure ETL file called AutoLogger-Diagtrack-Listener.etl. ETL files are generated by the Windows ETW (Event Tracing for Windows) infrastructure. Event Tracing for Windows (ETW) is a built-in, high-performance logging framework that enables Windows and applications to record detailed events with minimal overhead. Instead of writing plain text logs, ETW uses providers such as the kernel, TCP/IP stack, or registry, which send structured event data into ETW sessions. These sessions can buffer the data for real-time consumption or write it to binary Event Trace Log (ETL) files. ETW involves three main roles: providers (sources of events), controllers (which start, stop, and manage sessions using tools like logman or PerfMon), and consumers (such as debuggers, Event Viewer, or EDRs, which process and interpret the data). This framework is powerful because it provides granular visibility into low-level OS activity (like process creation,</description>
      <itunes:summary>Articles discussed
North Korean hackers exploit React2Shell flaw in EtherRAT malware attacks (https://www.bleepingcomputer.com/news/security/north-korean-hackers-exploit-react2shell-flaw-in-etherrat-malware-attacks/): North Korean hackers exploited a critical vulnerability in the React Server Components (RSC) &quot;Flight&quot; protocol, known as React2Shell, to deploy EtherRAT malware. This attack, tracked as CVE-2025-55182, enables remote code execution via crafted HTTP requests, affecting numerous cloud environments running React/Next.js. Researchers at Sysdig discovered EtherRAT, which employs Ethereum smart contracts for command-and-control (C2) communication, multi-layered Linux persistence, and on-the-fly payload rewriting. The malware's advanced features include aggressive persistence mechanisms across five layers on Linux systems and the ability to self-update by sending its source code to an API endpoint. This self-updating capability helps evade static detection and may introduce mission-specific functionality. The attack chain begins with exploiting React2Shell to execute a base64-encoded shell command, followed by downloading and executing a malicious shell script. Automated exploitation led to breaches of at least 30 organizations across multiple sectors, resulting in credential theft, cryptomining, and the deployment of commodity backdoors.
Shai-Hulud 2.0: Guidance for detecting, investigating, and defending against the supply chain attack (https://www.microsoft.com/en-us/security/blog/2025/12/09/shai-hulud-2-0-guidance-for-detecting-investigating-and-defending-against-the-supply-chain-attack/): The Shai-Hulud 2.0 attack is a significant cloud-native ecosystem compromise that targets developer environments, CI/CD pipelines, and cloud-connected workloads. Attackers maliciously modified hundreds of publicly available packages, exploiting preinstall scripts to execute malicious code before security checks. This campaign builds on earlier supply chain compromises by introducing automation, faster propagation, and a broader target set, compromising maintainer accounts from widely used projects like Zapier and Postman. Stolen credentials are exfiltrated to public attacker-controlled repositories, posing a significant risk to organizations. To defend against threats like Shai-Hulud 2.0, organizations benefit from a layered protection approach from Microsoft Defender, which provides security coverage from code to runtime. This defense-in-depth approach is especially valuable when facing supply chain-driven attacks that might introduce malicious dependencies. Microsoft Defender recommends reviewing Key Vault assets, rotating exposed credentials, isolating affected CI/CD agents, and prioritizing high-risk attack paths to reduce further exposure.
Prompt injection is a problem that may never be fixed, warns NCSC (https://www.malwarebytes.com/blog/news/2025/12/prompt-injection-is-a-problem-that-may-never-be-fixed-warns-ncsc): Prompt injection, a persistent AI security issue, allows attackers to manipulate AI models by embedding malicious instructions within user input, bypassing safety mechanisms known as guardrails. This vulnerability poses significant risks, particularly as Large Language Models (LLMs) increasingly operate autonomously, potentially leading to data breaches or fraud. The National Cyber Security Centre (NCSC) warns that prompt injection may never be fully resolved, unlike SQL injection, which has been effectively managed over the years. The NCSC emphasizes the need for careful design and implementation of AI systems to prevent such vulnerabilities, advising developers to limit AI agents' capabilities and monitor interactions closely. The potential impact of prompt injection is likened to past cybersecurity breaches, highlighting the urgent need for industry-wide improvements in AI security practices.
Microsoft won’t fix .NET RCE bug affecting slew of enterprise apps, researchers say (https://go.theregister.com/feed/www.theregister.com/2025/12/10/microsoft_wont_fix_net_rce/): Security researchers have revealed a .NET security flaw affecting a host of enterprise-grade products, which Microsoft refuses to fix. Piotr Bazydło, principal vulnerability researcher at watchTowr, unveiled the findings at Black Hat Europe, claiming that several vendor and in-house solutions could be vulnerable to remote code execution (RCE) attacks due to errors in the way applications built on Microsoft's .NET framework handle Simple Object Access Protocol (SOAP) messages. The researcher identified the SoapHttpClientProtocol class as the primary culprit, explaining that it can be exploited in different ways to achieve an attacker's goals. The class inherits from HttpWebClientProtocol, as do other client proxy types, but watchTowr highlighted SoapHttpClientProtocol as it is by far the most common across codebases. Bazydło said in a blog post shared with The Register before publication: &quot;Its name and the official documentation both paint a simple picture: it should handle SOAP messages transported over HTTP. Straightforward. Predictable. Safe. Reality is less cooperative.&quot; The class is responsible for setting the target URL of the SOAP service and defining a SOAP method, but the vulnerability arises when attackers can manipulate that target URL and the way SoapHttpClientProtocol creates clients. Despite being designed to handle HTTP requests, SoapHttpClientProtocol uses a
Google Chrome’s New AI Security Aims to Stop Hackers Cold (https://www.techrepublic.com/article/news-google-chrome-ai-security/): Google has announced a significant security overhaul for Chrome's AI-powered browsing features, designed to block indirect prompt injection attacks. This development addresses vulnerabilities flagged by security researchers, with Chrome controlling approximately 65% of the global browser market. The new system introduces the User Alignment Critic, an AI security guard that scrutinizes proposed actions without seeing actual web content, preventing manipulation. Google also implements Agent Origin Sets to restrict AI agents' access to certain web sections, requiring explicit user approval for sensitive actions. The company offers a $20,000 bounty for demonstrating successful breaches of these security measures, employing automated red-teaming systems to proactively identify weaknesses. These enhancements could set industry standards for AI agent protection, influencing how other companies approach AI safety.
Beyond the bomb: When adversaries bring their own virtual machine for persistence (https://redcanary.com/blog/threat-intelligence/email-bombing-virtual-machine/): Adversaries have been using spam bombing campaigns to gain initial access to compromised networks. In 2025, Red Canary Intelligence uncovered an interesting tactic: after a spam bombing attack, an adversary introduced their own virtual machine (VM) into a compromised environment and established persistence. This was the first time Red Canary Intelligence had detected an adversary bringing their own QEMU VM into an environment under the guise of a technical support call following a spam bombing attack. The adversary wasted no time after flooding the inbox, initiating a call to the organization posing as a technical support representative. Their offer was simple: help alleviate the deluge. After gaining the user’s confidence, the adversary leveraged remote assistance software Quick Assist. This legitimate, built-in Windows application allows a trusted person to take control of another computer remotely. While it can be used for support—like many remote monitoring and management (RMM) tools these days—it can be misused by threat actors to establish initial access and deploy malicious payloads. The adversary’s actions began with the execution of a Visual Basic Script (VBScript), Update.vbs. Due to Quick Assist’s nature of piggybacking on the user’s Explorer session, many of these initial actions appeared to originate from explorer.exe
Uncovering Hidden Forensic Evidence in Windows: The Mystery of AutoLogger-Diagtrack-Listener.etl (https://feeds.fortinet.com/~/932574884/0/fortinet/blog/threat-research~Uncovering-Hidden-Forensic-Evidence-in-Windows-The-Mystery-of-AutoLoggerDiagtrackListeneretl): A recent FortiGuard IR services (FGIR) incident response engagement revealed that the threat actor used anti-forensic techniques to cover their tracks and avoid their malware getting into the hands of researchers. While analyzing a disk image of a compromised Windows Server 2016 system, FGIR was able to identify historical evidence of deleted malware and tools used by the threat actor, inside an obscure ETL file called AutoLogger-Diagtrack-Listener.etl. ETL files are generated by the Windows ETW (Event Tracing for Windows) infrastructure. Event Tracing for Windows (ETW) is a built-in, high-performance logging framework that enables Windows and applications to record detailed events with minimal overhead. Instead of writing plain text logs, ETW uses providers such as the kernel, TCP/IP stack, or registry, which send structured event data into ETW sessions. These sessions can buffer the data for real-time consumption or write it to binary Event Trace Log (ETL) files. ETW involves three main roles: providers (sources of events), controllers (which start, stop, and manage sessions using tools like logman or PerfMon), and consumers (such as debuggers, Event Viewer, or EDRs, which process and interpret the data). This framework is powerful because it provides granular visibility into low-level OS activity (like process creation,</itunes:summary>
      <description>Articles discussed
North Korean hackers exploit React2Shell flaw in EtherRAT malware attacks (https://www.bleepingcomputer.com/news/security/north-korean-hackers-exploit-react2shell-flaw-in-etherrat-malware-attacks/): North Korean hackers exploited a critical vulnerability in the React Server Components (RSC) &quot;Flight&quot; protocol, known as React2Shell, to deploy EtherRAT malware. This attack, tracked as CVE-2025-55182, enables remote code execution via crafted HTTP requests, affecting numerous cloud environments running React/Next.js. Researchers at Sysdig discovered EtherRAT, which employs Ethereum smart contracts for command-and-control (C2) communication, multi-layered Linux persistence, and on-the-fly payload rewriting. The malware's advanced features include aggressive persistence mechanisms across five layers on Linux systems and the ability to self-update by sending its source code to an API endpoint. This self-updating capability helps evade static detection and may introduce mission-specific functionality. The attack chain begins with exploiting React2Shell to execute a base64-encoded shell command, followed by downloading and executing a malicious shell script. Automated exploitation led to breaches of at least 30 organizations across multiple sectors, resulting in credential theft, cryptomining, and the deployment of commodity backdoors.
Shai-Hulud 2.0: Guidance for detecting, investigating, and defending against the supply chain attack (https://www.microsoft.com/en-us/security/blog/2025/12/09/shai-hulud-2-0-guidance-for-detecting-investigating-and-defending-against-the-supply-chain-attack/): The Shai-Hulud 2.0 attack is a significant cloud-native ecosystem compromise that targets developer environments, CI/CD pipelines, and cloud-connected workloads. Attackers maliciously modified hundreds of publicly available packages, exploiting preinstall scripts to execute malicious code before security checks. This campaign builds on earlier supply chain compromises by introducing automation, faster propagation, and a broader target set, compromising maintainer accounts from widely used projects like Zapier and Postman. Stolen credentials are exfiltrated to public attacker-controlled repositories, posing a significant risk to organizations. To defend against threats like Shai-Hulud 2.0, organizations benefit from a layered protection approach from Microsoft Defender, which provides security coverage from code to runtime. This defense-in-depth approach is especially valuable when facing supply chain-driven attacks that might introduce malicious dependencies. Microsoft Defender recommends reviewing Key Vault assets, rotating exposed credentials, isolating affected CI/CD agents, and prioritizing high-risk attack paths to reduce further exposure.
Prompt injection is a problem that may never be fixed, warns NCSC (https://www.malwarebytes.com/blog/news/2025/12/prompt-injection-is-a-problem-that-may-never-be-fixed-warns-ncsc): Prompt injection, a persistent AI security issue, allows attackers to manipulate AI models by embedding malicious instructions within user input, bypassing safety mechanisms known as guardrails. This vulnerability poses significant risks, particularly as Large Language Models (LLMs) increasingly operate autonomously, potentially leading to data breaches or fraud. The National Cyber Security Centre (NCSC) warns that prompt injection may never be fully resolved, unlike SQL injection, which has been effectively managed over the years. The NCSC emphasizes the need for careful design and implementation of AI systems to prevent such vulnerabilities, advising developers to limit AI agents' capabilities and monitor interactions closely. The potential impact of prompt injection is likened to past cybersecurity breaches, highlighting the urgent need for industry-wide improvements in AI security practices.
Microsoft won’t fix .NET RCE bug affecting slew of enterprise apps, researchers say (https://go.theregister.com/feed/www.theregister.com/2025/12/10/microsoft_wont_fix_net_rce/): Security researchers have revealed a .NET security flaw affecting a host of enterprise-grade products, which Microsoft refuses to fix. Piotr Bazydło, principal vulnerability researcher at watchTowr, unveiled the findings at Black Hat Europe, claiming that several vendor and in-house solutions could be vulnerable to remote code execution (RCE) attacks due to errors in the way applications built on Microsoft's .NET framework handle Simple Object Access Protocol (SOAP) messages. The researcher identified the SoapHttpClientProtocol class as the primary culprit, explaining that it can be exploited in different ways to achieve an attacker's goals. The class inherits from HttpWebClientProtocol, as do other client proxy types, but watchTowr highlighted SoapHttpClientProtocol as it is by far the most common across codebases. Bazydło said in a blog post shared with The Register before publication: &quot;Its name and the official documentation both paint a simple picture: it should handle SOAP messages transported over HTTP. Straightforward. Predictable. Safe. Reality is less cooperative.&quot; The class is responsible for setting the target URL of the SOAP service and defining a SOAP method, but the vulnerability arises when attackers can manipulate that target URL and the way SoapHttpClientProtocol creates clients. Despite being designed to handle HTTP requests, SoapHttpClientProtocol uses a
Google Chrome’s New AI Security Aims to Stop Hackers Cold (https://www.techrepublic.com/article/news-google-chrome-ai-security/): Google has announced a significant security overhaul for Chrome's AI-powered browsing features, designed to block indirect prompt injection attacks. This development addresses vulnerabilities flagged by security researchers, with Chrome controlling approximately 65% of the global browser market. The new system introduces the User Alignment Critic, an AI security guard that scrutinizes proposed actions without seeing actual web content, preventing manipulation. Google also implements Agent Origin Sets to restrict AI agents' access to certain web sections, requiring explicit user approval for sensitive actions. The company offers a $20,000 bounty for demonstrating successful breaches of these security measures, employing automated red-teaming systems to proactively identify weaknesses. These enhancements could set industry standards for AI agent protection, influencing how other companies approach AI safety.
Beyond the bomb: When adversaries bring their own virtual machine for persistence (https://redcanary.com/blog/threat-intelligence/email-bombing-virtual-machine/): Adversaries have been using spam bombing campaigns to gain initial access to compromised networks. In 2025, Red Canary Intelligence uncovered an interesting tactic: after a spam bombing attack, an adversary introduced their own virtual machine (VM) into a compromised environment and established persistence. This was the first time Red Canary Intelligence had detected an adversary bringing their own QEMU VM into an environment under the guise of a technical support call following a spam bombing attack. The adversary wasted no time after flooding the inbox, initiating a call to the organization posing as a technical support representative. Their offer was simple: help alleviate the deluge. After gaining the user’s confidence, the adversary leveraged remote assistance software Quick Assist. This legitimate, built-in Windows application allows a trusted person to take control of another computer remotely. While it can be used for support—like many remote monitoring and management (RMM) tools these days—it can be misused by threat actors to establish initial access and deploy malicious payloads. The adversary’s actions began with the execution of a Visual Basic Script (VBScript), Update.vbs. Due to Quick Assist’s nature of piggybacking on the user’s Explorer session, many of these initial actions appeared to originate from explorer.exe
Uncovering Hidden Forensic Evidence in Windows: The Mystery of AutoLogger-Diagtrack-Listener.etl (https://feeds.fortinet.com/~/932574884/0/fortinet/blog/threat-research~Uncovering-Hidden-Forensic-Evidence-in-Windows-The-Mystery-of-AutoLoggerDiagtrackListeneretl): A recent FortiGuard IR services (FGIR) incident response engagement revealed that the threat actor used anti-forensic techniques to cover their tracks and avoid their malware getting into the hands of researchers. While analyzing a disk image of a compromised Windows Server 2016 system, FGIR was able to identify historical evidence of deleted malware and tools used by the threat actor, inside an obscure ETL file called AutoLogger-Diagtrack-Listener.etl. ETL files are generated by the Windows ETW (Event Tracing for Windows) infrastructure. Event Tracing for Windows (ETW) is a built-in, high-performance logging framework that enables Windows and applications to record detailed events with minimal overhead. Instead of writing plain text logs, ETW uses providers such as the kernel, TCP/IP stack, or registry, which send structured event data into ETW sessions. These sessions can buffer the data for real-time consumption or write it to binary Event Trace Log (ETL) files. ETW involves three main roles: providers (sources of events), controllers (which start, stop, and manage sessions using tools like logman or PerfMon), and consumers (such as debuggers, Event Viewer, or EDRs, which process and interpret the data). This framework is powerful because it provides granular visibility into low-level OS activity (like process creation,</description>
      <itunes:summary>Articles discussed
North Korean hackers exploit React2Shell flaw in EtherRAT malware attacks (https://www.bleepingcomputer.com/news/security/north-korean-hackers-exploit-react2shell-flaw-in-etherrat-malware-attacks/): North Korean hackers exploited a critical vulnerability in the React Server Components (RSC) &quot;Flight&quot; protocol, known as React2Shell, to deploy EtherRAT malware. This attack, tracked as CVE-2025-55182, enables remote code execution via crafted HTTP requests, affecting numerous cloud environments running React/Next.js. Researchers at Sysdig discovered EtherRAT, which employs Ethereum smart contracts for command-and-control (C2) communication, multi-layered Linux persistence, and on-the-fly payload rewriting. The malware's advanced features include aggressive persistence mechanisms across five layers on Linux systems and the ability to self-update by sending its source code to an API endpoint. This self-updating capability helps evade static detection and may introduce mission-specific functionality. The attack chain begins with exploiting React2Shell to execute a base64-encoded shell command, followed by downloading and executing a malicious shell script. Automated exploitation led to breaches of at least 30 organizations across multiple sectors, resulting in credential theft, cryptomining, and the deployment of commodity backdoors.
Shai-Hulud 2.0: Guidance for detecting, investigating, and defending against the supply chain attack (https://www.microsoft.com/en-us/security/blog/2025/12/09/shai-hulud-2-0-guidance-for-detecting-investigating-and-defending-against-the-supply-chain-attack/): The Shai-Hulud 2.0 attack is a significant cloud-native ecosystem compromise that targets developer environments, CI/CD pipelines, and cloud-connected workloads. Attackers maliciously modified hundreds of publicly available packages, exploiting preinstall scripts to execute malicious code before security checks. This campaign builds on earlier supply chain compromises by introducing automation, faster propagation, and a broader target set, compromising maintainer accounts from widely used projects like Zapier and Postman. Stolen credentials are exfiltrated to public attacker-controlled repositories, posing a significant risk to organizations. To defend against threats like Shai-Hulud 2.0, organizations benefit from a layered protection approach from Microsoft Defender, which provides security coverage from code to runtime. This defense-in-depth approach is especially valuable when facing supply chain-driven attacks that might introduce malicious dependencies. Microsoft Defender recommends reviewing Key Vault assets, rotating exposed credentials, isolating affected CI/CD agents, and prioritizing high-risk attack paths to reduce further exposure.
Prompt injection is a problem that may never be fixed, warns NCSC (https://www.malwarebytes.com/blog/news/2025/12/prompt-injection-is-a-problem-that-may-never-be-fixed-warns-ncsc): Prompt injection, a persistent AI security issue, allows attackers to manipulate AI models by embedding malicious instructions within user input, bypassing safety mechanisms known as guardrails. This vulnerability poses significant risks, particularly as Large Language Models (LLMs) increasingly operate autonomously, potentially leading to data breaches or fraud. The National Cyber Security Centre (NCSC) warns that prompt injection may never be fully resolved, unlike SQL injection, which has been effectively managed over the years. The NCSC emphasizes the need for careful design and implementation of AI systems to prevent such vulnerabilities, advising developers to limit AI agents' capabilities and monitor interactions closely. The potential impact of prompt injection is likened to past cybersecurity breaches, highlighting the urgent need for industry-wide improvements in AI security practices.
Microsoft won’t fix .NET RCE bug affecting slew of enterprise apps, researchers say (https://go.theregister.com/feed/www.theregister.com/2025/12/10/microsoft_wont_fix_net_rce/): Security researchers have revealed a .NET security flaw affecting a host of enterprise-grade products, which Microsoft refuses to fix. Piotr Bazydło, principal vulnerability researcher at watchTowr, unveiled the findings at Black Hat Europe, claiming that several vendor and in-house solutions could be vulnerable to remote code execution (RCE) attacks due to errors in the way applications built on Microsoft's .NET framework handle Simple Object Access Protocol (SOAP) messages. The researcher identified the SoapHttpClientProtocol class as the primary culprit, explaining that it can be exploited in different ways to achieve an attacker's goals. The class inherits from HttpWebClientProtocol, as do other client proxy types, but watchTowr highlighted SoapHttpClientProtocol as it is by far the most common across codebases. Bazydło said in a blog post shared with The Register before publication: &quot;Its name and the official documentation both paint a simple picture: it should handle SOAP messages transported over HTTP. Straightforward. Predictable. Safe. Reality is less cooperative.&quot; The class is responsible for setting the target URL of the SOAP service and defining a SOAP method, but the vulnerability arises when attackers can manipulate that target URL and the way SoapHttpClientProtocol creates clients. Despite being designed to handle HTTP requests, SoapHttpClientProtocol uses a
Google Chrome’s New AI Security Aims to Stop Hackers Cold (https://www.techrepublic.com/article/news-google-chrome-ai-security/): Google has announced a significant security overhaul for Chrome's AI-powered browsing features, designed to block indirect prompt injection attacks. This development addresses vulnerabilities flagged by security researchers, with Chrome controlling approximately 65% of the global browser market. The new system introduces the User Alignment Critic, an AI security guard that scrutinizes proposed actions without seeing actual web content, preventing manipulation. Google also implements Agent Origin Sets to restrict AI agents' access to certain web sections, requiring explicit user approval for sensitive actions. The company offers a $20,000 bounty for demonstrating successful breaches of these security measures, employing automated red-teaming systems to proactively identify weaknesses. These enhancements could set industry standards for AI agent protection, influencing how other companies approach AI safety.
Beyond the bomb: When adversaries bring their own virtual machine for persistence (https://redcanary.com/blog/threat-intelligence/email-bombing-virtual-machine/): Adversaries have been using spam bombing campaigns to gain initial access to compromised networks. In 2025, Red Canary Intelligence uncovered an interesting tactic: after a spam bombing attack, an adversary introduced their own virtual machine (VM) into a compromised environment and established persistence. This was the first time Red Canary Intelligence had detected an adversary bringing their own QEMU VM into an environment under the guise of a technical support call following a spam bombing attack. The adversary wasted no time after flooding the inbox, initiating a call to the organization posing as a technical support representative. Their offer was simple: help alleviate the deluge. After gaining the user’s confidence, the adversary leveraged remote assistance software Quick Assist. This legitimate, built-in Windows application allows a trusted person to take control of another computer remotely. While it can be used for support—like many remote monitoring and management (RMM) tools these days—it can be misused by threat actors to establish initial access and deploy malicious payloads. The adversary’s actions began with the execution of a Visual Basic Script (VBScript), Update.vbs. Due to Quick Assist’s nature of piggybacking on the user’s Explorer session, many of these initial actions appeared to originate from explorer.exe
Uncovering Hidden Forensic Evidence in Windows: The Mystery of AutoLogger-Diagtrack-Listener.etl (https://feeds.fortinet.com/~/932574884/0/fortinet/blog/threat-research~Uncovering-Hidden-Forensic-Evidence-in-Windows-The-Mystery-of-AutoLoggerDiagtrackListeneretl): A recent FortiGuard IR services (FGIR) incident response engagement revealed that the threat actor used anti-forensic techniques to cover their tracks and avoid their malware getting into the hands of researchers. While analyzing a disk image of a compromised Windows Server 2016 system, FGIR was able to identify historical evidence of deleted malware and tools used by the threat actor, inside an obscure ETL file called AutoLogger-Diagtrack-Listener.etl. ETL files are generated by the Windows ETW (Event Tracing for Windows) infrastructure. Event Tracing for Windows (ETW) is a built-in, high-performance logging framework that enables Windows and applications to record detailed events with minimal overhead. Instead of writing plain text logs, ETW uses providers such as the kernel, TCP/IP stack, or registry, which send structured event data into ETW sessions. These sessions can buffer the data for real-time consumption or write it to binary Event Trace Log (ETL) files. ETW involves three main roles: providers (sources of events), controllers (which start, stop, and manage sessions using tools like logman or PerfMon), and consumers (such as debuggers, Event Viewer, or EDRs, which process and interpret the data). This framework is powerful because it provides granular visibility into low-level OS activity (like process creation,</itunes:summary>
      <pubDate>Sat, 17 Jan 2026 15:58:59 GMT</pubDate>
      <enclosure url="https://raw.githubusercontent.com/Refugeek/podcast-feed/main/episodes-EScyber/20251211061509-north-korean-hackers-exploit-react2shell-flaw-in-etherrat-malware-attacks-shai-hulud-2-0-guidance-for-detecting-investigating-and-defending-against-the-supply-chain-attack.mp3" type="audio/mpeg" length="40869164"/>
      <itunes:duration>2043</itunes:duration>
    </item>
    <item>
      <title>20251220070951-defending-against-the-cve-2025-55182-react2shell-vulnerability-in-react-server-components-ghostposter-attacks-hide-malicious-javascript-in-firefox-addon-logos</title>
      <description>Articles discussed
Defending against the CVE-2025-55182 (React2Shell) vulnerability in React Server Components (https://www.microsoft.com/en-us/security/blog/2025/12/15/defending-against-the-cve-2025-55182-react2shell-vulnerability-in-react-server-components/): The CVE-2025-55182 (React2Shell) vulnerability is a critical pre-authentication remote code execution vulnerability affecting React Server Components, Next.js, and related frameworks. With a CVSS score of 10.0, this vulnerability could allow attackers to execute arbitrary code on vulnerable servers through a single malicious HTTP request. Exploitation activity related to this vulnerability was detected as early as December 5, 2025. Most successful exploits originated from red team assessments; however, we also observed real-world exploitation attempts by threat actors delivering multiple subsequent payloads, majority of which are coin miners. Both Windows and Linux environments have been observed to be impacted. The React Server Components ecosystem is a collection of packages, frameworks, and bundlers that enable React 19 applications to run parts of their logic on the server rather than the browser. It uses the Flight protocol to communicate between client and server. When a client requests data, the server receives a payload, parses this payload, executes server-side logic, and returns a serialized component tree. The vulnerability exists because affected React Server Components versions fail to validate incoming payloads. This could allow attackers to inject malicious structures that React accepts as valid, leading to prototype pollution and remote code execution. This vulnerability presents a
GhostPoster attacks hide malicious JavaScript in Firefox addon logos (https://www.bleepingcomputer.com/news/security/ghostposter-attacks-hide-malicious-javascript-in-firefox-addon-logos/): The GhostPoster campaign is a new attack that hides malicious JavaScript code in the image logos of over 50,000 popular Firefox extensions. The hidden script is acting as a loader that fetches the main payload from a remote server. Koi Security researchers discovered the GhostPoster campaign and identified 17 compromised Firefox extensions that either read the PNG logo to extract and execute the malware loader or download the main payload from the attacker's server. The malicious extensions are from popular categories: free-vpn-forever screenshot-saved-easy weather-best-forecast crxmouse-gesture cache-fast-site-loader freemp3downloader google-translate-right-clicks google-traductor-esp world-wide-vpn dark-reader-for-ff translator-gbbd i-like-weather google-translate-pro-extension 谷歌-翻译 libretv-watch-free-videos ad-stop right-click-google-translate The JavaScript loader activates 48 hours later to fetch a payload from a hardcoded domain. A second backup domain is available if the payload is not retrieved from the first one. The final payload has the following capabilities: Hijacks affiliate links on major e-commerce sites, redirecting commissions to the attackers. Injects Google Analytics tracking into every page the user visits. Strips security headers from all HTTP responses. Bypasses CAPTCHA via three distinct mechanisms to circumvent bot protections. Injects invisible iframes for ad fraud, click fraud,
Browser 'privacy' extensions have eye on your AI, log all your chats (https://go.theregister.com/feed/www.theregister.com/2025/12/16/chrome_edge_privacy_extensions_quietly/): Four popular browser extensions have been harvesting the text of chatbot conversations from over 8 million people and sending them back to the developers. The four extensions are Urban VPN Proxy, 1ClickVPN Proxy, Urban Browser Guard, and Urban Ad Blocker. They are distributed via the Chrome Web Store and Microsoft Edge Add-ons. The extensions include code designed to capture and transmit browser-based interactions with popular AI tools. Urban VPN Proxy targets conversations across ten AI platforms, including ChatGPT, Claude, Gemini, Microsoft Copilot, Perplexity, DeepSeek, Grok, and Meta AI. The research firm said that the platforms targeted include ChatGPT, Claude, Gemini, Microsoft Copilot, Perplexity, DeepSeek, Grok, and Meta AI. The extension monitors the user's browser tabs and, when the user visits one of the targeted platforms, it injects the &quot;executor&quot; script into the page. The script overrides fetch() and XMLHttpRequest – the fundamental browser APIs that handle all network requests. The script parses the intercepted API responses and then packages and transmits the data via window.postMessage to the extension's content script, along with the identifier PANELOS_MESSAGE. The content script then passes the data to a background service worker for exfiltration over the network to endpoints at analytics.urban-vpn.com and stats.urban-vpn.com.
UK intelligence warns AI 'prompt injection' attacks might never go away (https://therecord.media/prompt-injection-attacks-uk-intelligence-warning): The UK National Cyber Security Centre (NCSC) warns that prompt injection attacks on large language models (LLMs) may never be fully mitigated. Prompt injection attacks manipulate AI systems into ignoring their original instructions by confusing user content with commands. Examples include Microsoft’s New Bing search engine and GitHub’s Copilot. The NCSC’s technical director for platforms research, David C, warns that embedding generative AI into digital systems globally could trigger a wave of security breaches worldwide. Researchers are attempting to develop methods to mitigate these attacks by detecting prompts or training models to differentiate instructions and data. However, all approaches are trying to overlay a concept of “instruction” and “data” on a technology that inherently does not distinguish between the two. The better approach would be to stop considering prompt injection as a form of code injection and instead to view it as what security researchers call a “Confused Deputy” vulnerability.</description>
      <itunes:summary>Articles discussed
Defending against the CVE-2025-55182 (React2Shell) vulnerability in React Server Components (https://www.microsoft.com/en-us/security/blog/2025/12/15/defending-against-the-cve-2025-55182-react2shell-vulnerability-in-react-server-components/): The CVE-2025-55182 (React2Shell) vulnerability is a critical pre-authentication remote code execution vulnerability affecting React Server Components, Next.js, and related frameworks. With a CVSS score of 10.0, this vulnerability could allow attackers to execute arbitrary code on vulnerable servers through a single malicious HTTP request. Exploitation activity related to this vulnerability was detected as early as December 5, 2025. Most successful exploits originated from red team assessments; however, we also observed real-world exploitation attempts by threat actors delivering multiple subsequent payloads, majority of which are coin miners. Both Windows and Linux environments have been observed to be impacted. The React Server Components ecosystem is a collection of packages, frameworks, and bundlers that enable React 19 applications to run parts of their logic on the server rather than the browser. It uses the Flight protocol to communicate between client and server. When a client requests data, the server receives a payload, parses this payload, executes server-side logic, and returns a serialized component tree. The vulnerability exists because affected React Server Components versions fail to validate incoming payloads. This could allow attackers to inject malicious structures that React accepts as valid, leading to prototype pollution and remote code execution. This vulnerability presents a
GhostPoster attacks hide malicious JavaScript in Firefox addon logos (https://www.bleepingcomputer.com/news/security/ghostposter-attacks-hide-malicious-javascript-in-firefox-addon-logos/): The GhostPoster campaign is a new attack that hides malicious JavaScript code in the image logos of over 50,000 popular Firefox extensions. The hidden script is acting as a loader that fetches the main payload from a remote server. Koi Security researchers discovered the GhostPoster campaign and identified 17 compromised Firefox extensions that either read the PNG logo to extract and execute the malware loader or download the main payload from the attacker's server. The malicious extensions are from popular categories: free-vpn-forever screenshot-saved-easy weather-best-forecast crxmouse-gesture cache-fast-site-loader freemp3downloader google-translate-right-clicks google-traductor-esp world-wide-vpn dark-reader-for-ff translator-gbbd i-like-weather google-translate-pro-extension 谷歌-翻译 libretv-watch-free-videos ad-stop right-click-google-translate The JavaScript loader activates 48 hours later to fetch a payload from a hardcoded domain. A second backup domain is available if the payload is not retrieved from the first one. The final payload has the following capabilities: Hijacks affiliate links on major e-commerce sites, redirecting commissions to the attackers. Injects Google Analytics tracking into every page the user visits. Strips security headers from all HTTP responses. Bypasses CAPTCHA via three distinct mechanisms to circumvent bot protections. Injects invisible iframes for ad fraud, click fraud,
Browser 'privacy' extensions have eye on your AI, log all your chats (https://go.theregister.com/feed/www.theregister.com/2025/12/16/chrome_edge_privacy_extensions_quietly/): Four popular browser extensions have been harvesting the text of chatbot conversations from over 8 million people and sending them back to the developers. The four extensions are Urban VPN Proxy, 1ClickVPN Proxy, Urban Browser Guard, and Urban Ad Blocker. They are distributed via the Chrome Web Store and Microsoft Edge Add-ons. The extensions include code designed to capture and transmit browser-based interactions with popular AI tools. Urban VPN Proxy targets conversations across ten AI platforms, including ChatGPT, Claude, Gemini, Microsoft Copilot, Perplexity, DeepSeek, Grok, and Meta AI. The research firm said that the platforms targeted include ChatGPT, Claude, Gemini, Microsoft Copilot, Perplexity, DeepSeek, Grok, and Meta AI. The extension monitors the user's browser tabs and, when the user visits one of the targeted platforms, it injects the &quot;executor&quot; script into the page. The script overrides fetch() and XMLHttpRequest – the fundamental browser APIs that handle all network requests. The script parses the intercepted API responses and then packages and transmits the data via window.postMessage to the extension's content script, along with the identifier PANELOS_MESSAGE. The content script then passes the data to a background service worker for exfiltration over the network to endpoints at analytics.urban-vpn.com and stats.urban-vpn.com.
UK intelligence warns AI 'prompt injection' attacks might never go away (https://therecord.media/prompt-injection-attacks-uk-intelligence-warning): The UK National Cyber Security Centre (NCSC) warns that prompt injection attacks on large language models (LLMs) may never be fully mitigated. Prompt injection attacks manipulate AI systems into ignoring their original instructions by confusing user content with commands. Examples include Microsoft’s New Bing search engine and GitHub’s Copilot. The NCSC’s technical director for platforms research, David C, warns that embedding generative AI into digital systems globally could trigger a wave of security breaches worldwide. Researchers are attempting to develop methods to mitigate these attacks by detecting prompts or training models to differentiate instructions and data. However, all approaches are trying to overlay a concept of “instruction” and “data” on a technology that inherently does not distinguish between the two. The better approach would be to stop considering prompt injection as a form of code injection and instead to view it as what security researchers call a “Confused Deputy” vulnerability.</itunes:summary>
      <description>Articles discussed
Defending against the CVE-2025-55182 (React2Shell) vulnerability in React Server Components (https://www.microsoft.com/en-us/security/blog/2025/12/15/defending-against-the-cve-2025-55182-react2shell-vulnerability-in-react-server-components/): The CVE-2025-55182 (React2Shell) vulnerability is a critical pre-authentication remote code execution vulnerability affecting React Server Components, Next.js, and related frameworks. With a CVSS score of 10.0, this vulnerability could allow attackers to execute arbitrary code on vulnerable servers through a single malicious HTTP request. Exploitation activity related to this vulnerability was detected as early as December 5, 2025. Most successful exploits originated from red team assessments; however, we also observed real-world exploitation attempts by threat actors delivering multiple subsequent payloads, majority of which are coin miners. Both Windows and Linux environments have been observed to be impacted. The React Server Components ecosystem is a collection of packages, frameworks, and bundlers that enable React 19 applications to run parts of their logic on the server rather than the browser. It uses the Flight protocol to communicate between client and server. When a client requests data, the server receives a payload, parses this payload, executes server-side logic, and returns a serialized component tree. The vulnerability exists because affected React Server Components versions fail to validate incoming payloads. This could allow attackers to inject malicious structures that React accepts as valid, leading to prototype pollution and remote code execution. This vulnerability presents a
GhostPoster attacks hide malicious JavaScript in Firefox addon logos (https://www.bleepingcomputer.com/news/security/ghostposter-attacks-hide-malicious-javascript-in-firefox-addon-logos/): The GhostPoster campaign is a new attack that hides malicious JavaScript code in the image logos of over 50,000 popular Firefox extensions. The hidden script is acting as a loader that fetches the main payload from a remote server. Koi Security researchers discovered the GhostPoster campaign and identified 17 compromised Firefox extensions that either read the PNG logo to extract and execute the malware loader or download the main payload from the attacker's server. The malicious extensions are from popular categories: free-vpn-forever screenshot-saved-easy weather-best-forecast crxmouse-gesture cache-fast-site-loader freemp3downloader google-translate-right-clicks google-traductor-esp world-wide-vpn dark-reader-for-ff translator-gbbd i-like-weather google-translate-pro-extension 谷歌-翻译 libretv-watch-free-videos ad-stop right-click-google-translate The JavaScript loader activates 48 hours later to fetch a payload from a hardcoded domain. A second backup domain is available if the payload is not retrieved from the first one. The final payload has the following capabilities: Hijacks affiliate links on major e-commerce sites, redirecting commissions to the attackers. Injects Google Analytics tracking into every page the user visits. Strips security headers from all HTTP responses. Bypasses CAPTCHA via three distinct mechanisms to circumvent bot protections. Injects invisible iframes for ad fraud, click fraud,
Browser 'privacy' extensions have eye on your AI, log all your chats (https://go.theregister.com/feed/www.theregister.com/2025/12/16/chrome_edge_privacy_extensions_quietly/): Four popular browser extensions have been harvesting the text of chatbot conversations from over 8 million people and sending them back to the developers. The four extensions are Urban VPN Proxy, 1ClickVPN Proxy, Urban Browser Guard, and Urban Ad Blocker. They are distributed via the Chrome Web Store and Microsoft Edge Add-ons. The extensions include code designed to capture and transmit browser-based interactions with popular AI tools. Urban VPN Proxy targets conversations across ten AI platforms, including ChatGPT, Claude, Gemini, Microsoft Copilot, Perplexity, DeepSeek, Grok, and Meta AI. The research firm said that the platforms targeted include ChatGPT, Claude, Gemini, Microsoft Copilot, Perplexity, DeepSeek, Grok, and Meta AI. The extension monitors the user's browser tabs and, when the user visits one of the targeted platforms, it injects the &quot;executor&quot; script into the page. The script overrides fetch() and XMLHttpRequest – the fundamental browser APIs that handle all network requests. The script parses the intercepted API responses and then packages and transmits the data via window.postMessage to the extension's content script, along with the identifier PANELOS_MESSAGE. The content script then passes the data to a background service worker for exfiltration over the network to endpoints at analytics.urban-vpn.com and stats.urban-vpn.com.
UK intelligence warns AI 'prompt injection' attacks might never go away (https://therecord.media/prompt-injection-attacks-uk-intelligence-warning): The UK National Cyber Security Centre (NCSC) warns that prompt injection attacks on large language models (LLMs) may never be fully mitigated. Prompt injection attacks manipulate AI systems into ignoring their original instructions by confusing user content with commands. Examples include Microsoft’s New Bing search engine and GitHub’s Copilot. The NCSC’s technical director for platforms research, David C, warns that embedding generative AI into digital systems globally could trigger a wave of security breaches worldwide. Researchers are attempting to develop methods to mitigate these attacks by detecting prompts or training models to differentiate instructions and data. However, all approaches are trying to overlay a concept of “instruction” and “data” on a technology that inherently does not distinguish between the two. The better approach would be to stop considering prompt injection as a form of code injection and instead to view it as what security researchers call a “Confused Deputy” vulnerability.</description>
      <itunes:summary>Articles discussed
Defending against the CVE-2025-55182 (React2Shell) vulnerability in React Server Components (https://www.microsoft.com/en-us/security/blog/2025/12/15/defending-against-the-cve-2025-55182-react2shell-vulnerability-in-react-server-components/): The CVE-2025-55182 (React2Shell) vulnerability is a critical pre-authentication remote code execution vulnerability affecting React Server Components, Next.js, and related frameworks. With a CVSS score of 10.0, this vulnerability could allow attackers to execute arbitrary code on vulnerable servers through a single malicious HTTP request. Exploitation activity related to this vulnerability was detected as early as December 5, 2025. Most successful exploits originated from red team assessments; however, we also observed real-world exploitation attempts by threat actors delivering multiple subsequent payloads, majority of which are coin miners. Both Windows and Linux environments have been observed to be impacted. The React Server Components ecosystem is a collection of packages, frameworks, and bundlers that enable React 19 applications to run parts of their logic on the server rather than the browser. It uses the Flight protocol to communicate between client and server. When a client requests data, the server receives a payload, parses this payload, executes server-side logic, and returns a serialized component tree. The vulnerability exists because affected React Server Components versions fail to validate incoming payloads. This could allow attackers to inject malicious structures that React accepts as valid, leading to prototype pollution and remote code execution. This vulnerability presents a
GhostPoster attacks hide malicious JavaScript in Firefox addon logos (https://www.bleepingcomputer.com/news/security/ghostposter-attacks-hide-malicious-javascript-in-firefox-addon-logos/): The GhostPoster campaign is a new attack that hides malicious JavaScript code in the image logos of over 50,000 popular Firefox extensions. The hidden script is acting as a loader that fetches the main payload from a remote server. Koi Security researchers discovered the GhostPoster campaign and identified 17 compromised Firefox extensions that either read the PNG logo to extract and execute the malware loader or download the main payload from the attacker's server. The malicious extensions are from popular categories: free-vpn-forever screenshot-saved-easy weather-best-forecast crxmouse-gesture cache-fast-site-loader freemp3downloader google-translate-right-clicks google-traductor-esp world-wide-vpn dark-reader-for-ff translator-gbbd i-like-weather google-translate-pro-extension 谷歌-翻译 libretv-watch-free-videos ad-stop right-click-google-translate The JavaScript loader activates 48 hours later to fetch a payload from a hardcoded domain. A second backup domain is available if the payload is not retrieved from the first one. The final payload has the following capabilities: Hijacks affiliate links on major e-commerce sites, redirecting commissions to the attackers. Injects Google Analytics tracking into every page the user visits. Strips security headers from all HTTP responses. Bypasses CAPTCHA via three distinct mechanisms to circumvent bot protections. Injects invisible iframes for ad fraud, click fraud,
Browser 'privacy' extensions have eye on your AI, log all your chats (https://go.theregister.com/feed/www.theregister.com/2025/12/16/chrome_edge_privacy_extensions_quietly/): Four popular browser extensions have been harvesting the text of chatbot conversations from over 8 million people and sending them back to the developers. The four extensions are Urban VPN Proxy, 1ClickVPN Proxy, Urban Browser Guard, and Urban Ad Blocker. They are distributed via the Chrome Web Store and Microsoft Edge Add-ons. The extensions include code designed to capture and transmit browser-based interactions with popular AI tools. Urban VPN Proxy targets conversations across ten AI platforms, including ChatGPT, Claude, Gemini, Microsoft Copilot, Perplexity, DeepSeek, Grok, and Meta AI. The research firm said that the platforms targeted include ChatGPT, Claude, Gemini, Microsoft Copilot, Perplexity, DeepSeek, Grok, and Meta AI. The extension monitors the user's browser tabs and, when the user visits one of the targeted platforms, it injects the &quot;executor&quot; script into the page. The script overrides fetch() and XMLHttpRequest – the fundamental browser APIs that handle all network requests. The script parses the intercepted API responses and then packages and transmits the data via window.postMessage to the extension's content script, along with the identifier PANELOS_MESSAGE. The content script then passes the data to a background service worker for exfiltration over the network to endpoints at analytics.urban-vpn.com and stats.urban-vpn.com.
UK intelligence warns AI 'prompt injection' attacks might never go away (https://therecord.media/prompt-injection-attacks-uk-intelligence-warning): The UK National Cyber Security Centre (NCSC) warns that prompt injection attacks on large language models (LLMs) may never be fully mitigated. Prompt injection attacks manipulate AI systems into ignoring their original instructions by confusing user content with commands. Examples include Microsoft’s New Bing search engine and GitHub’s Copilot. The NCSC’s technical director for platforms research, David C, warns that embedding generative AI into digital systems globally could trigger a wave of security breaches worldwide. Researchers are attempting to develop methods to mitigate these attacks by detecting prompts or training models to differentiate instructions and data. However, all approaches are trying to overlay a concept of “instruction” and “data” on a technology that inherently does not distinguish between the two. The better approach would be to stop considering prompt injection as a form of code injection and instead to view it as what security researchers call a “Confused Deputy” vulnerability.</itunes:summary>
      <pubDate>Sat, 17 Jan 2026 15:58:59 GMT</pubDate>
      <enclosure url="https://raw.githubusercontent.com/Refugeek/podcast-feed/main/episodes-EScyber/20251220070951-defending-against-the-cve-2025-55182-react2shell-vulnerability-in-react-server-components-ghostposter-attacks-hide-malicious-javascript-in-firefox-addon-logos.mp3" type="audio/mpeg" length="29857484"/>
      <itunes:duration>1492</itunes:duration>
    </item>
    <item>
      <title>20251220173557-owasp-drops-first-ai-agent-risk-list-the-subset-sum-problem-solved-in-linear-time-for-dense-enough-inputs</title>
      <description>Articles discussed
OWASP Drops First AI Agent Risk List (https://www.techrepublic.com/article/news-owasp-ai-agent-risk-list/): The OWASP organization has released its first-ever &quot;Top 10 for Agentic Applications&quot; for 2026. This list was developed from input by over 100 security researchers and evaluated by experts from NIST, the European Commission, and the Alan Turing Institute. The list is based on real incidents, not theoretical academic risks. The framework addresses the problem of agentic architectures operating on probabilistic reasoning, untrusted inputs, and making decisions with minimal human oversight. This creates an entirely new attack surface where intent can be hijacked through natural language alone. The list identifies ten different threat models, each representing a fundamentally different threat model than traditional software security.
The Subset Sum Problem Solved in Linear Time for Dense Enough Inputs (https://towardsdatascience.com/subset-sum-problem-solved-in-linear-time-for-dense-enough-inputs/): A new algorithm, called &quot;Interval-based solution,&quot; has been developed to solve the Subset Sum Problem in linear time complexity O(n) for dense enough inputs. The Subset Sum Problem is a well-known NP-complete problem where given 'n' input integers X={x1, x2, …, xn}, and a target sum 'q', it is necessary to figure out if there exists such a subset 'Y' of those 'n' integers, numbers of which will sum up exactly to 'q'. The brute-force technique is a trivial solution to the Subset Sum Problem, which enumerates all possible subsets. The Interval-based solution is a novel algorithm that solves the Subset Sum Problem in linear time complexity O(n) for dense enough inputs.
Dismantling Defenses: Trump 2.0 Cyber Year in Review (https://krebsonsecurity.com/2025/12/dismantling-defenses-trump-2-0-cyber-year-in-review/): The Trump administration has implemented significant policy changes impacting cybersecurity, privacy, and free speech. Key actions include the signing of NSPM-7, directing law enforcement to target anti-American activities, and a memo from Attorney General Pam Bondi urging the FBI to compile a list of Americans potentially involved in domestic terrorism. The administration has also imposed social media restrictions on tourists, requiring extensive data collection, and halted enforcement of the Foreign Corrupt Practices Act, dismantling several investigative units. In the realm of cryptocurrency, the SEC shifted focus from enforcement to supporting the industry, dropping major cases against companies like Coinbase and Binance. Additionally, the administration pardoned Binance founder Changpeng Zhao, amid allegations of conflicts of interest and foreign influence. These actions reflect a broader strategy to weaken regulatory oversight and expand certain industries, while raising concerns about civil liberties and national security.
The ghosts of WhatsApp: How GhostPairing hijacks accounts (https://www.malwarebytes.com/blog/news/2025/12/the-ghosts-of-whatsapp-how-ghostpairing-hijacks-accounts): Researchers have discovered a new attack method called GhostPairing, which exploits WhatsApp's device-pairing feature to hijack accounts. This attack tricks users into completing the pairing process, silently adding the attacker's browser as an invisible linked device. The attack begins with a fake message, often claiming to show a user's photo, leading victims to a login page designed to look like it belongs to Facebook. This page prompts users to log in with their phone number, which the attackers use to submit a WhatsApp pairing request. This allows the attackers to gain direct access to the account, bypassing end-to-end encryption. The impact of this attack is significant, as attackers can read messages, download media, impersonate users, and spread scams. To mitigate this risk, users are advised to avoid unsolicited links, enable two-step verification, and regularly review device connections.
Defending against the CVE-2025-55182 (React2Shell) vulnerability in React Server Components (https://www.microsoft.com/en-us/security/blog/2025/12/15/defending-against-the-cve-2025-55182-react2shell-vulnerability-in-react-server-components/): The CVE-2025-55182 (React2Shell) vulnerability in React Server Components allows attackers to execute arbitrary code through a single malicious HTTP request, posing a significant risk due to its pre-authentication nature and high CVSS score of 10.0. This vulnerability affects Windows and Linux environments and has been exploited by threat actors, including coin miners, for remote code execution. Microsoft Defender researchers have observed exploitation activities, including reverse shell operations and lateral movement to cloud resources, using tactics such as bind mounts and remote monitoring tools. To mitigate the risk, Microsoft recommends immediate patching to affected versions of React and Next.js, prioritizing internet-facing assets, and implementing additional security measures like Azure Web Application Firewall (WAF) protections. The vulnerability's impact highlights the need for robust security practices in enterprise environments relying on React Server Components.
New UEFI flaw enables pre-boot attacks on motherboards from Gigabyte, MSI, ASUS, ASRock (https://www.bleepingcomputer.com/news/security/new-uefi-flaw-enables-pre-boot-attacks-on-motherboards-from-gigabyte-msi-asus-asrock/): A flaw in UEFI firmware on motherboards from ASUS, Gigabyte, MSI, and ASRock allows DMA attacks that bypass early-boot memory protections. Researchers Nick Peterson and Mohamed Al-Sharifi discovered the vulnerability and responsibly disclosed it, working with CERT Taiwan to coordinate a response. The flaw enables malicious PCIe devices to read or modify RAM before the operating system starts, posing a risk to the entire system. Riot Games updated its anti-cheat system Vanguard to block affected systems from launching games like Valorant. The impact extends beyond gaming, affecting any system with DMA-capable PCIe devices. Users are advised to update firmware and ensure data backups.</description>
      <itunes:summary>Articles discussed
OWASP Drops First AI Agent Risk List (https://www.techrepublic.com/article/news-owasp-ai-agent-risk-list/): The OWASP organization has released its first-ever &quot;Top 10 for Agentic Applications&quot; for 2026. This list was developed from input by over 100 security researchers and evaluated by experts from NIST, the European Commission, and the Alan Turing Institute. The list is based on real incidents, not theoretical academic risks. The framework addresses the problem of agentic architectures operating on probabilistic reasoning, untrusted inputs, and making decisions with minimal human oversight. This creates an entirely new attack surface where intent can be hijacked through natural language alone. The list identifies ten different threat models, each representing a fundamentally different threat model than traditional software security.
The Subset Sum Problem Solved in Linear Time for Dense Enough Inputs (https://towardsdatascience.com/subset-sum-problem-solved-in-linear-time-for-dense-enough-inputs/): A new algorithm, called &quot;Interval-based solution,&quot; has been developed to solve the Subset Sum Problem in linear time complexity O(n) for dense enough inputs. The Subset Sum Problem is a well-known NP-complete problem where given 'n' input integers X={x1, x2, …, xn}, and a target sum 'q', it is necessary to figure out if there exists such a subset 'Y' of those 'n' integers, numbers of which will sum up exactly to 'q'. The brute-force technique is a trivial solution to the Subset Sum Problem, which enumerates all possible subsets. The Interval-based solution is a novel algorithm that solves the Subset Sum Problem in linear time complexity O(n) for dense enough inputs.
Dismantling Defenses: Trump 2.0 Cyber Year in Review (https://krebsonsecurity.com/2025/12/dismantling-defenses-trump-2-0-cyber-year-in-review/): The Trump administration has implemented significant policy changes impacting cybersecurity, privacy, and free speech. Key actions include the signing of NSPM-7, directing law enforcement to target anti-American activities, and a memo from Attorney General Pam Bondi urging the FBI to compile a list of Americans potentially involved in domestic terrorism. The administration has also imposed social media restrictions on tourists, requiring extensive data collection, and halted enforcement of the Foreign Corrupt Practices Act, dismantling several investigative units. In the realm of cryptocurrency, the SEC shifted focus from enforcement to supporting the industry, dropping major cases against companies like Coinbase and Binance. Additionally, the administration pardoned Binance founder Changpeng Zhao, amid allegations of conflicts of interest and foreign influence. These actions reflect a broader strategy to weaken regulatory oversight and expand certain industries, while raising concerns about civil liberties and national security.
The ghosts of WhatsApp: How GhostPairing hijacks accounts (https://www.malwarebytes.com/blog/news/2025/12/the-ghosts-of-whatsapp-how-ghostpairing-hijacks-accounts): Researchers have discovered a new attack method called GhostPairing, which exploits WhatsApp's device-pairing feature to hijack accounts. This attack tricks users into completing the pairing process, silently adding the attacker's browser as an invisible linked device. The attack begins with a fake message, often claiming to show a user's photo, leading victims to a login page designed to look like it belongs to Facebook. This page prompts users to log in with their phone number, which the attackers use to submit a WhatsApp pairing request. This allows the attackers to gain direct access to the account, bypassing end-to-end encryption. The impact of this attack is significant, as attackers can read messages, download media, impersonate users, and spread scams. To mitigate this risk, users are advised to avoid unsolicited links, enable two-step verification, and regularly review device connections.
Defending against the CVE-2025-55182 (React2Shell) vulnerability in React Server Components (https://www.microsoft.com/en-us/security/blog/2025/12/15/defending-against-the-cve-2025-55182-react2shell-vulnerability-in-react-server-components/): The CVE-2025-55182 (React2Shell) vulnerability in React Server Components allows attackers to execute arbitrary code through a single malicious HTTP request, posing a significant risk due to its pre-authentication nature and high CVSS score of 10.0. This vulnerability affects Windows and Linux environments and has been exploited by threat actors, including coin miners, for remote code execution. Microsoft Defender researchers have observed exploitation activities, including reverse shell operations and lateral movement to cloud resources, using tactics such as bind mounts and remote monitoring tools. To mitigate the risk, Microsoft recommends immediate patching to affected versions of React and Next.js, prioritizing internet-facing assets, and implementing additional security measures like Azure Web Application Firewall (WAF) protections. The vulnerability's impact highlights the need for robust security practices in enterprise environments relying on React Server Components.
New UEFI flaw enables pre-boot attacks on motherboards from Gigabyte, MSI, ASUS, ASRock (https://www.bleepingcomputer.com/news/security/new-uefi-flaw-enables-pre-boot-attacks-on-motherboards-from-gigabyte-msi-asus-asrock/): A flaw in UEFI firmware on motherboards from ASUS, Gigabyte, MSI, and ASRock allows DMA attacks that bypass early-boot memory protections. Researchers Nick Peterson and Mohamed Al-Sharifi discovered the vulnerability and responsibly disclosed it, working with CERT Taiwan to coordinate a response. The flaw enables malicious PCIe devices to read or modify RAM before the operating system starts, posing a risk to the entire system. Riot Games updated its anti-cheat system Vanguard to block affected systems from launching games like Valorant. The impact extends beyond gaming, affecting any system with DMA-capable PCIe devices. Users are advised to update firmware and ensure data backups.</itunes:summary>
      <description>Articles discussed
OWASP Drops First AI Agent Risk List (https://www.techrepublic.com/article/news-owasp-ai-agent-risk-list/): The OWASP organization has released its first-ever &quot;Top 10 for Agentic Applications&quot; for 2026. This list was developed from input by over 100 security researchers and evaluated by experts from NIST, the European Commission, and the Alan Turing Institute. The list is based on real incidents, not theoretical academic risks. The framework addresses the problem of agentic architectures operating on probabilistic reasoning, untrusted inputs, and making decisions with minimal human oversight. This creates an entirely new attack surface where intent can be hijacked through natural language alone. The list identifies ten different threat models, each representing a fundamentally different threat model than traditional software security.
The Subset Sum Problem Solved in Linear Time for Dense Enough Inputs (https://towardsdatascience.com/subset-sum-problem-solved-in-linear-time-for-dense-enough-inputs/): A new algorithm, called &quot;Interval-based solution,&quot; has been developed to solve the Subset Sum Problem in linear time complexity O(n) for dense enough inputs. The Subset Sum Problem is a well-known NP-complete problem where given 'n' input integers X={x1, x2, …, xn}, and a target sum 'q', it is necessary to figure out if there exists such a subset 'Y' of those 'n' integers, numbers of which will sum up exactly to 'q'. The brute-force technique is a trivial solution to the Subset Sum Problem, which enumerates all possible subsets. The Interval-based solution is a novel algorithm that solves the Subset Sum Problem in linear time complexity O(n) for dense enough inputs.
Dismantling Defenses: Trump 2.0 Cyber Year in Review (https://krebsonsecurity.com/2025/12/dismantling-defenses-trump-2-0-cyber-year-in-review/): The Trump administration has implemented significant policy changes impacting cybersecurity, privacy, and free speech. Key actions include the signing of NSPM-7, directing law enforcement to target anti-American activities, and a memo from Attorney General Pam Bondi urging the FBI to compile a list of Americans potentially involved in domestic terrorism. The administration has also imposed social media restrictions on tourists, requiring extensive data collection, and halted enforcement of the Foreign Corrupt Practices Act, dismantling several investigative units. In the realm of cryptocurrency, the SEC shifted focus from enforcement to supporting the industry, dropping major cases against companies like Coinbase and Binance. Additionally, the administration pardoned Binance founder Changpeng Zhao, amid allegations of conflicts of interest and foreign influence. These actions reflect a broader strategy to weaken regulatory oversight and expand certain industries, while raising concerns about civil liberties and national security.
The ghosts of WhatsApp: How GhostPairing hijacks accounts (https://www.malwarebytes.com/blog/news/2025/12/the-ghosts-of-whatsapp-how-ghostpairing-hijacks-accounts): Researchers have discovered a new attack method called GhostPairing, which exploits WhatsApp's device-pairing feature to hijack accounts. This attack tricks users into completing the pairing process, silently adding the attacker's browser as an invisible linked device. The attack begins with a fake message, often claiming to show a user's photo, leading victims to a login page designed to look like it belongs to Facebook. This page prompts users to log in with their phone number, which the attackers use to submit a WhatsApp pairing request. This allows the attackers to gain direct access to the account, bypassing end-to-end encryption. The impact of this attack is significant, as attackers can read messages, download media, impersonate users, and spread scams. To mitigate this risk, users are advised to avoid unsolicited links, enable two-step verification, and regularly review device connections.
Defending against the CVE-2025-55182 (React2Shell) vulnerability in React Server Components (https://www.microsoft.com/en-us/security/blog/2025/12/15/defending-against-the-cve-2025-55182-react2shell-vulnerability-in-react-server-components/): The CVE-2025-55182 (React2Shell) vulnerability in React Server Components allows attackers to execute arbitrary code through a single malicious HTTP request, posing a significant risk due to its pre-authentication nature and high CVSS score of 10.0. This vulnerability affects Windows and Linux environments and has been exploited by threat actors, including coin miners, for remote code execution. Microsoft Defender researchers have observed exploitation activities, including reverse shell operations and lateral movement to cloud resources, using tactics such as bind mounts and remote monitoring tools. To mitigate the risk, Microsoft recommends immediate patching to affected versions of React and Next.js, prioritizing internet-facing assets, and implementing additional security measures like Azure Web Application Firewall (WAF) protections. The vulnerability's impact highlights the need for robust security practices in enterprise environments relying on React Server Components.
New UEFI flaw enables pre-boot attacks on motherboards from Gigabyte, MSI, ASUS, ASRock (https://www.bleepingcomputer.com/news/security/new-uefi-flaw-enables-pre-boot-attacks-on-motherboards-from-gigabyte-msi-asus-asrock/): A flaw in UEFI firmware on motherboards from ASUS, Gigabyte, MSI, and ASRock allows DMA attacks that bypass early-boot memory protections. Researchers Nick Peterson and Mohamed Al-Sharifi discovered the vulnerability and responsibly disclosed it, working with CERT Taiwan to coordinate a response. The flaw enables malicious PCIe devices to read or modify RAM before the operating system starts, posing a risk to the entire system. Riot Games updated its anti-cheat system Vanguard to block affected systems from launching games like Valorant. The impact extends beyond gaming, affecting any system with DMA-capable PCIe devices. Users are advised to update firmware and ensure data backups.</description>
      <itunes:summary>Articles discussed
OWASP Drops First AI Agent Risk List (https://www.techrepublic.com/article/news-owasp-ai-agent-risk-list/): The OWASP organization has released its first-ever &quot;Top 10 for Agentic Applications&quot; for 2026. This list was developed from input by over 100 security researchers and evaluated by experts from NIST, the European Commission, and the Alan Turing Institute. The list is based on real incidents, not theoretical academic risks. The framework addresses the problem of agentic architectures operating on probabilistic reasoning, untrusted inputs, and making decisions with minimal human oversight. This creates an entirely new attack surface where intent can be hijacked through natural language alone. The list identifies ten different threat models, each representing a fundamentally different threat model than traditional software security.
The Subset Sum Problem Solved in Linear Time for Dense Enough Inputs (https://towardsdatascience.com/subset-sum-problem-solved-in-linear-time-for-dense-enough-inputs/): A new algorithm, called &quot;Interval-based solution,&quot; has been developed to solve the Subset Sum Problem in linear time complexity O(n) for dense enough inputs. The Subset Sum Problem is a well-known NP-complete problem where given 'n' input integers X={x1, x2, …, xn}, and a target sum 'q', it is necessary to figure out if there exists such a subset 'Y' of those 'n' integers, numbers of which will sum up exactly to 'q'. The brute-force technique is a trivial solution to the Subset Sum Problem, which enumerates all possible subsets. The Interval-based solution is a novel algorithm that solves the Subset Sum Problem in linear time complexity O(n) for dense enough inputs.
Dismantling Defenses: Trump 2.0 Cyber Year in Review (https://krebsonsecurity.com/2025/12/dismantling-defenses-trump-2-0-cyber-year-in-review/): The Trump administration has implemented significant policy changes impacting cybersecurity, privacy, and free speech. Key actions include the signing of NSPM-7, directing law enforcement to target anti-American activities, and a memo from Attorney General Pam Bondi urging the FBI to compile a list of Americans potentially involved in domestic terrorism. The administration has also imposed social media restrictions on tourists, requiring extensive data collection, and halted enforcement of the Foreign Corrupt Practices Act, dismantling several investigative units. In the realm of cryptocurrency, the SEC shifted focus from enforcement to supporting the industry, dropping major cases against companies like Coinbase and Binance. Additionally, the administration pardoned Binance founder Changpeng Zhao, amid allegations of conflicts of interest and foreign influence. These actions reflect a broader strategy to weaken regulatory oversight and expand certain industries, while raising concerns about civil liberties and national security.
The ghosts of WhatsApp: How GhostPairing hijacks accounts (https://www.malwarebytes.com/blog/news/2025/12/the-ghosts-of-whatsapp-how-ghostpairing-hijacks-accounts): Researchers have discovered a new attack method called GhostPairing, which exploits WhatsApp's device-pairing feature to hijack accounts. This attack tricks users into completing the pairing process, silently adding the attacker's browser as an invisible linked device. The attack begins with a fake message, often claiming to show a user's photo, leading victims to a login page designed to look like it belongs to Facebook. This page prompts users to log in with their phone number, which the attackers use to submit a WhatsApp pairing request. This allows the attackers to gain direct access to the account, bypassing end-to-end encryption. The impact of this attack is significant, as attackers can read messages, download media, impersonate users, and spread scams. To mitigate this risk, users are advised to avoid unsolicited links, enable two-step verification, and regularly review device connections.
Defending against the CVE-2025-55182 (React2Shell) vulnerability in React Server Components (https://www.microsoft.com/en-us/security/blog/2025/12/15/defending-against-the-cve-2025-55182-react2shell-vulnerability-in-react-server-components/): The CVE-2025-55182 (React2Shell) vulnerability in React Server Components allows attackers to execute arbitrary code through a single malicious HTTP request, posing a significant risk due to its pre-authentication nature and high CVSS score of 10.0. This vulnerability affects Windows and Linux environments and has been exploited by threat actors, including coin miners, for remote code execution. Microsoft Defender researchers have observed exploitation activities, including reverse shell operations and lateral movement to cloud resources, using tactics such as bind mounts and remote monitoring tools. To mitigate the risk, Microsoft recommends immediate patching to affected versions of React and Next.js, prioritizing internet-facing assets, and implementing additional security measures like Azure Web Application Firewall (WAF) protections. The vulnerability's impact highlights the need for robust security practices in enterprise environments relying on React Server Components.
New UEFI flaw enables pre-boot attacks on motherboards from Gigabyte, MSI, ASUS, ASRock (https://www.bleepingcomputer.com/news/security/new-uefi-flaw-enables-pre-boot-attacks-on-motherboards-from-gigabyte-msi-asus-asrock/): A flaw in UEFI firmware on motherboards from ASUS, Gigabyte, MSI, and ASRock allows DMA attacks that bypass early-boot memory protections. Researchers Nick Peterson and Mohamed Al-Sharifi discovered the vulnerability and responsibly disclosed it, working with CERT Taiwan to coordinate a response. The flaw enables malicious PCIe devices to read or modify RAM before the operating system starts, posing a risk to the entire system. Riot Games updated its anti-cheat system Vanguard to block affected systems from launching games like Valorant. The impact extends beyond gaming, affecting any system with DMA-capable PCIe devices. Users are advised to update firmware and ensure data backups.</itunes:summary>
      <pubDate>Sat, 17 Jan 2026 15:58:59 GMT</pubDate>
      <enclosure url="https://raw.githubusercontent.com/Refugeek/podcast-feed/main/episodes-EScyber/20251220173557-owasp-drops-first-ai-agent-risk-list-the-subset-sum-problem-solved-in-linear-time-for-dense-enough-inputs.mp3" type="audio/mpeg" length="41254604"/>
      <itunes:duration>2062</itunes:duration>
    </item>
    <item>
      <title>20251224063813-industrial-routers-bear-the-brunt-of-ot-cyberattacks-new-forescout-research-finds-understanding-vibe-proving</title>
      <description>Articles discussed
Industrial routers bear the brunt of OT cyberattacks, new Forescout research finds (https://www.itsecurityguru.org/2025/12/18/industrial-routers-bear-the-brunt-of-ot-cyberattacks-new-forescout-research-finds/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=industrial-routers-bear-the-brunt-of-ot-cyberattacks-new-forescout-research-finds): Industrial routers and other operational technology (OT) perimeter devices are absorbing the majority of cyberattacks targeting OT environments, according to new Forescout Vedere Labs research. Analyzing 90 days of real-world honeypot data, researchers found that 67% of malicious activity was directed at OT perimeter devices, such as industrial routers and firewalls, compared with 33% aimed at directly exposed OT assets like PLCs and HMIs. The findings highlight the growing risk facing edge devices that sit between IT and OT networks. Automated attacks dominate the OT perimeter, with more than 60 million requests logged across 11 devices in just three months. Researchers identified several malware families actively targeting OT perimeter devices, including RondoDox, Redtail, and ShadowV2. Chaya_005: a long-running reconnaissance campaign was one of the most significant findings, active for at least two years, appears to focus on fingerprinting and capability testing of industrial edge devices, rather than immediate mass exploitation. The research also highlights the growing interest of hacktivist groups in OT targets. Security teams are urged to rethink IT/OT boundaries.
Understanding Vibe Proving (https://towardsdatascience.com/understanding-vibe-proving-part-1/): Vibe proving is a novel approach in mathematics where AI systems, particularly LLMs, generate mathematical proofs that are then verified by specialized software. This method has seen significant success, with DeepMind achieving gold at the International Mathematical Olympiad and Harmonic solving a long-standing problem in number theory. The process involves training LLMs to produce candidate proofs, which are then checked by external verification software to ensure correctness. This approach raises questions about the reliability of LLMs in generating mathematical reasoning and the mechanisms behind the verification software. The novelty lies in the combination of AI's creative potential in generating proofs with the precision of formal verification to distinguish valid reasoning from hallucinations. This development marks a potential shift in how mathematical proofs are approached and verified, highlighting the intersection of AI and formal logic.
What Happens When You Build an LLM Using Only 1s and 0s (https://towardsdatascience.com/what-happens-when-you-build-an-llm-using-only-1s-and-0s/): Researchers from Microsoft have developed BitNet b1.58, a novel architecture for training large language models (LLMs) using only three possible values: {-1, 0, 1}. This approach eliminates the need for complex floating-point multiplications, which are the most expensive operations in modern deep learning hardware. BitNet b1.58 achieves performance parity with full-precision models, outperforming them on benchmarks like ARC-Challenge, Hellaswag, and Winogrande. The architecture also eliminates the need for matrix multiplication, which is a significant energy saver.</description>
      <itunes:summary>Articles discussed
Industrial routers bear the brunt of OT cyberattacks, new Forescout research finds (https://www.itsecurityguru.org/2025/12/18/industrial-routers-bear-the-brunt-of-ot-cyberattacks-new-forescout-research-finds/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=industrial-routers-bear-the-brunt-of-ot-cyberattacks-new-forescout-research-finds): Industrial routers and other operational technology (OT) perimeter devices are absorbing the majority of cyberattacks targeting OT environments, according to new Forescout Vedere Labs research. Analyzing 90 days of real-world honeypot data, researchers found that 67% of malicious activity was directed at OT perimeter devices, such as industrial routers and firewalls, compared with 33% aimed at directly exposed OT assets like PLCs and HMIs. The findings highlight the growing risk facing edge devices that sit between IT and OT networks. Automated attacks dominate the OT perimeter, with more than 60 million requests logged across 11 devices in just three months. Researchers identified several malware families actively targeting OT perimeter devices, including RondoDox, Redtail, and ShadowV2. Chaya_005: a long-running reconnaissance campaign was one of the most significant findings, active for at least two years, appears to focus on fingerprinting and capability testing of industrial edge devices, rather than immediate mass exploitation. The research also highlights the growing interest of hacktivist groups in OT targets. Security teams are urged to rethink IT/OT boundaries.
Understanding Vibe Proving (https://towardsdatascience.com/understanding-vibe-proving-part-1/): Vibe proving is a novel approach in mathematics where AI systems, particularly LLMs, generate mathematical proofs that are then verified by specialized software. This method has seen significant success, with DeepMind achieving gold at the International Mathematical Olympiad and Harmonic solving a long-standing problem in number theory. The process involves training LLMs to produce candidate proofs, which are then checked by external verification software to ensure correctness. This approach raises questions about the reliability of LLMs in generating mathematical reasoning and the mechanisms behind the verification software. The novelty lies in the combination of AI's creative potential in generating proofs with the precision of formal verification to distinguish valid reasoning from hallucinations. This development marks a potential shift in how mathematical proofs are approached and verified, highlighting the intersection of AI and formal logic.
What Happens When You Build an LLM Using Only 1s and 0s (https://towardsdatascience.com/what-happens-when-you-build-an-llm-using-only-1s-and-0s/): Researchers from Microsoft have developed BitNet b1.58, a novel architecture for training large language models (LLMs) using only three possible values: {-1, 0, 1}. This approach eliminates the need for complex floating-point multiplications, which are the most expensive operations in modern deep learning hardware. BitNet b1.58 achieves performance parity with full-precision models, outperforming them on benchmarks like ARC-Challenge, Hellaswag, and Winogrande. The architecture also eliminates the need for matrix multiplication, which is a significant energy saver.</itunes:summary>
      <description>Articles discussed
Industrial routers bear the brunt of OT cyberattacks, new Forescout research finds (https://www.itsecurityguru.org/2025/12/18/industrial-routers-bear-the-brunt-of-ot-cyberattacks-new-forescout-research-finds/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=industrial-routers-bear-the-brunt-of-ot-cyberattacks-new-forescout-research-finds): Industrial routers and other operational technology (OT) perimeter devices are absorbing the majority of cyberattacks targeting OT environments, according to new Forescout Vedere Labs research. Analyzing 90 days of real-world honeypot data, researchers found that 67% of malicious activity was directed at OT perimeter devices, such as industrial routers and firewalls, compared with 33% aimed at directly exposed OT assets like PLCs and HMIs. The findings highlight the growing risk facing edge devices that sit between IT and OT networks. Automated attacks dominate the OT perimeter, with more than 60 million requests logged across 11 devices in just three months. Researchers identified several malware families actively targeting OT perimeter devices, including RondoDox, Redtail, and ShadowV2. Chaya_005: a long-running reconnaissance campaign was one of the most significant findings, active for at least two years, appears to focus on fingerprinting and capability testing of industrial edge devices, rather than immediate mass exploitation. The research also highlights the growing interest of hacktivist groups in OT targets. Security teams are urged to rethink IT/OT boundaries.
Understanding Vibe Proving (https://towardsdatascience.com/understanding-vibe-proving-part-1/): Vibe proving is a novel approach in mathematics where AI systems, particularly LLMs, generate mathematical proofs that are then verified by specialized software. This method has seen significant success, with DeepMind achieving gold at the International Mathematical Olympiad and Harmonic solving a long-standing problem in number theory. The process involves training LLMs to produce candidate proofs, which are then checked by external verification software to ensure correctness. This approach raises questions about the reliability of LLMs in generating mathematical reasoning and the mechanisms behind the verification software. The novelty lies in the combination of AI's creative potential in generating proofs with the precision of formal verification to distinguish valid reasoning from hallucinations. This development marks a potential shift in how mathematical proofs are approached and verified, highlighting the intersection of AI and formal logic.
What Happens When You Build an LLM Using Only 1s and 0s (https://towardsdatascience.com/what-happens-when-you-build-an-llm-using-only-1s-and-0s/): Researchers from Microsoft have developed BitNet b1.58, a novel architecture for training large language models (LLMs) using only three possible values: {-1, 0, 1}. This approach eliminates the need for complex floating-point multiplications, which are the most expensive operations in modern deep learning hardware. BitNet b1.58 achieves performance parity with full-precision models, outperforming them on benchmarks like ARC-Challenge, Hellaswag, and Winogrande. The architecture also eliminates the need for matrix multiplication, which is a significant energy saver.</description>
      <itunes:summary>Articles discussed
Industrial routers bear the brunt of OT cyberattacks, new Forescout research finds (https://www.itsecurityguru.org/2025/12/18/industrial-routers-bear-the-brunt-of-ot-cyberattacks-new-forescout-research-finds/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=industrial-routers-bear-the-brunt-of-ot-cyberattacks-new-forescout-research-finds): Industrial routers and other operational technology (OT) perimeter devices are absorbing the majority of cyberattacks targeting OT environments, according to new Forescout Vedere Labs research. Analyzing 90 days of real-world honeypot data, researchers found that 67% of malicious activity was directed at OT perimeter devices, such as industrial routers and firewalls, compared with 33% aimed at directly exposed OT assets like PLCs and HMIs. The findings highlight the growing risk facing edge devices that sit between IT and OT networks. Automated attacks dominate the OT perimeter, with more than 60 million requests logged across 11 devices in just three months. Researchers identified several malware families actively targeting OT perimeter devices, including RondoDox, Redtail, and ShadowV2. Chaya_005: a long-running reconnaissance campaign was one of the most significant findings, active for at least two years, appears to focus on fingerprinting and capability testing of industrial edge devices, rather than immediate mass exploitation. The research also highlights the growing interest of hacktivist groups in OT targets. Security teams are urged to rethink IT/OT boundaries.
Understanding Vibe Proving (https://towardsdatascience.com/understanding-vibe-proving-part-1/): Vibe proving is a novel approach in mathematics where AI systems, particularly LLMs, generate mathematical proofs that are then verified by specialized software. This method has seen significant success, with DeepMind achieving gold at the International Mathematical Olympiad and Harmonic solving a long-standing problem in number theory. The process involves training LLMs to produce candidate proofs, which are then checked by external verification software to ensure correctness. This approach raises questions about the reliability of LLMs in generating mathematical reasoning and the mechanisms behind the verification software. The novelty lies in the combination of AI's creative potential in generating proofs with the precision of formal verification to distinguish valid reasoning from hallucinations. This development marks a potential shift in how mathematical proofs are approached and verified, highlighting the intersection of AI and formal logic.
What Happens When You Build an LLM Using Only 1s and 0s (https://towardsdatascience.com/what-happens-when-you-build-an-llm-using-only-1s-and-0s/): Researchers from Microsoft have developed BitNet b1.58, a novel architecture for training large language models (LLMs) using only three possible values: {-1, 0, 1}. This approach eliminates the need for complex floating-point multiplications, which are the most expensive operations in modern deep learning hardware. BitNet b1.58 achieves performance parity with full-precision models, outperforming them on benchmarks like ARC-Challenge, Hellaswag, and Winogrande. The architecture also eliminates the need for matrix multiplication, which is a significant energy saver.</itunes:summary>
      <pubDate>Sat, 17 Jan 2026 15:58:59 GMT</pubDate>
      <enclosure url="https://raw.githubusercontent.com/Refugeek/podcast-feed/main/episodes-EScyber/20251224063813-industrial-routers-bear-the-brunt-of-ot-cyberattacks-new-forescout-research-finds-understanding-vibe-proving.mp3" type="audio/mpeg" length="33580844"/>
      <itunes:duration>1679</itunes:duration>
    </item>
    <item>
      <title>20260109102244-retrieval-for-time-series-how-looking-back-improves-forecasts-i-evaluated-half-a-million-credit-records-with-federated-learning-heres-what-i-found</title>
      <description>Articles discussed
Retrieval for Time-Series: How Looking Back Improves Forecasts (https://towardsdatascience.com/retrieval-for-time-series-how-looking-back-improves-forecasts/): Retrieval-augmented forecasting (RAF) is a method that improves time-series forecasting by leveraging historical data to enhance predictions. This approach involves converting current situations into queries and searching databases of historical time-series segments to find similar patterns. By integrating retrieved information into forecasting models, RAF addresses challenges posed by rare events, zero-shot scenarios, and evolving seasonal trends. Key actors in this development include researchers and engineers who have implemented RAF in real-world forecasting pipelines. The method employs structured mechanisms, such as embedding-based similarity measures, to efficiently search large databases for relevant historical patterns. The impact of RAF lies in its ability to provide context and grounding for forecasts, making predictions more reliable, especially in unpredictable scenarios. This innovation marks a significant advancement in time-series forecasting by augmenting traditional models with external historical knowledge.
I Evaluated Half a Million Credit Records with Federated Learning. Here’s What I Found (https://towardsdatascience.com/i-evaluated-half-a-million-credit-records-with-federated-learning-heres-what-i-found/): Federated learning has been shown to be a viable solution to the &quot;regulator's paradox&quot; of credit scoring, where a bank must comply with three conflicting mandates: privacy, fairness, and accuracy. At a small scale, achieving all three simultaneously is difficult, but at an enterprise scale, federated learning can solve this paradox. At a production scale of 300 federated institutions collaborating, accuracy reached 96.94%, fairness gap was 0.069%, and privacy was guaranteed at ε = 1.0.
2025 exposed the risks we ignored while rushing AI (https://www.malwarebytes.com/blog/news/2025/12/2025-exposed-risks-we-ignored-while-rushing-ai): The rapid advancement of Artificial Intelligence (AI) has introduced significant security and privacy risks, particularly with the rise of agentic browsers that can autonomously execute tasks. These browsers have been exploited by attackers, such as those who tricked OpenAI's Atlas into executing malicious commands by manipulating URL inputs. Scammers have also created spoofed AI interfaces that mimic real ones, making them difficult to detect. Misconfigurations in AI-enhanced products, like a plush teddy bear that generated inappropriate content, highlight the potential for unintended consequences. Additionally, AI systems sometimes misinterpret inputs, as seen in a school incident where an AI system mistook a bag for a gun, leading to a police response. Privacy concerns have also surged, with data breaches linked to mishandled chat logs and private conversations being exposed without user consent. The article emphasizes the need for caution and informed decision-making as consumers navigate these evolving AI technologies.
The Real-World Attacks Behind OWASP Agentic AI Top 10 (https://www.bleepingcomputer.com/news/security/the-real-world-attacks-behind-owasp-agentic-ai-top-10/): The OWASP Agentic Top 10 identifies ten risk categories specific to autonomous AI systems, including agent goal hijack, tool misuse, identity and privilege abuse, supply chain vulnerabilities, unexpected code execution, memory and context poisoning, insecure inter-agent communication, cascading failures, human-agent trust exploitation, and rogue agents. The framework aims to provide a shared language for these risks, improving defenses across the industry. The attacks documented this year include prompt injection in malware, poisoned AI assistants, malicious MCP servers, and invisible dependencies. These threats are already happening now, with the OWASP Agentic Top 10 providing a framework for understanding and coordinating defenses against them.</description>
      <itunes:summary>Articles discussed
Retrieval for Time-Series: How Looking Back Improves Forecasts (https://towardsdatascience.com/retrieval-for-time-series-how-looking-back-improves-forecasts/): Retrieval-augmented forecasting (RAF) is a method that improves time-series forecasting by leveraging historical data to enhance predictions. This approach involves converting current situations into queries and searching databases of historical time-series segments to find similar patterns. By integrating retrieved information into forecasting models, RAF addresses challenges posed by rare events, zero-shot scenarios, and evolving seasonal trends. Key actors in this development include researchers and engineers who have implemented RAF in real-world forecasting pipelines. The method employs structured mechanisms, such as embedding-based similarity measures, to efficiently search large databases for relevant historical patterns. The impact of RAF lies in its ability to provide context and grounding for forecasts, making predictions more reliable, especially in unpredictable scenarios. This innovation marks a significant advancement in time-series forecasting by augmenting traditional models with external historical knowledge.
I Evaluated Half a Million Credit Records with Federated Learning. Here’s What I Found (https://towardsdatascience.com/i-evaluated-half-a-million-credit-records-with-federated-learning-heres-what-i-found/): Federated learning has been shown to be a viable solution to the &quot;regulator's paradox&quot; of credit scoring, where a bank must comply with three conflicting mandates: privacy, fairness, and accuracy. At a small scale, achieving all three simultaneously is difficult, but at an enterprise scale, federated learning can solve this paradox. At a production scale of 300 federated institutions collaborating, accuracy reached 96.94%, fairness gap was 0.069%, and privacy was guaranteed at ε = 1.0.
2025 exposed the risks we ignored while rushing AI (https://www.malwarebytes.com/blog/news/2025/12/2025-exposed-risks-we-ignored-while-rushing-ai): The rapid advancement of Artificial Intelligence (AI) has introduced significant security and privacy risks, particularly with the rise of agentic browsers that can autonomously execute tasks. These browsers have been exploited by attackers, such as those who tricked OpenAI's Atlas into executing malicious commands by manipulating URL inputs. Scammers have also created spoofed AI interfaces that mimic real ones, making them difficult to detect. Misconfigurations in AI-enhanced products, like a plush teddy bear that generated inappropriate content, highlight the potential for unintended consequences. Additionally, AI systems sometimes misinterpret inputs, as seen in a school incident where an AI system mistook a bag for a gun, leading to a police response. Privacy concerns have also surged, with data breaches linked to mishandled chat logs and private conversations being exposed without user consent. The article emphasizes the need for caution and informed decision-making as consumers navigate these evolving AI technologies.
The Real-World Attacks Behind OWASP Agentic AI Top 10 (https://www.bleepingcomputer.com/news/security/the-real-world-attacks-behind-owasp-agentic-ai-top-10/): The OWASP Agentic Top 10 identifies ten risk categories specific to autonomous AI systems, including agent goal hijack, tool misuse, identity and privilege abuse, supply chain vulnerabilities, unexpected code execution, memory and context poisoning, insecure inter-agent communication, cascading failures, human-agent trust exploitation, and rogue agents. The framework aims to provide a shared language for these risks, improving defenses across the industry. The attacks documented this year include prompt injection in malware, poisoned AI assistants, malicious MCP servers, and invisible dependencies. These threats are already happening now, with the OWASP Agentic Top 10 providing a framework for understanding and coordinating defenses against them.</itunes:summary>
      <description>Articles discussed
Retrieval for Time-Series: How Looking Back Improves Forecasts (https://towardsdatascience.com/retrieval-for-time-series-how-looking-back-improves-forecasts/): Retrieval-augmented forecasting (RAF) is a method that improves time-series forecasting by leveraging historical data to enhance predictions. This approach involves converting current situations into queries and searching databases of historical time-series segments to find similar patterns. By integrating retrieved information into forecasting models, RAF addresses challenges posed by rare events, zero-shot scenarios, and evolving seasonal trends. Key actors in this development include researchers and engineers who have implemented RAF in real-world forecasting pipelines. The method employs structured mechanisms, such as embedding-based similarity measures, to efficiently search large databases for relevant historical patterns. The impact of RAF lies in its ability to provide context and grounding for forecasts, making predictions more reliable, especially in unpredictable scenarios. This innovation marks a significant advancement in time-series forecasting by augmenting traditional models with external historical knowledge.
I Evaluated Half a Million Credit Records with Federated Learning. Here’s What I Found (https://towardsdatascience.com/i-evaluated-half-a-million-credit-records-with-federated-learning-heres-what-i-found/): Federated learning has been shown to be a viable solution to the &quot;regulator's paradox&quot; of credit scoring, where a bank must comply with three conflicting mandates: privacy, fairness, and accuracy. At a small scale, achieving all three simultaneously is difficult, but at an enterprise scale, federated learning can solve this paradox. At a production scale of 300 federated institutions collaborating, accuracy reached 96.94%, fairness gap was 0.069%, and privacy was guaranteed at ε = 1.0.
2025 exposed the risks we ignored while rushing AI (https://www.malwarebytes.com/blog/news/2025/12/2025-exposed-risks-we-ignored-while-rushing-ai): The rapid advancement of Artificial Intelligence (AI) has introduced significant security and privacy risks, particularly with the rise of agentic browsers that can autonomously execute tasks. These browsers have been exploited by attackers, such as those who tricked OpenAI's Atlas into executing malicious commands by manipulating URL inputs. Scammers have also created spoofed AI interfaces that mimic real ones, making them difficult to detect. Misconfigurations in AI-enhanced products, like a plush teddy bear that generated inappropriate content, highlight the potential for unintended consequences. Additionally, AI systems sometimes misinterpret inputs, as seen in a school incident where an AI system mistook a bag for a gun, leading to a police response. Privacy concerns have also surged, with data breaches linked to mishandled chat logs and private conversations being exposed without user consent. The article emphasizes the need for caution and informed decision-making as consumers navigate these evolving AI technologies.
The Real-World Attacks Behind OWASP Agentic AI Top 10 (https://www.bleepingcomputer.com/news/security/the-real-world-attacks-behind-owasp-agentic-ai-top-10/): The OWASP Agentic Top 10 identifies ten risk categories specific to autonomous AI systems, including agent goal hijack, tool misuse, identity and privilege abuse, supply chain vulnerabilities, unexpected code execution, memory and context poisoning, insecure inter-agent communication, cascading failures, human-agent trust exploitation, and rogue agents. The framework aims to provide a shared language for these risks, improving defenses across the industry. The attacks documented this year include prompt injection in malware, poisoned AI assistants, malicious MCP servers, and invisible dependencies. These threats are already happening now, with the OWASP Agentic Top 10 providing a framework for understanding and coordinating defenses against them.</description>
      <itunes:summary>Articles discussed
Retrieval for Time-Series: How Looking Back Improves Forecasts (https://towardsdatascience.com/retrieval-for-time-series-how-looking-back-improves-forecasts/): Retrieval-augmented forecasting (RAF) is a method that improves time-series forecasting by leveraging historical data to enhance predictions. This approach involves converting current situations into queries and searching databases of historical time-series segments to find similar patterns. By integrating retrieved information into forecasting models, RAF addresses challenges posed by rare events, zero-shot scenarios, and evolving seasonal trends. Key actors in this development include researchers and engineers who have implemented RAF in real-world forecasting pipelines. The method employs structured mechanisms, such as embedding-based similarity measures, to efficiently search large databases for relevant historical patterns. The impact of RAF lies in its ability to provide context and grounding for forecasts, making predictions more reliable, especially in unpredictable scenarios. This innovation marks a significant advancement in time-series forecasting by augmenting traditional models with external historical knowledge.
I Evaluated Half a Million Credit Records with Federated Learning. Here’s What I Found (https://towardsdatascience.com/i-evaluated-half-a-million-credit-records-with-federated-learning-heres-what-i-found/): Federated learning has been shown to be a viable solution to the &quot;regulator's paradox&quot; of credit scoring, where a bank must comply with three conflicting mandates: privacy, fairness, and accuracy. At a small scale, achieving all three simultaneously is difficult, but at an enterprise scale, federated learning can solve this paradox. At a production scale of 300 federated institutions collaborating, accuracy reached 96.94%, fairness gap was 0.069%, and privacy was guaranteed at ε = 1.0.
2025 exposed the risks we ignored while rushing AI (https://www.malwarebytes.com/blog/news/2025/12/2025-exposed-risks-we-ignored-while-rushing-ai): The rapid advancement of Artificial Intelligence (AI) has introduced significant security and privacy risks, particularly with the rise of agentic browsers that can autonomously execute tasks. These browsers have been exploited by attackers, such as those who tricked OpenAI's Atlas into executing malicious commands by manipulating URL inputs. Scammers have also created spoofed AI interfaces that mimic real ones, making them difficult to detect. Misconfigurations in AI-enhanced products, like a plush teddy bear that generated inappropriate content, highlight the potential for unintended consequences. Additionally, AI systems sometimes misinterpret inputs, as seen in a school incident where an AI system mistook a bag for a gun, leading to a police response. Privacy concerns have also surged, with data breaches linked to mishandled chat logs and private conversations being exposed without user consent. The article emphasizes the need for caution and informed decision-making as consumers navigate these evolving AI technologies.
The Real-World Attacks Behind OWASP Agentic AI Top 10 (https://www.bleepingcomputer.com/news/security/the-real-world-attacks-behind-owasp-agentic-ai-top-10/): The OWASP Agentic Top 10 identifies ten risk categories specific to autonomous AI systems, including agent goal hijack, tool misuse, identity and privilege abuse, supply chain vulnerabilities, unexpected code execution, memory and context poisoning, insecure inter-agent communication, cascading failures, human-agent trust exploitation, and rogue agents. The framework aims to provide a shared language for these risks, improving defenses across the industry. The attacks documented this year include prompt injection in malware, poisoned AI assistants, malicious MCP servers, and invisible dependencies. These threats are already happening now, with the OWASP Agentic Top 10 providing a framework for understanding and coordinating defenses against them.</itunes:summary>
      <pubDate>Sat, 17 Jan 2026 15:58:59 GMT</pubDate>
      <enclosure url="https://raw.githubusercontent.com/Refugeek/podcast-feed/main/episodes-EScyber/20260109102244-retrieval-for-time-series-how-looking-back-improves-forecasts-i-evaluated-half-a-million-credit-records-with-federated-learning-heres-what-i-found.mp3" type="audio/mpeg" length="37795724"/>
      <itunes:duration>1889</itunes:duration>
    </item>
    <item>
      <title>20260113182254-how-llms-handle-infinite-context-with-finite-memory-remote-code-execution-with-modern-ai-ml-formats-and-libraries</title>
      <description>Articles discussed
How LLMs Handle Infinite Context With Finite Memory (https://towardsdatascience.com/llms-can-now-process-infinite-context-windows/): Researchers at Google developed Infini-attention, an innovative approach to handling long-context sequences in AI language models. This method addresses the memory limitations of traditional Transformer architectures by compressing the entire historical context into a fixed-size Memory Matrix. This enables efficient memory usage, reducing the required parameters by 114 times compared to previous models. Infini-attention achieves state-of-the-art perplexity scores on benchmarks like PG19 and Arxiv-math while using significantly less memory. The method involves segmenting input sequences, compressing relevant information into a Memory Matrix, and retrieving this data for generating subsequent tokens. This approach allows for efficient long-term memory retention without the need for extensive GPU VRAM resources.
Remote Code Execution With Modern AI/ML Formats and Libraries (https://unit42.paloaltonetworks.com/rce-vulnerabilities-in-ai-python-libraries/): Researchers have identified vulnerabilities in three open-source AI/ML Python libraries: NeMo, Uni2TS, and FlexTok. These vulnerabilities allow for remote code execution (RCE) when malicious metadata is embedded in model files. Affected libraries are used in popular models on HuggingFace, with tens of millions of downloads. Palo Alto Networks notified vendors in April 2025, and fixes were released by NVIDIA, Salesforce, and Apple by July 2025. The vulnerabilities were discovered by Prisma AIRS, which can identify models using these vulnerabilities and extract payloads. Hydra, a Python library used by these models, has been updated to warn users of RCE risks and add a block-list mechanism to mitigate the issue.
Facebook login thieves now using browser-in-browser trick (https://www.bleepingcomputer.com/news/security/facebook-login-thieves-now-using-browser-in-browser-trick/): Hackers have developed a new phishing technique called browser-in-browser (BitB) to trick users into providing Facebook account credentials. This technique was developed by security researcher mr.d0x in 2022 and later adopted by cybercriminals targeting various online services, including Facebook and Steam. BitB attacks involve presenting users with a fake browser pop-up containing a login form, implemented using an iframe that imitates the authentication interface of legitimate platforms. Cybercriminals have added shortened URLs and fake Meta CAPTCHA pages to increase the legitimacy of the phishing pages. These campaigns constitute a significant evolution compared to standard Facebook phishing campaigns, as they utilize legitimate cloud hosting services like Netlify and Vercel to bypass traditional security filters. The emergence of the BitB technique represents a major escalation, as it capitalizes on user familiarity with authentication flows, making credential theft nearly impossible to detect visually.
Block CISO: We red-teamed our own AI agent to run an infostealer on an employee laptop (https://go.theregister.com/feed/www.theregister.com/2026/01/12/block_ai_agent_goose/): Block, the parent company of Square, Cash App, and Afterpay, has been red-teaming its own AI agent, Goose, to run an infostealer on an employee laptop. This was done to test the security of the agent and its workflows. Block is pushing hard to position itself as an AI leader, co-designing the Model Context Protocol (MCP) with Anthropic and using MCP to build Goose, its open-source AI agent that's used by almost all Block's 12,000 employees and connects to all of the company's systems including Google accounts and Square payments. Block is applying least-privilege access to humans and machines. Block employees should only have access to data they need to do their jobs - same with the company's AI agents. Block uses penetration testing and other offensive security measures to identify how attackers could abuse its AI agent, and then find ways to fix the issue. Block is working on new ways to prevent prompt injection; some of these, like improved detection, have already been integrated into Goose.
Hidden Telegram proxy links can reveal your IP address in one click (https://www.bleepingcomputer.com/news/security/hidden-telegram-proxy-links-can-reveal-your-ip-address-in-one-click/): Researchers have demonstrated that Telegram clients on both Android and iOS automatically attempt to connect to a proxy when a user taps a specially crafted internal link. These links can be disguised as ordinary usernames, for example, appearing as @durov in a Telegram message, but actually lead to a Telegram proxy link. These links are special URLs used to quickly configure MTProto proxies in Telegram clients. They allow users to add a proxy by clicking a link instead of manually entering server details: https://t.me/proxy?server=[proxy IP address/hostname]&amp;port=[proxy_port]&amp;secret=[MTProto_secret]. When opened in Telegram, the app reads the proxy parameters (including the server, port, and secret), and prompts the user to add the proxy to their settings. These links are widely shared to help users bypass network blocks or internet censorship and to conceal their real location, particularly in restrictive environments, making the feature valuable to activists, journalists, and others seeking anonymity. Attackers can abuse this behavior by setting up their own MTProto proxies and distributing links that are visually disguised as harmless usernames or website URLs but actually point to proxy configuration endpoints. If a user clicks such a link on a mobile client, the Telegram app will attempt to connect to the
Identity &amp; Beyond: 2026 Incident Response Predictions (https://www.cybereason.com/blog/identity-beyond-2026-incident-response-predictions): In 2026, incident response (IR) will increasingly focus on identity-driven intrusions, abuse of trusted cloud services, and low-signal, high-impact activities that blend seamlessly into normal business operations. Phishing and social engineering will remain primary vectors, with attackers bypassing traditional defenses through phishing-resistant MFA bypass attempts, OAuth application abuse, and session hijacking. OAuth and API abuse will become standard persistence mechanisms, enabling attackers to establish long-lived access tokens and bypass traditional identity-based remediation. BEC will evolve beyond email, targeting collaboration tools and internal workflows for financial actions. &quot;Living-off-the-tenant&quot; attacks will increase, with attackers exploiting native cloud tooling and default configurations. To mitigate these trends, organizations should enforce phishing-resistant MFA, treat identity telemetry as tier-1 forensic evidence, centralize identity provider logs, and maintain comprehensive inventories of enterprise applications.</description>
      <itunes:summary>Articles discussed
How LLMs Handle Infinite Context With Finite Memory (https://towardsdatascience.com/llms-can-now-process-infinite-context-windows/): Researchers at Google developed Infini-attention, an innovative approach to handling long-context sequences in AI language models. This method addresses the memory limitations of traditional Transformer architectures by compressing the entire historical context into a fixed-size Memory Matrix. This enables efficient memory usage, reducing the required parameters by 114 times compared to previous models. Infini-attention achieves state-of-the-art perplexity scores on benchmarks like PG19 and Arxiv-math while using significantly less memory. The method involves segmenting input sequences, compressing relevant information into a Memory Matrix, and retrieving this data for generating subsequent tokens. This approach allows for efficient long-term memory retention without the need for extensive GPU VRAM resources.
Remote Code Execution With Modern AI/ML Formats and Libraries (https://unit42.paloaltonetworks.com/rce-vulnerabilities-in-ai-python-libraries/): Researchers have identified vulnerabilities in three open-source AI/ML Python libraries: NeMo, Uni2TS, and FlexTok. These vulnerabilities allow for remote code execution (RCE) when malicious metadata is embedded in model files. Affected libraries are used in popular models on HuggingFace, with tens of millions of downloads. Palo Alto Networks notified vendors in April 2025, and fixes were released by NVIDIA, Salesforce, and Apple by July 2025. The vulnerabilities were discovered by Prisma AIRS, which can identify models using these vulnerabilities and extract payloads. Hydra, a Python library used by these models, has been updated to warn users of RCE risks and add a block-list mechanism to mitigate the issue.
Facebook login thieves now using browser-in-browser trick (https://www.bleepingcomputer.com/news/security/facebook-login-thieves-now-using-browser-in-browser-trick/): Hackers have developed a new phishing technique called browser-in-browser (BitB) to trick users into providing Facebook account credentials. This technique was developed by security researcher mr.d0x in 2022 and later adopted by cybercriminals targeting various online services, including Facebook and Steam. BitB attacks involve presenting users with a fake browser pop-up containing a login form, implemented using an iframe that imitates the authentication interface of legitimate platforms. Cybercriminals have added shortened URLs and fake Meta CAPTCHA pages to increase the legitimacy of the phishing pages. These campaigns constitute a significant evolution compared to standard Facebook phishing campaigns, as they utilize legitimate cloud hosting services like Netlify and Vercel to bypass traditional security filters. The emergence of the BitB technique represents a major escalation, as it capitalizes on user familiarity with authentication flows, making credential theft nearly impossible to detect visually.
Block CISO: We red-teamed our own AI agent to run an infostealer on an employee laptop (https://go.theregister.com/feed/www.theregister.com/2026/01/12/block_ai_agent_goose/): Block, the parent company of Square, Cash App, and Afterpay, has been red-teaming its own AI agent, Goose, to run an infostealer on an employee laptop. This was done to test the security of the agent and its workflows. Block is pushing hard to position itself as an AI leader, co-designing the Model Context Protocol (MCP) with Anthropic and using MCP to build Goose, its open-source AI agent that's used by almost all Block's 12,000 employees and connects to all of the company's systems including Google accounts and Square payments. Block is applying least-privilege access to humans and machines. Block employees should only have access to data they need to do their jobs - same with the company's AI agents. Block uses penetration testing and other offensive security measures to identify how attackers could abuse its AI agent, and then find ways to fix the issue. Block is working on new ways to prevent prompt injection; some of these, like improved detection, have already been integrated into Goose.
Hidden Telegram proxy links can reveal your IP address in one click (https://www.bleepingcomputer.com/news/security/hidden-telegram-proxy-links-can-reveal-your-ip-address-in-one-click/): Researchers have demonstrated that Telegram clients on both Android and iOS automatically attempt to connect to a proxy when a user taps a specially crafted internal link. These links can be disguised as ordinary usernames, for example, appearing as @durov in a Telegram message, but actually lead to a Telegram proxy link. These links are special URLs used to quickly configure MTProto proxies in Telegram clients. They allow users to add a proxy by clicking a link instead of manually entering server details: https://t.me/proxy?server=[proxy IP address/hostname]&amp;port=[proxy_port]&amp;secret=[MTProto_secret]. When opened in Telegram, the app reads the proxy parameters (including the server, port, and secret), and prompts the user to add the proxy to their settings. These links are widely shared to help users bypass network blocks or internet censorship and to conceal their real location, particularly in restrictive environments, making the feature valuable to activists, journalists, and others seeking anonymity. Attackers can abuse this behavior by setting up their own MTProto proxies and distributing links that are visually disguised as harmless usernames or website URLs but actually point to proxy configuration endpoints. If a user clicks such a link on a mobile client, the Telegram app will attempt to connect to the
Identity &amp; Beyond: 2026 Incident Response Predictions (https://www.cybereason.com/blog/identity-beyond-2026-incident-response-predictions): In 2026, incident response (IR) will increasingly focus on identity-driven intrusions, abuse of trusted cloud services, and low-signal, high-impact activities that blend seamlessly into normal business operations. Phishing and social engineering will remain primary vectors, with attackers bypassing traditional defenses through phishing-resistant MFA bypass attempts, OAuth application abuse, and session hijacking. OAuth and API abuse will become standard persistence mechanisms, enabling attackers to establish long-lived access tokens and bypass traditional identity-based remediation. BEC will evolve beyond email, targeting collaboration tools and internal workflows for financial actions. &quot;Living-off-the-tenant&quot; attacks will increase, with attackers exploiting native cloud tooling and default configurations. To mitigate these trends, organizations should enforce phishing-resistant MFA, treat identity telemetry as tier-1 forensic evidence, centralize identity provider logs, and maintain comprehensive inventories of enterprise applications.</itunes:summary>
      <description>Articles discussed
How LLMs Handle Infinite Context With Finite Memory (https://towardsdatascience.com/llms-can-now-process-infinite-context-windows/): Researchers at Google developed Infini-attention, an innovative approach to handling long-context sequences in AI language models. This method addresses the memory limitations of traditional Transformer architectures by compressing the entire historical context into a fixed-size Memory Matrix. This enables efficient memory usage, reducing the required parameters by 114 times compared to previous models. Infini-attention achieves state-of-the-art perplexity scores on benchmarks like PG19 and Arxiv-math while using significantly less memory. The method involves segmenting input sequences, compressing relevant information into a Memory Matrix, and retrieving this data for generating subsequent tokens. This approach allows for efficient long-term memory retention without the need for extensive GPU VRAM resources.
Remote Code Execution With Modern AI/ML Formats and Libraries (https://unit42.paloaltonetworks.com/rce-vulnerabilities-in-ai-python-libraries/): Researchers have identified vulnerabilities in three open-source AI/ML Python libraries: NeMo, Uni2TS, and FlexTok. These vulnerabilities allow for remote code execution (RCE) when malicious metadata is embedded in model files. Affected libraries are used in popular models on HuggingFace, with tens of millions of downloads. Palo Alto Networks notified vendors in April 2025, and fixes were released by NVIDIA, Salesforce, and Apple by July 2025. The vulnerabilities were discovered by Prisma AIRS, which can identify models using these vulnerabilities and extract payloads. Hydra, a Python library used by these models, has been updated to warn users of RCE risks and add a block-list mechanism to mitigate the issue.
Facebook login thieves now using browser-in-browser trick (https://www.bleepingcomputer.com/news/security/facebook-login-thieves-now-using-browser-in-browser-trick/): Hackers have developed a new phishing technique called browser-in-browser (BitB) to trick users into providing Facebook account credentials. This technique was developed by security researcher mr.d0x in 2022 and later adopted by cybercriminals targeting various online services, including Facebook and Steam. BitB attacks involve presenting users with a fake browser pop-up containing a login form, implemented using an iframe that imitates the authentication interface of legitimate platforms. Cybercriminals have added shortened URLs and fake Meta CAPTCHA pages to increase the legitimacy of the phishing pages. These campaigns constitute a significant evolution compared to standard Facebook phishing campaigns, as they utilize legitimate cloud hosting services like Netlify and Vercel to bypass traditional security filters. The emergence of the BitB technique represents a major escalation, as it capitalizes on user familiarity with authentication flows, making credential theft nearly impossible to detect visually.
Block CISO: We red-teamed our own AI agent to run an infostealer on an employee laptop (https://go.theregister.com/feed/www.theregister.com/2026/01/12/block_ai_agent_goose/): Block, the parent company of Square, Cash App, and Afterpay, has been red-teaming its own AI agent, Goose, to run an infostealer on an employee laptop. This was done to test the security of the agent and its workflows. Block is pushing hard to position itself as an AI leader, co-designing the Model Context Protocol (MCP) with Anthropic and using MCP to build Goose, its open-source AI agent that's used by almost all Block's 12,000 employees and connects to all of the company's systems including Google accounts and Square payments. Block is applying least-privilege access to humans and machines. Block employees should only have access to data they need to do their jobs - same with the company's AI agents. Block uses penetration testing and other offensive security measures to identify how attackers could abuse its AI agent, and then find ways to fix the issue. Block is working on new ways to prevent prompt injection; some of these, like improved detection, have already been integrated into Goose.
Hidden Telegram proxy links can reveal your IP address in one click (https://www.bleepingcomputer.com/news/security/hidden-telegram-proxy-links-can-reveal-your-ip-address-in-one-click/): Researchers have demonstrated that Telegram clients on both Android and iOS automatically attempt to connect to a proxy when a user taps a specially crafted internal link. These links can be disguised as ordinary usernames, for example, appearing as @durov in a Telegram message, but actually lead to a Telegram proxy link. These links are special URLs used to quickly configure MTProto proxies in Telegram clients. They allow users to add a proxy by clicking a link instead of manually entering server details: https://t.me/proxy?server=[proxy IP address/hostname]&amp;port=[proxy_port]&amp;secret=[MTProto_secret]. When opened in Telegram, the app reads the proxy parameters (including the server, port, and secret), and prompts the user to add the proxy to their settings. These links are widely shared to help users bypass network blocks or internet censorship and to conceal their real location, particularly in restrictive environments, making the feature valuable to activists, journalists, and others seeking anonymity. Attackers can abuse this behavior by setting up their own MTProto proxies and distributing links that are visually disguised as harmless usernames or website URLs but actually point to proxy configuration endpoints. If a user clicks such a link on a mobile client, the Telegram app will attempt to connect to the
Identity &amp; Beyond: 2026 Incident Response Predictions (https://www.cybereason.com/blog/identity-beyond-2026-incident-response-predictions): In 2026, incident response (IR) will increasingly focus on identity-driven intrusions, abuse of trusted cloud services, and low-signal, high-impact activities that blend seamlessly into normal business operations. Phishing and social engineering will remain primary vectors, with attackers bypassing traditional defenses through phishing-resistant MFA bypass attempts, OAuth application abuse, and session hijacking. OAuth and API abuse will become standard persistence mechanisms, enabling attackers to establish long-lived access tokens and bypass traditional identity-based remediation. BEC will evolve beyond email, targeting collaboration tools and internal workflows for financial actions. &quot;Living-off-the-tenant&quot; attacks will increase, with attackers exploiting native cloud tooling and default configurations. To mitigate these trends, organizations should enforce phishing-resistant MFA, treat identity telemetry as tier-1 forensic evidence, centralize identity provider logs, and maintain comprehensive inventories of enterprise applications.</description>
      <itunes:summary>Articles discussed
How LLMs Handle Infinite Context With Finite Memory (https://towardsdatascience.com/llms-can-now-process-infinite-context-windows/): Researchers at Google developed Infini-attention, an innovative approach to handling long-context sequences in AI language models. This method addresses the memory limitations of traditional Transformer architectures by compressing the entire historical context into a fixed-size Memory Matrix. This enables efficient memory usage, reducing the required parameters by 114 times compared to previous models. Infini-attention achieves state-of-the-art perplexity scores on benchmarks like PG19 and Arxiv-math while using significantly less memory. The method involves segmenting input sequences, compressing relevant information into a Memory Matrix, and retrieving this data for generating subsequent tokens. This approach allows for efficient long-term memory retention without the need for extensive GPU VRAM resources.
Remote Code Execution With Modern AI/ML Formats and Libraries (https://unit42.paloaltonetworks.com/rce-vulnerabilities-in-ai-python-libraries/): Researchers have identified vulnerabilities in three open-source AI/ML Python libraries: NeMo, Uni2TS, and FlexTok. These vulnerabilities allow for remote code execution (RCE) when malicious metadata is embedded in model files. Affected libraries are used in popular models on HuggingFace, with tens of millions of downloads. Palo Alto Networks notified vendors in April 2025, and fixes were released by NVIDIA, Salesforce, and Apple by July 2025. The vulnerabilities were discovered by Prisma AIRS, which can identify models using these vulnerabilities and extract payloads. Hydra, a Python library used by these models, has been updated to warn users of RCE risks and add a block-list mechanism to mitigate the issue.
Facebook login thieves now using browser-in-browser trick (https://www.bleepingcomputer.com/news/security/facebook-login-thieves-now-using-browser-in-browser-trick/): Hackers have developed a new phishing technique called browser-in-browser (BitB) to trick users into providing Facebook account credentials. This technique was developed by security researcher mr.d0x in 2022 and later adopted by cybercriminals targeting various online services, including Facebook and Steam. BitB attacks involve presenting users with a fake browser pop-up containing a login form, implemented using an iframe that imitates the authentication interface of legitimate platforms. Cybercriminals have added shortened URLs and fake Meta CAPTCHA pages to increase the legitimacy of the phishing pages. These campaigns constitute a significant evolution compared to standard Facebook phishing campaigns, as they utilize legitimate cloud hosting services like Netlify and Vercel to bypass traditional security filters. The emergence of the BitB technique represents a major escalation, as it capitalizes on user familiarity with authentication flows, making credential theft nearly impossible to detect visually.
Block CISO: We red-teamed our own AI agent to run an infostealer on an employee laptop (https://go.theregister.com/feed/www.theregister.com/2026/01/12/block_ai_agent_goose/): Block, the parent company of Square, Cash App, and Afterpay, has been red-teaming its own AI agent, Goose, to run an infostealer on an employee laptop. This was done to test the security of the agent and its workflows. Block is pushing hard to position itself as an AI leader, co-designing the Model Context Protocol (MCP) with Anthropic and using MCP to build Goose, its open-source AI agent that's used by almost all Block's 12,000 employees and connects to all of the company's systems including Google accounts and Square payments. Block is applying least-privilege access to humans and machines. Block employees should only have access to data they need to do their jobs - same with the company's AI agents. Block uses penetration testing and other offensive security measures to identify how attackers could abuse its AI agent, and then find ways to fix the issue. Block is working on new ways to prevent prompt injection; some of these, like improved detection, have already been integrated into Goose.
Hidden Telegram proxy links can reveal your IP address in one click (https://www.bleepingcomputer.com/news/security/hidden-telegram-proxy-links-can-reveal-your-ip-address-in-one-click/): Researchers have demonstrated that Telegram clients on both Android and iOS automatically attempt to connect to a proxy when a user taps a specially crafted internal link. These links can be disguised as ordinary usernames, for example, appearing as @durov in a Telegram message, but actually lead to a Telegram proxy link. These links are special URLs used to quickly configure MTProto proxies in Telegram clients. They allow users to add a proxy by clicking a link instead of manually entering server details: https://t.me/proxy?server=[proxy IP address/hostname]&amp;port=[proxy_port]&amp;secret=[MTProto_secret]. When opened in Telegram, the app reads the proxy parameters (including the server, port, and secret), and prompts the user to add the proxy to their settings. These links are widely shared to help users bypass network blocks or internet censorship and to conceal their real location, particularly in restrictive environments, making the feature valuable to activists, journalists, and others seeking anonymity. Attackers can abuse this behavior by setting up their own MTProto proxies and distributing links that are visually disguised as harmless usernames or website URLs but actually point to proxy configuration endpoints. If a user clicks such a link on a mobile client, the Telegram app will attempt to connect to the
Identity &amp; Beyond: 2026 Incident Response Predictions (https://www.cybereason.com/blog/identity-beyond-2026-incident-response-predictions): In 2026, incident response (IR) will increasingly focus on identity-driven intrusions, abuse of trusted cloud services, and low-signal, high-impact activities that blend seamlessly into normal business operations. Phishing and social engineering will remain primary vectors, with attackers bypassing traditional defenses through phishing-resistant MFA bypass attempts, OAuth application abuse, and session hijacking. OAuth and API abuse will become standard persistence mechanisms, enabling attackers to establish long-lived access tokens and bypass traditional identity-based remediation. BEC will evolve beyond email, targeting collaboration tools and internal workflows for financial actions. &quot;Living-off-the-tenant&quot; attacks will increase, with attackers exploiting native cloud tooling and default configurations. To mitigate these trends, organizations should enforce phishing-resistant MFA, treat identity telemetry as tier-1 forensic evidence, centralize identity provider logs, and maintain comprehensive inventories of enterprise applications.</itunes:summary>
      <pubDate>Sat, 17 Jan 2026 15:58:59 GMT</pubDate>
      <enclosure url="https://raw.githubusercontent.com/Refugeek/podcast-feed/main/episodes-EScyber/20260113182254-how-llms-handle-infinite-context-with-finite-memory-remote-code-execution-with-modern-ai-ml-formats-and-libraries.mp3" type="audio/mpeg" length="43673324"/>
      <itunes:duration>2183</itunes:duration>
    </item>
    <item>
      <title>20260115063540-glitches-in-the-attention-matrix-inside-redvds-how-a-single-virtual-desktop-provider-fueled-worldwide-cybercriminal-operations</title>
      <description>Articles discussed
Glitches in the Attention Matrix (https://towardsdatascience.com/glitches-in-the-attention-matrix-a-history-of-transformer-artifacts-and-the-latest-research-on-how-to-fix-them/): ### Glitches in the Attention Matrix ViT models, pivotal in foundation models for computer vision, suffer from high-norm artifacts, which can substantially impact performance across various tasks. These artifacts, characterized by high L2 norms, sparsity, patch localization, and layer localization, arise from the Softmax function in transformer architectures. Two hypotheses explain these artifacts: global processing and the mechanistic hypothesis related to the Softmax function. Research has developed methods to mitigate these artifacts, including the Register Solution, which introduces learnable tokens not used in training or predictions, and the Denoising Vision Transformers (DVT) approach, which post-processes outputs to clean high-norm artifacts. These methods enhance performance on tasks like unsupervised object discovery and zero-shot segmentation, with DVT showing synergistic benefits with registers. The Register Solution requires retraining, while DVT introduces latency, highlighting the need for efficient solutions. The NeurIPS 2025 paper proposes a general solution to these artifacts by modifying the self-attention transformer architecture, benefiting multiple tasks and being integrated into the Qwen model, Qwen3-Next.
Inside RedVDS: How a single virtual desktop provider fueled worldwide cybercriminal operations (https://www.microsoft.com/en-us/security/blog/2026/01/14/inside-redvds-how-a-single-virtual-desktop-provider-fueled-worldwide-cybercriminal-operations/): RedVDS is a virtual dedicated server provider that has been used by multiple cybercriminals to commit various financial crimes. Microsoft Threat Intelligence observed the proliferation of RedVDS, a virtual dedicated server (VDS) provider used by multiple financially motivated threat actors to commit business email compromise (BEC), mass phishing, account takeover, and financial fraud. Microsoft’s investigation into RedVDS services and infrastructure uncovered a global network of disparate cybercriminals purchasing and using to target multiple sectors, including legal, construction, manufacturing, real estate, healthcare, and education in the United States, Canada, United Kingdom, France, Germany, Australia, and countries with substantial banking infrastructure targets that have a higher potential for financial gain. In collaboration with law enforcement agencies worldwide, Microsoft’s Digital Crimes Unit (DCU) recently facilitated a disruption of RedVDS infrastructure and related operations. RedVDS is a criminal marketplace selling illegal software and services that facilitated and enabled cybercrime. The marketplace offers a simple and feature-rich user interface for purchasing unlicensed and inexpensive Windows-based Remote Desktop Protocol (RDP) servers with full administrator control and no usage limits – a combination eagerly exploited by cybercriminals. Microsoft’s investigation into RedVDS revealed a single, cloned Windows host image being reused across the service, leaving unique technical fingerprints
Reprompt attack let hackers hijack Microsoft Copilot sessions (https://www.bleepingcomputer.com/news/security/reprompt-attack-let-hackers-hijack-microsoft-copilot-sessions/): Researchers at Varonis discovered a new attack method called &quot;Reprompt,&quot; which allows hackers to hijack Microsoft Copilot sessions and exfiltrate sensitive data. The attack works by embedding malicious prompts within legitimate URLs, bypassing Copilot's security measures. Researchers demonstrated how Reprompt can maintain access to a victim's session after a single click, using techniques like Parameter-to-Prompt injection, double-request exploitation, and chain-request techniques. The attack bypasses Copilot's safeguards by instructing it to repeat actions twice, allowing data exfiltration. Varonis responsibly disclosed the vulnerability to Microsoft, which addressed it in a recent security update. While exploitation has not been detected in the wild, users are advised to apply the latest Windows security updates to protect against this threat.
Popular Python libraries used in Hugging Face models subject to poisoned metadata attack (https://go.theregister.com/feed/www.theregister.com/2026/01/13/ai_python_library_bugs_allow/): Three popular AI and ML Python libraries—NeMo, Uni2TS, and FlexTok—were found to have vulnerabilities that allow remote attackers to hide malicious code in metadata. These vulnerabilities were discovered by Palo Alto Networks' Unit 42 and reported to the libraries' maintainers, who have since issued security warnings, fixes, and CVEs. The vulnerabilities involve Hydra's instantiate() function, which allows for remote code execution (RCE). Meta has since updated Hydra's documentation with a warning that states RCE is possible when using instantiate() and urges users to add a block-list mechanism that compares the _target_ value against a list of dangerous functions before it is called. As of now, however, the block-list mechanism hasn't been made available in a Hydra release.</description>
      <itunes:summary>Articles discussed
Glitches in the Attention Matrix (https://towardsdatascience.com/glitches-in-the-attention-matrix-a-history-of-transformer-artifacts-and-the-latest-research-on-how-to-fix-them/): ### Glitches in the Attention Matrix ViT models, pivotal in foundation models for computer vision, suffer from high-norm artifacts, which can substantially impact performance across various tasks. These artifacts, characterized by high L2 norms, sparsity, patch localization, and layer localization, arise from the Softmax function in transformer architectures. Two hypotheses explain these artifacts: global processing and the mechanistic hypothesis related to the Softmax function. Research has developed methods to mitigate these artifacts, including the Register Solution, which introduces learnable tokens not used in training or predictions, and the Denoising Vision Transformers (DVT) approach, which post-processes outputs to clean high-norm artifacts. These methods enhance performance on tasks like unsupervised object discovery and zero-shot segmentation, with DVT showing synergistic benefits with registers. The Register Solution requires retraining, while DVT introduces latency, highlighting the need for efficient solutions. The NeurIPS 2025 paper proposes a general solution to these artifacts by modifying the self-attention transformer architecture, benefiting multiple tasks and being integrated into the Qwen model, Qwen3-Next.
Inside RedVDS: How a single virtual desktop provider fueled worldwide cybercriminal operations (https://www.microsoft.com/en-us/security/blog/2026/01/14/inside-redvds-how-a-single-virtual-desktop-provider-fueled-worldwide-cybercriminal-operations/): RedVDS is a virtual dedicated server provider that has been used by multiple cybercriminals to commit various financial crimes. Microsoft Threat Intelligence observed the proliferation of RedVDS, a virtual dedicated server (VDS) provider used by multiple financially motivated threat actors to commit business email compromise (BEC), mass phishing, account takeover, and financial fraud. Microsoft’s investigation into RedVDS services and infrastructure uncovered a global network of disparate cybercriminals purchasing and using to target multiple sectors, including legal, construction, manufacturing, real estate, healthcare, and education in the United States, Canada, United Kingdom, France, Germany, Australia, and countries with substantial banking infrastructure targets that have a higher potential for financial gain. In collaboration with law enforcement agencies worldwide, Microsoft’s Digital Crimes Unit (DCU) recently facilitated a disruption of RedVDS infrastructure and related operations. RedVDS is a criminal marketplace selling illegal software and services that facilitated and enabled cybercrime. The marketplace offers a simple and feature-rich user interface for purchasing unlicensed and inexpensive Windows-based Remote Desktop Protocol (RDP) servers with full administrator control and no usage limits – a combination eagerly exploited by cybercriminals. Microsoft’s investigation into RedVDS revealed a single, cloned Windows host image being reused across the service, leaving unique technical fingerprints
Reprompt attack let hackers hijack Microsoft Copilot sessions (https://www.bleepingcomputer.com/news/security/reprompt-attack-let-hackers-hijack-microsoft-copilot-sessions/): Researchers at Varonis discovered a new attack method called &quot;Reprompt,&quot; which allows hackers to hijack Microsoft Copilot sessions and exfiltrate sensitive data. The attack works by embedding malicious prompts within legitimate URLs, bypassing Copilot's security measures. Researchers demonstrated how Reprompt can maintain access to a victim's session after a single click, using techniques like Parameter-to-Prompt injection, double-request exploitation, and chain-request techniques. The attack bypasses Copilot's safeguards by instructing it to repeat actions twice, allowing data exfiltration. Varonis responsibly disclosed the vulnerability to Microsoft, which addressed it in a recent security update. While exploitation has not been detected in the wild, users are advised to apply the latest Windows security updates to protect against this threat.
Popular Python libraries used in Hugging Face models subject to poisoned metadata attack (https://go.theregister.com/feed/www.theregister.com/2026/01/13/ai_python_library_bugs_allow/): Three popular AI and ML Python libraries—NeMo, Uni2TS, and FlexTok—were found to have vulnerabilities that allow remote attackers to hide malicious code in metadata. These vulnerabilities were discovered by Palo Alto Networks' Unit 42 and reported to the libraries' maintainers, who have since issued security warnings, fixes, and CVEs. The vulnerabilities involve Hydra's instantiate() function, which allows for remote code execution (RCE). Meta has since updated Hydra's documentation with a warning that states RCE is possible when using instantiate() and urges users to add a block-list mechanism that compares the _target_ value against a list of dangerous functions before it is called. As of now, however, the block-list mechanism hasn't been made available in a Hydra release.</itunes:summary>
      <description>Articles discussed
Glitches in the Attention Matrix (https://towardsdatascience.com/glitches-in-the-attention-matrix-a-history-of-transformer-artifacts-and-the-latest-research-on-how-to-fix-them/): ### Glitches in the Attention Matrix ViT models, pivotal in foundation models for computer vision, suffer from high-norm artifacts, which can substantially impact performance across various tasks. These artifacts, characterized by high L2 norms, sparsity, patch localization, and layer localization, arise from the Softmax function in transformer architectures. Two hypotheses explain these artifacts: global processing and the mechanistic hypothesis related to the Softmax function. Research has developed methods to mitigate these artifacts, including the Register Solution, which introduces learnable tokens not used in training or predictions, and the Denoising Vision Transformers (DVT) approach, which post-processes outputs to clean high-norm artifacts. These methods enhance performance on tasks like unsupervised object discovery and zero-shot segmentation, with DVT showing synergistic benefits with registers. The Register Solution requires retraining, while DVT introduces latency, highlighting the need for efficient solutions. The NeurIPS 2025 paper proposes a general solution to these artifacts by modifying the self-attention transformer architecture, benefiting multiple tasks and being integrated into the Qwen model, Qwen3-Next.
Inside RedVDS: How a single virtual desktop provider fueled worldwide cybercriminal operations (https://www.microsoft.com/en-us/security/blog/2026/01/14/inside-redvds-how-a-single-virtual-desktop-provider-fueled-worldwide-cybercriminal-operations/): RedVDS is a virtual dedicated server provider that has been used by multiple cybercriminals to commit various financial crimes. Microsoft Threat Intelligence observed the proliferation of RedVDS, a virtual dedicated server (VDS) provider used by multiple financially motivated threat actors to commit business email compromise (BEC), mass phishing, account takeover, and financial fraud. Microsoft’s investigation into RedVDS services and infrastructure uncovered a global network of disparate cybercriminals purchasing and using to target multiple sectors, including legal, construction, manufacturing, real estate, healthcare, and education in the United States, Canada, United Kingdom, France, Germany, Australia, and countries with substantial banking infrastructure targets that have a higher potential for financial gain. In collaboration with law enforcement agencies worldwide, Microsoft’s Digital Crimes Unit (DCU) recently facilitated a disruption of RedVDS infrastructure and related operations. RedVDS is a criminal marketplace selling illegal software and services that facilitated and enabled cybercrime. The marketplace offers a simple and feature-rich user interface for purchasing unlicensed and inexpensive Windows-based Remote Desktop Protocol (RDP) servers with full administrator control and no usage limits – a combination eagerly exploited by cybercriminals. Microsoft’s investigation into RedVDS revealed a single, cloned Windows host image being reused across the service, leaving unique technical fingerprints
Reprompt attack let hackers hijack Microsoft Copilot sessions (https://www.bleepingcomputer.com/news/security/reprompt-attack-let-hackers-hijack-microsoft-copilot-sessions/): Researchers at Varonis discovered a new attack method called &quot;Reprompt,&quot; which allows hackers to hijack Microsoft Copilot sessions and exfiltrate sensitive data. The attack works by embedding malicious prompts within legitimate URLs, bypassing Copilot's security measures. Researchers demonstrated how Reprompt can maintain access to a victim's session after a single click, using techniques like Parameter-to-Prompt injection, double-request exploitation, and chain-request techniques. The attack bypasses Copilot's safeguards by instructing it to repeat actions twice, allowing data exfiltration. Varonis responsibly disclosed the vulnerability to Microsoft, which addressed it in a recent security update. While exploitation has not been detected in the wild, users are advised to apply the latest Windows security updates to protect against this threat.
Popular Python libraries used in Hugging Face models subject to poisoned metadata attack (https://go.theregister.com/feed/www.theregister.com/2026/01/13/ai_python_library_bugs_allow/): Three popular AI and ML Python libraries—NeMo, Uni2TS, and FlexTok—were found to have vulnerabilities that allow remote attackers to hide malicious code in metadata. These vulnerabilities were discovered by Palo Alto Networks' Unit 42 and reported to the libraries' maintainers, who have since issued security warnings, fixes, and CVEs. The vulnerabilities involve Hydra's instantiate() function, which allows for remote code execution (RCE). Meta has since updated Hydra's documentation with a warning that states RCE is possible when using instantiate() and urges users to add a block-list mechanism that compares the _target_ value against a list of dangerous functions before it is called. As of now, however, the block-list mechanism hasn't been made available in a Hydra release.</description>
      <itunes:summary>Articles discussed
Glitches in the Attention Matrix (https://towardsdatascience.com/glitches-in-the-attention-matrix-a-history-of-transformer-artifacts-and-the-latest-research-on-how-to-fix-them/): ### Glitches in the Attention Matrix ViT models, pivotal in foundation models for computer vision, suffer from high-norm artifacts, which can substantially impact performance across various tasks. These artifacts, characterized by high L2 norms, sparsity, patch localization, and layer localization, arise from the Softmax function in transformer architectures. Two hypotheses explain these artifacts: global processing and the mechanistic hypothesis related to the Softmax function. Research has developed methods to mitigate these artifacts, including the Register Solution, which introduces learnable tokens not used in training or predictions, and the Denoising Vision Transformers (DVT) approach, which post-processes outputs to clean high-norm artifacts. These methods enhance performance on tasks like unsupervised object discovery and zero-shot segmentation, with DVT showing synergistic benefits with registers. The Register Solution requires retraining, while DVT introduces latency, highlighting the need for efficient solutions. The NeurIPS 2025 paper proposes a general solution to these artifacts by modifying the self-attention transformer architecture, benefiting multiple tasks and being integrated into the Qwen model, Qwen3-Next.
Inside RedVDS: How a single virtual desktop provider fueled worldwide cybercriminal operations (https://www.microsoft.com/en-us/security/blog/2026/01/14/inside-redvds-how-a-single-virtual-desktop-provider-fueled-worldwide-cybercriminal-operations/): RedVDS is a virtual dedicated server provider that has been used by multiple cybercriminals to commit various financial crimes. Microsoft Threat Intelligence observed the proliferation of RedVDS, a virtual dedicated server (VDS) provider used by multiple financially motivated threat actors to commit business email compromise (BEC), mass phishing, account takeover, and financial fraud. Microsoft’s investigation into RedVDS services and infrastructure uncovered a global network of disparate cybercriminals purchasing and using to target multiple sectors, including legal, construction, manufacturing, real estate, healthcare, and education in the United States, Canada, United Kingdom, France, Germany, Australia, and countries with substantial banking infrastructure targets that have a higher potential for financial gain. In collaboration with law enforcement agencies worldwide, Microsoft’s Digital Crimes Unit (DCU) recently facilitated a disruption of RedVDS infrastructure and related operations. RedVDS is a criminal marketplace selling illegal software and services that facilitated and enabled cybercrime. The marketplace offers a simple and feature-rich user interface for purchasing unlicensed and inexpensive Windows-based Remote Desktop Protocol (RDP) servers with full administrator control and no usage limits – a combination eagerly exploited by cybercriminals. Microsoft’s investigation into RedVDS revealed a single, cloned Windows host image being reused across the service, leaving unique technical fingerprints
Reprompt attack let hackers hijack Microsoft Copilot sessions (https://www.bleepingcomputer.com/news/security/reprompt-attack-let-hackers-hijack-microsoft-copilot-sessions/): Researchers at Varonis discovered a new attack method called &quot;Reprompt,&quot; which allows hackers to hijack Microsoft Copilot sessions and exfiltrate sensitive data. The attack works by embedding malicious prompts within legitimate URLs, bypassing Copilot's security measures. Researchers demonstrated how Reprompt can maintain access to a victim's session after a single click, using techniques like Parameter-to-Prompt injection, double-request exploitation, and chain-request techniques. The attack bypasses Copilot's safeguards by instructing it to repeat actions twice, allowing data exfiltration. Varonis responsibly disclosed the vulnerability to Microsoft, which addressed it in a recent security update. While exploitation has not been detected in the wild, users are advised to apply the latest Windows security updates to protect against this threat.
Popular Python libraries used in Hugging Face models subject to poisoned metadata attack (https://go.theregister.com/feed/www.theregister.com/2026/01/13/ai_python_library_bugs_allow/): Three popular AI and ML Python libraries—NeMo, Uni2TS, and FlexTok—were found to have vulnerabilities that allow remote attackers to hide malicious code in metadata. These vulnerabilities were discovered by Palo Alto Networks' Unit 42 and reported to the libraries' maintainers, who have since issued security warnings, fixes, and CVEs. The vulnerabilities involve Hydra's instantiate() function, which allows for remote code execution (RCE). Meta has since updated Hydra's documentation with a warning that states RCE is possible when using instantiate() and urges users to add a block-list mechanism that compares the _target_ value against a list of dangerous functions before it is called. As of now, however, the block-list mechanism hasn't been made available in a Hydra release.</itunes:summary>
      <pubDate>Sat, 17 Jan 2026 15:58:59 GMT</pubDate>
      <enclosure url="https://raw.githubusercontent.com/Refugeek/podcast-feed/main/episodes-EScyber/20260115063540-glitches-in-the-attention-matrix-inside-redvds-how-a-single-virtual-desktop-provider-fueled-worldwide-cybercriminal-operations.mp3" type="audio/mpeg" length="44441804"/>
      <itunes:duration>2222</itunes:duration>
    </item>
    <item>
      <title>20260116100517-a-0-click-exploit-chain-for-the-pixel-9-part-2-cracking-the-sandbox-with-a-big-wave-a-0-click-exploit-chain-for-the-pixel-9-part-1-decoding-dolby</title>
      <description>Articles discussed
A 0-click exploit chain for the Pixel 9 Part 2: Cracking the Sandbox with a Big Wave (https://projectzero.google/2026/01/pixel-0-click-part-2.html): A 0-click exploit chain for the Pixel 9 Part 2: Cracking the Sandbox with a Big Wave involves a Linux kernel driver, /dev/bigwave, which is used for accelerating AV1 decoding tasks on the Pixel SOC. The driver's design allows for a powerful local privilege escalation bug, enabling arbitrary read/write on the Pixel 9. This exploit chain exploits a bug in the BigWave driver's handling of ioctl BIGO_IOCX_PROCESS, which can lead to a race condition where a job is dequeued prematurely, allowing an attacker to control the destination of writes to kernel memory. This control enables arbitrary writes to kernel .data, bypassing KASLR and allowing for complete kernel privilege escalation. The exploit chain is notable for its simplicity and effectiveness, requiring no user interaction or additional exploits to achieve full kernel control.
A 0-click exploit chain for the Pixel 9 Part 1: Decoding Dolby (https://projectzero.google/2026/01/pixel-0-click-part-1.html): Researchers at Project Zero have identified vulnerabilities in the Dolby Unified Decoder (UDC) on Android devices, particularly affecting Pixel 9 models. These vulnerabilities allow for 0-click exploits, enabling attackers to execute arbitrary code without user interaction. The researchers reported two critical CVEs: CVE-2025-54957 and CVE-2025-36934, which allow for code execution in the mediacodec context. These vulnerabilities exploit a flaw in memory allocation within the evo heap used for processing EMDF payloads, leading to potential integer overflow issues. The findings highlight the need for improved security measures in media decoding components on mobile devices. The vulnerabilities have been fixed as of January 5, 2026, and the research aims to inform defenders about these attack vectors and the importance of addressing such vulnerabilities.
Flipping one bit leaves AMD CPUs open to VM vuln (https://go.theregister.com/feed/www.theregister.com/2026/01/15/stackwarp_bug_amd_cpus/): Researchers affiliated with the CISPA Helmholtz Center for Information Security in Germany have discovered a vulnerability in AMD CPUs that could allow malicious insiders to access sensitive data in AMD SEV-SNP virtual machines. The flaw, dubbed StackWarp, exploits a microarchitecture designed to accelerate stack operations and can be exploited via a previously undocumented control bit on the hypervisor side. The attack scenario applies to AMD SEV-SNP, a successor to AMD Secure Encrypted Virtualization (SEV) and SEV-ES (Encrypted State), when Simultaneous Multithreading (SMT) has been enabled. Researchers Ruiyi Zhang, Tristan Hornetz, Daniel Weber, Fabian Thomas, and Michael Schwarz report that they were able to abuse the stack engine on AMD Zen CPUs in a way that breaks CVMs. They describe their findings in a paper titled, &quot;StackWarp: Breaking AMD SEV-SNP Integrity via Deterministic Stack-Pointer Manipulation through the CPU's Stack Engine.&quot; The paper is scheduled for publication at USENIX Security 2026.</description>
      <itunes:summary>Articles discussed
A 0-click exploit chain for the Pixel 9 Part 2: Cracking the Sandbox with a Big Wave (https://projectzero.google/2026/01/pixel-0-click-part-2.html): A 0-click exploit chain for the Pixel 9 Part 2: Cracking the Sandbox with a Big Wave involves a Linux kernel driver, /dev/bigwave, which is used for accelerating AV1 decoding tasks on the Pixel SOC. The driver's design allows for a powerful local privilege escalation bug, enabling arbitrary read/write on the Pixel 9. This exploit chain exploits a bug in the BigWave driver's handling of ioctl BIGO_IOCX_PROCESS, which can lead to a race condition where a job is dequeued prematurely, allowing an attacker to control the destination of writes to kernel memory. This control enables arbitrary writes to kernel .data, bypassing KASLR and allowing for complete kernel privilege escalation. The exploit chain is notable for its simplicity and effectiveness, requiring no user interaction or additional exploits to achieve full kernel control.
A 0-click exploit chain for the Pixel 9 Part 1: Decoding Dolby (https://projectzero.google/2026/01/pixel-0-click-part-1.html): Researchers at Project Zero have identified vulnerabilities in the Dolby Unified Decoder (UDC) on Android devices, particularly affecting Pixel 9 models. These vulnerabilities allow for 0-click exploits, enabling attackers to execute arbitrary code without user interaction. The researchers reported two critical CVEs: CVE-2025-54957 and CVE-2025-36934, which allow for code execution in the mediacodec context. These vulnerabilities exploit a flaw in memory allocation within the evo heap used for processing EMDF payloads, leading to potential integer overflow issues. The findings highlight the need for improved security measures in media decoding components on mobile devices. The vulnerabilities have been fixed as of January 5, 2026, and the research aims to inform defenders about these attack vectors and the importance of addressing such vulnerabilities.
Flipping one bit leaves AMD CPUs open to VM vuln (https://go.theregister.com/feed/www.theregister.com/2026/01/15/stackwarp_bug_amd_cpus/): Researchers affiliated with the CISPA Helmholtz Center for Information Security in Germany have discovered a vulnerability in AMD CPUs that could allow malicious insiders to access sensitive data in AMD SEV-SNP virtual machines. The flaw, dubbed StackWarp, exploits a microarchitecture designed to accelerate stack operations and can be exploited via a previously undocumented control bit on the hypervisor side. The attack scenario applies to AMD SEV-SNP, a successor to AMD Secure Encrypted Virtualization (SEV) and SEV-ES (Encrypted State), when Simultaneous Multithreading (SMT) has been enabled. Researchers Ruiyi Zhang, Tristan Hornetz, Daniel Weber, Fabian Thomas, and Michael Schwarz report that they were able to abuse the stack engine on AMD Zen CPUs in a way that breaks CVMs. They describe their findings in a paper titled, &quot;StackWarp: Breaking AMD SEV-SNP Integrity via Deterministic Stack-Pointer Manipulation through the CPU's Stack Engine.&quot; The paper is scheduled for publication at USENIX Security 2026.</itunes:summary>
      <description>Articles discussed
A 0-click exploit chain for the Pixel 9 Part 2: Cracking the Sandbox with a Big Wave (https://projectzero.google/2026/01/pixel-0-click-part-2.html): A 0-click exploit chain for the Pixel 9 Part 2: Cracking the Sandbox with a Big Wave involves a Linux kernel driver, /dev/bigwave, which is used for accelerating AV1 decoding tasks on the Pixel SOC. The driver's design allows for a powerful local privilege escalation bug, enabling arbitrary read/write on the Pixel 9. This exploit chain exploits a bug in the BigWave driver's handling of ioctl BIGO_IOCX_PROCESS, which can lead to a race condition where a job is dequeued prematurely, allowing an attacker to control the destination of writes to kernel memory. This control enables arbitrary writes to kernel .data, bypassing KASLR and allowing for complete kernel privilege escalation. The exploit chain is notable for its simplicity and effectiveness, requiring no user interaction or additional exploits to achieve full kernel control.
A 0-click exploit chain for the Pixel 9 Part 1: Decoding Dolby (https://projectzero.google/2026/01/pixel-0-click-part-1.html): Researchers at Project Zero have identified vulnerabilities in the Dolby Unified Decoder (UDC) on Android devices, particularly affecting Pixel 9 models. These vulnerabilities allow for 0-click exploits, enabling attackers to execute arbitrary code without user interaction. The researchers reported two critical CVEs: CVE-2025-54957 and CVE-2025-36934, which allow for code execution in the mediacodec context. These vulnerabilities exploit a flaw in memory allocation within the evo heap used for processing EMDF payloads, leading to potential integer overflow issues. The findings highlight the need for improved security measures in media decoding components on mobile devices. The vulnerabilities have been fixed as of January 5, 2026, and the research aims to inform defenders about these attack vectors and the importance of addressing such vulnerabilities.
Flipping one bit leaves AMD CPUs open to VM vuln (https://go.theregister.com/feed/www.theregister.com/2026/01/15/stackwarp_bug_amd_cpus/): Researchers affiliated with the CISPA Helmholtz Center for Information Security in Germany have discovered a vulnerability in AMD CPUs that could allow malicious insiders to access sensitive data in AMD SEV-SNP virtual machines. The flaw, dubbed StackWarp, exploits a microarchitecture designed to accelerate stack operations and can be exploited via a previously undocumented control bit on the hypervisor side. The attack scenario applies to AMD SEV-SNP, a successor to AMD Secure Encrypted Virtualization (SEV) and SEV-ES (Encrypted State), when Simultaneous Multithreading (SMT) has been enabled. Researchers Ruiyi Zhang, Tristan Hornetz, Daniel Weber, Fabian Thomas, and Michael Schwarz report that they were able to abuse the stack engine on AMD Zen CPUs in a way that breaks CVMs. They describe their findings in a paper titled, &quot;StackWarp: Breaking AMD SEV-SNP Integrity via Deterministic Stack-Pointer Manipulation through the CPU's Stack Engine.&quot; The paper is scheduled for publication at USENIX Security 2026.</description>
      <itunes:summary>Articles discussed
A 0-click exploit chain for the Pixel 9 Part 2: Cracking the Sandbox with a Big Wave (https://projectzero.google/2026/01/pixel-0-click-part-2.html): A 0-click exploit chain for the Pixel 9 Part 2: Cracking the Sandbox with a Big Wave involves a Linux kernel driver, /dev/bigwave, which is used for accelerating AV1 decoding tasks on the Pixel SOC. The driver's design allows for a powerful local privilege escalation bug, enabling arbitrary read/write on the Pixel 9. This exploit chain exploits a bug in the BigWave driver's handling of ioctl BIGO_IOCX_PROCESS, which can lead to a race condition where a job is dequeued prematurely, allowing an attacker to control the destination of writes to kernel memory. This control enables arbitrary writes to kernel .data, bypassing KASLR and allowing for complete kernel privilege escalation. The exploit chain is notable for its simplicity and effectiveness, requiring no user interaction or additional exploits to achieve full kernel control.
A 0-click exploit chain for the Pixel 9 Part 1: Decoding Dolby (https://projectzero.google/2026/01/pixel-0-click-part-1.html): Researchers at Project Zero have identified vulnerabilities in the Dolby Unified Decoder (UDC) on Android devices, particularly affecting Pixel 9 models. These vulnerabilities allow for 0-click exploits, enabling attackers to execute arbitrary code without user interaction. The researchers reported two critical CVEs: CVE-2025-54957 and CVE-2025-36934, which allow for code execution in the mediacodec context. These vulnerabilities exploit a flaw in memory allocation within the evo heap used for processing EMDF payloads, leading to potential integer overflow issues. The findings highlight the need for improved security measures in media decoding components on mobile devices. The vulnerabilities have been fixed as of January 5, 2026, and the research aims to inform defenders about these attack vectors and the importance of addressing such vulnerabilities.
Flipping one bit leaves AMD CPUs open to VM vuln (https://go.theregister.com/feed/www.theregister.com/2026/01/15/stackwarp_bug_amd_cpus/): Researchers affiliated with the CISPA Helmholtz Center for Information Security in Germany have discovered a vulnerability in AMD CPUs that could allow malicious insiders to access sensitive data in AMD SEV-SNP virtual machines. The flaw, dubbed StackWarp, exploits a microarchitecture designed to accelerate stack operations and can be exploited via a previously undocumented control bit on the hypervisor side. The attack scenario applies to AMD SEV-SNP, a successor to AMD Secure Encrypted Virtualization (SEV) and SEV-ES (Encrypted State), when Simultaneous Multithreading (SMT) has been enabled. Researchers Ruiyi Zhang, Tristan Hornetz, Daniel Weber, Fabian Thomas, and Michael Schwarz report that they were able to abuse the stack engine on AMD Zen CPUs in a way that breaks CVMs. They describe their findings in a paper titled, &quot;StackWarp: Breaking AMD SEV-SNP Integrity via Deterministic Stack-Pointer Manipulation through the CPU's Stack Engine.&quot; The paper is scheduled for publication at USENIX Security 2026.</itunes:summary>
      <pubDate>Sat, 17 Jan 2026 15:58:59 GMT</pubDate>
      <enclosure url="https://raw.githubusercontent.com/Refugeek/podcast-feed/main/episodes-EScyber/20260116100517-a-0-click-exploit-chain-for-the-pixel-9-part-2-cracking-the-sandbox-with-a-big-wave-a-0-click-exploit-chain-for-the-pixel-9-part-1-decoding-dolby.mp3" type="audio/mpeg" length="31676204"/>
      <itunes:duration>1583</itunes:duration>
    </item>
  </channel>
</rss>
