<?xml version="1.0" ?>
<rss xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" version="2.0">
  <channel>
    <title>EScyber</title>
    <description>An AI generated podcast about cybersecurity and AI in Spanish.</description>
    <link>https://raw.githubusercontent.com/refugeek/podcast-feed/main/episodes-EScyber/feed.xml</link>
    <language>es-es</language>
    <itunes:author>Rfgk</itunes:author>
    <itunes:summary>An AI generated podcast about cybersecurity and AI in Spanish.</itunes:summary>
    <itunes:image href="https://raw.githubusercontent.com/refugeek/podcast-feed/main/episodes-EScyber/cover-EScyber.png"/>
    <item>
      <title>20251029091515-what-brain-privacy-will-look-like-in-the-age-of-neurotech-zero-trust-has-a-blind-spot-your-ai-agents</title>
      <description>Articles discussed
What brain privacy will look like in the age of neurotech: Nita Farahany spoke with Recorded Future News about whether brain data will be commodified and the role artificial intelligence plays in allowing internal speech to be decoded.
Zero Trust Has a Blind Spot—Your AI Agents: AI agents now act, decide, and access systems on their own — creating new blind spots Zero Trust can't see. Token Security helps organizations govern AI identities so every agent's access, intent, and action are verified and accountable. [...]
New AI training method creates powerful software agents with just 78 examples: A new study by Shanghai Jiao Tong University and SII Generative AI Research Lab (GAIR) shows that training large language models (LLMs) for complex, autonomous tasks does not require massive datasets. Their framework, LIMI (Less Is More for Intelligent Agency), builds on similar work in other areas of LLM research and finds that “machine autonomy emerges not from data abundance but from strategic curation of high-quality agentic demonstrations.” In other words, it's data quality, not quantity, that matters. In experiments, the researchers found that with a small, but carefully curated, dataset of just 78 examples, they could train LLMs to outperform models trained on thousands of examples by a considerable margin on key industry benchmarks. This discovery could have important implications for enterprise applications where data is scarce or expensive to collect.The challenge of building agents that workThe researchers define agency as “the emergent capacity of AI systems to function as autonomous agents–actively discovering problems, formulating hypotheses, and executing solutions through self-d
Beatings, killings, and lasting fear: The human toll of MoD's Afghan data breach: Research submitted to Parliament details deaths, raids, and mental trauma linked to 2022 relocation leak Research submitted to the UK Parliament has revealed explicit threats to life and the deaths of family members and colleagues directly linked to the Ministry of Defence's 2022 Afghan relocation scheme data breach.…
North Korea's Lazarus Group shares its malware with IT work scammers: North Korean-linked cybercriminals have enhanced their malware capabilities, sharing advanced tools with IT work scammers. DeceptiveDevelopment, a North Korea-aligned group active since 2023, overlaps with campaigns like Contagious Interview and WageMole, and is tracked by CrowdStrike as Famous Chollima. The group uses social engineering tactics, such as fake job profiles and CAPTCHA prompts, to infect victims' computers with trojanized codebases. They steal data and identities to help North Korean IT workers secure jobs with Western companies, funneling salaries back to Pyongyang. DeceptiveDevelopment's malware payloads include BeaverTail, InvisibleFerret, and OtterCookie, with BeaverTail evolving into OtterCookie. BeaverTail is an infostealer targeting cryptocurrency wallets and browser logins, while InvisibleFerret is a Python-based malware with remote control capabilities. Tropidoor, a sophisticated backdoor, supports Windows commands like schtasks and ping, and TsunamiKit, a toolkit for stealing information and cryptocurrency, predates DeceptiveDevelopment's activity. The overlap between DeceptiveDevelopment's campaigns and North Korean IT worker operations highlights the need for defenders to consider broader threat ecosystems.</description>
      <itunes:summary>Articles discussed
What brain privacy will look like in the age of neurotech: Nita Farahany spoke with Recorded Future News about whether brain data will be commodified and the role artificial intelligence plays in allowing internal speech to be decoded.
Zero Trust Has a Blind Spot—Your AI Agents: AI agents now act, decide, and access systems on their own — creating new blind spots Zero Trust can't see. Token Security helps organizations govern AI identities so every agent's access, intent, and action are verified and accountable. [...]
New AI training method creates powerful software agents with just 78 examples: A new study by Shanghai Jiao Tong University and SII Generative AI Research Lab (GAIR) shows that training large language models (LLMs) for complex, autonomous tasks does not require massive datasets. Their framework, LIMI (Less Is More for Intelligent Agency), builds on similar work in other areas of LLM research and finds that “machine autonomy emerges not from data abundance but from strategic curation of high-quality agentic demonstrations.” In other words, it's data quality, not quantity, that matters. In experiments, the researchers found that with a small, but carefully curated, dataset of just 78 examples, they could train LLMs to outperform models trained on thousands of examples by a considerable margin on key industry benchmarks. This discovery could have important implications for enterprise applications where data is scarce or expensive to collect.The challenge of building agents that workThe researchers define agency as “the emergent capacity of AI systems to function as autonomous agents–actively discovering problems, formulating hypotheses, and executing solutions through self-d
Beatings, killings, and lasting fear: The human toll of MoD's Afghan data breach: Research submitted to Parliament details deaths, raids, and mental trauma linked to 2022 relocation leak Research submitted to the UK Parliament has revealed explicit threats to life and the deaths of family members and colleagues directly linked to the Ministry of Defence's 2022 Afghan relocation scheme data breach.…
North Korea's Lazarus Group shares its malware with IT work scammers: North Korean-linked cybercriminals have enhanced their malware capabilities, sharing advanced tools with IT work scammers. DeceptiveDevelopment, a North Korea-aligned group active since 2023, overlaps with campaigns like Contagious Interview and WageMole, and is tracked by CrowdStrike as Famous Chollima. The group uses social engineering tactics, such as fake job profiles and CAPTCHA prompts, to infect victims' computers with trojanized codebases. They steal data and identities to help North Korean IT workers secure jobs with Western companies, funneling salaries back to Pyongyang. DeceptiveDevelopment's malware payloads include BeaverTail, InvisibleFerret, and OtterCookie, with BeaverTail evolving into OtterCookie. BeaverTail is an infostealer targeting cryptocurrency wallets and browser logins, while InvisibleFerret is a Python-based malware with remote control capabilities. Tropidoor, a sophisticated backdoor, supports Windows commands like schtasks and ping, and TsunamiKit, a toolkit for stealing information and cryptocurrency, predates DeceptiveDevelopment's activity. The overlap between DeceptiveDevelopment's campaigns and North Korean IT worker operations highlights the need for defenders to consider broader threat ecosystems.</itunes:summary>
      <pubDate>Wed, 12 Nov 2025 17:38:53 GMT</pubDate>
      <enclosure url="https://raw.githubusercontent.com/Refugeek/podcast-feed/main/episodes-EScyber/20251029091515-what-brain-privacy-will-look-like-in-the-age-of-neurotech-zero-trust-has-a-blind-spot-your-ai-agents.mp3" type="audio/mpeg" length="34708364"/>
      <itunes:duration>1735</itunes:duration>
    </item>
    <item>
      <title>20251102104026-is-rag-dead-the-rise-of-context-engineering-and-semantic-layers-for-agentic-ai-greg-kroah-hartman-explains-the-cyber-resilience-act-for-open-source-developers</title>
      <description>Articles discussed
Is RAG Dead? The Rise of Context Engineering and Semantic Layers for Agentic AI: Retrieval-Augmented Generation (RAG) has evolved into a broader field known as context engineering, which is essential for agentic AI systems. Initially popularized by ChatGPT, RAG aimed to enhance LLM responses by retrieving relevant information at query time. However, its limitations in handling incorrect or irrelevant data led to the development of new techniques like re-rankers. GraphRAG, a variation of RAG using knowledge graphs, has gained prominence for its ability to reason over entities and relationships, improving LLM accuracy and explainability. This evolution has spurred significant industry activity, with companies like Microsoft, Samsung, and Ontotext acquiring companies to advance knowledge graphs and semantic technologies. The rise of GraphRAG has reignited interest in knowledge graphs, positioning them as critical components in AI systems, moving beyond metadata management to become the semantic backbone for AI applications.
Greg Kroah-Hartman explains the Cyber Resilience Act for open source developers: The European Union's Cyber Resilience Act (CRA) mandates digital product producers to document vulnerabilities and respond to security incidents, impacting open-source software developers. Greg Kroah-Hartman, a Linux kernel maintainer, argues the CRA benefits open-source contributors by ensuring transparency and accountability. The CRA distinguishes between commercial entities and unpaid contributors, allowing individuals to publish software without legal risk. It applies to commercial manufacturers using open-source code, requiring Software Bill of Materials (SBOMs) and proactive incident management. While the CRA primarily affects EU-based manufacturers, its global reach extends to companies selling products accessible within the EU. The act is expected to increase demand for well-supported open-source projects, countering fears of reduced open-source use. Foundations and large projects are collaborating with the EU to simplify compliance through checklists and templates.
This security hole can crash billions of Chromium browsers, and Google hasn't patched it yet: Security researcher Jose Pino discovered a critical vulnerability in Chromium's Blink rendering engine, which can be exploited to crash many Chromium-based browsers within seconds. This flaw, known as Brash, affects over 3 billion users worldwide, as it can be used to crash browsers like Chrome, Microsoft Edge, Brave, and Vivaldi. Pino demonstrated the vulnerability by testing it on 11 major browsers, finding that it works on nine of them. The attack exploits an architectural flaw in Blink, specifically the absence of rate limiting on document.title API updates, allowing for millions of DOM mutations per second. This saturates the main thread, disrupting the event loop and causing the interface to collapse. Pino disclosed the issue to the Chromium security team, but did not receive a response.</description>
      <itunes:summary>Articles discussed
Is RAG Dead? The Rise of Context Engineering and Semantic Layers for Agentic AI: Retrieval-Augmented Generation (RAG) has evolved into a broader field known as context engineering, which is essential for agentic AI systems. Initially popularized by ChatGPT, RAG aimed to enhance LLM responses by retrieving relevant information at query time. However, its limitations in handling incorrect or irrelevant data led to the development of new techniques like re-rankers. GraphRAG, a variation of RAG using knowledge graphs, has gained prominence for its ability to reason over entities and relationships, improving LLM accuracy and explainability. This evolution has spurred significant industry activity, with companies like Microsoft, Samsung, and Ontotext acquiring companies to advance knowledge graphs and semantic technologies. The rise of GraphRAG has reignited interest in knowledge graphs, positioning them as critical components in AI systems, moving beyond metadata management to become the semantic backbone for AI applications.
Greg Kroah-Hartman explains the Cyber Resilience Act for open source developers: The European Union's Cyber Resilience Act (CRA) mandates digital product producers to document vulnerabilities and respond to security incidents, impacting open-source software developers. Greg Kroah-Hartman, a Linux kernel maintainer, argues the CRA benefits open-source contributors by ensuring transparency and accountability. The CRA distinguishes between commercial entities and unpaid contributors, allowing individuals to publish software without legal risk. It applies to commercial manufacturers using open-source code, requiring Software Bill of Materials (SBOMs) and proactive incident management. While the CRA primarily affects EU-based manufacturers, its global reach extends to companies selling products accessible within the EU. The act is expected to increase demand for well-supported open-source projects, countering fears of reduced open-source use. Foundations and large projects are collaborating with the EU to simplify compliance through checklists and templates.
This security hole can crash billions of Chromium browsers, and Google hasn't patched it yet: Security researcher Jose Pino discovered a critical vulnerability in Chromium's Blink rendering engine, which can be exploited to crash many Chromium-based browsers within seconds. This flaw, known as Brash, affects over 3 billion users worldwide, as it can be used to crash browsers like Chrome, Microsoft Edge, Brave, and Vivaldi. Pino demonstrated the vulnerability by testing it on 11 major browsers, finding that it works on nine of them. The attack exploits an architectural flaw in Blink, specifically the absence of rate limiting on document.title API updates, allowing for millions of DOM mutations per second. This saturates the main thread, disrupting the event loop and causing the interface to collapse. Pino disclosed the issue to the Chromium security team, but did not receive a response.</itunes:summary>
      <pubDate>Wed, 12 Nov 2025 17:38:53 GMT</pubDate>
      <enclosure url="https://raw.githubusercontent.com/Refugeek/podcast-feed/main/episodes-EScyber/20251102104026-is-rag-dead-the-rise-of-context-engineering-and-semantic-layers-for-agentic-ai-greg-kroah-hartman-explains-the-cyber-resilience-act-for-open-source-developers.mp3" type="audio/mpeg" length="32911244"/>
      <itunes:duration>1645</itunes:duration>
    </item>
    <item>
      <title>20251102131848-as-scientists-show-they-can-read-inner-speech-brain-implant-pioneers-fight-for-neural-data-privacy-access-rights-what-we-know-about-the-npm-supply-chain-attack</title>
      <description>Articles discussed
As scientists show they can read inner speech, brain implant ‘pioneers’ fight for neural data privacy, access rights: J. Galen Buckwalter, a quadriplegic, underwent a craniotomy in 2024 to participate in a Caltech study. The study aims to decode how his neurons signal his hands to grasp objects. Buckwalter expressed concerns about the lack of explicit protections for his neural data privacy and access rights. With advancements in brain-computer interface (BCI) research, scientists can decode inner speech from neural data, raising privacy concerns. Buckwalter and others are forming the BCI Pioneers Coalition to advocate for ethical and legal norms for future research studies. They plan to draft guidelines for informed consent agreements to protect participants' data rights.
What We Know About the NPM Supply Chain Attack: A supply chain attack on Node Package Manager (NPM) compromised accounts and injected malicious code into JavaScript packages, affecting software ecosystems globally. Attackers used phishing to steal credentials, exploiting vulnerabilities in NPM maintainer accounts. The attack injected the Shai-hulud worm, which autonomously replicates and spreads through compromised packages, stealing cloud service tokens and deploying secret-scanning tools. This worm exploits post-install scripts to infect additional projects, making it difficult to contain. The attack impacted over 500 NPM packages, particularly those with high global download rates, threatening organizations' trust in open-source dependencies. Security measures like Trend Vision One™ can detect and block indicators of compromise, providing tailored threat hunting and intelligence reports.
When AI Agents Go Rogue: Agent Session Smuggling Attack in A2A Systems: Agent session smuggling is a new attack vector that exploits stateful communication sessions in A2A systems. A malicious remote agent intercepts a legitimate client request and server response, injecting covert instructions into the conversation. This hidden manipulation can lead to context poisoning, data exfiltration, or unauthorized tool execution. The attack exploits the built-in trust relationships between agents in stateful protocols, making it particularly dangerous. Mitigation strategies include human-in-the-loop enforcement, remote agent verification, and context-grounding techniques.
Invisible npm malware pulls a disappearing act – then nicks your tokens: PhantomRaven is a supply chain attack that has been active since at least August 2025. It involves 126 malicious packages published by multiple accounts. At least 86,000 downloads were recorded before the campaign was exposed this week, and more than 80 of the infected packages were still live at the time of disclosure. What sets PhantomRaven apart is its use of a new technique the researchers call Remote Dynamic Dependencies (RDD). Unlike typical npm malware, which relies on visible dependencies or post-install scripts, PhantomRaven packages initially appear empty – no dependencies, no suspicious code. But when a user installs them, the package fetches additional code from a remote server controlled by the attacker. The malicious payload is then executed locally, stealing data and exfiltrating it to the attacker's infrastructure. This makes the attack extremely difficult to detect using conventional methods. Security tools that rely on static analysis of package metadata or dependency graphs will see nothing out of the ordinary, because the harmful code doesn't exist in the registry itself. Instead, it's dynamically retrieved during installation, leaving no obvious trace in the source files. The stolen information includes npm and GitHub tokens, cloud credentials, SSH keys, and other sensitive environment</description>
      <itunes:summary>Articles discussed
As scientists show they can read inner speech, brain implant ‘pioneers’ fight for neural data privacy, access rights: J. Galen Buckwalter, a quadriplegic, underwent a craniotomy in 2024 to participate in a Caltech study. The study aims to decode how his neurons signal his hands to grasp objects. Buckwalter expressed concerns about the lack of explicit protections for his neural data privacy and access rights. With advancements in brain-computer interface (BCI) research, scientists can decode inner speech from neural data, raising privacy concerns. Buckwalter and others are forming the BCI Pioneers Coalition to advocate for ethical and legal norms for future research studies. They plan to draft guidelines for informed consent agreements to protect participants' data rights.
What We Know About the NPM Supply Chain Attack: A supply chain attack on Node Package Manager (NPM) compromised accounts and injected malicious code into JavaScript packages, affecting software ecosystems globally. Attackers used phishing to steal credentials, exploiting vulnerabilities in NPM maintainer accounts. The attack injected the Shai-hulud worm, which autonomously replicates and spreads through compromised packages, stealing cloud service tokens and deploying secret-scanning tools. This worm exploits post-install scripts to infect additional projects, making it difficult to contain. The attack impacted over 500 NPM packages, particularly those with high global download rates, threatening organizations' trust in open-source dependencies. Security measures like Trend Vision One™ can detect and block indicators of compromise, providing tailored threat hunting and intelligence reports.
When AI Agents Go Rogue: Agent Session Smuggling Attack in A2A Systems: Agent session smuggling is a new attack vector that exploits stateful communication sessions in A2A systems. A malicious remote agent intercepts a legitimate client request and server response, injecting covert instructions into the conversation. This hidden manipulation can lead to context poisoning, data exfiltration, or unauthorized tool execution. The attack exploits the built-in trust relationships between agents in stateful protocols, making it particularly dangerous. Mitigation strategies include human-in-the-loop enforcement, remote agent verification, and context-grounding techniques.
Invisible npm malware pulls a disappearing act – then nicks your tokens: PhantomRaven is a supply chain attack that has been active since at least August 2025. It involves 126 malicious packages published by multiple accounts. At least 86,000 downloads were recorded before the campaign was exposed this week, and more than 80 of the infected packages were still live at the time of disclosure. What sets PhantomRaven apart is its use of a new technique the researchers call Remote Dynamic Dependencies (RDD). Unlike typical npm malware, which relies on visible dependencies or post-install scripts, PhantomRaven packages initially appear empty – no dependencies, no suspicious code. But when a user installs them, the package fetches additional code from a remote server controlled by the attacker. The malicious payload is then executed locally, stealing data and exfiltrating it to the attacker's infrastructure. This makes the attack extremely difficult to detect using conventional methods. Security tools that rely on static analysis of package metadata or dependency graphs will see nothing out of the ordinary, because the harmful code doesn't exist in the registry itself. Instead, it's dynamically retrieved during installation, leaving no obvious trace in the source files. The stolen information includes npm and GitHub tokens, cloud credentials, SSH keys, and other sensitive environment</itunes:summary>
      <pubDate>Wed, 12 Nov 2025 17:38:53 GMT</pubDate>
      <enclosure url="https://raw.githubusercontent.com/Refugeek/podcast-feed/main/episodes-EScyber/20251102131848-as-scientists-show-they-can-read-inner-speech-brain-implant-pioneers-fight-for-neural-data-privacy-access-rights-what-we-know-about-the-npm-supply-chain-attack.mp3" type="audio/mpeg" length="32936684"/>
      <itunes:duration>1646</itunes:duration>
    </item>
    <item>
      <title>20251104083555-self-propagating-supply-chain-attack-hits-187-npm-packages-enisa-will-operate-the-eu-cybersecurity-reserve-what-this-means-for-managed-security-service-providers</title>
      <description>Articles discussed
Self-propagating supply chain attack hits 187 npm packages: A coordinated worm-style campaign dubbed 'Shai-Hulud' has compromised at least 187 npm packages, starting with the @ctrl/tinycolor package. This attack uses a self-propagating payload to infect other packages, exploiting vulnerabilities in package management systems. Researchers identified the attack's origin in npmjs.com and linked it to CrowdStrike's npm namespace. The malware employs TruffleHog, a legitimate secret scanner, to exfiltrate sensitive data like API keys and credentials. This incident follows high-profile supply chain attacks, highlighting the fragility of modern software supply chains and the need for enhanced security measures.
ENISA Will Operate the EU Cybersecurity Reserve. What This Means for Managed Security Service Providers: The European Union is building a new line of defense. On August 26, 2025, the European Commission and the EU Agency for Cybersecurity (ENISA) signed a contribution agreement that hands ENISA the keys to the EU Cybersecurity Reserve. The deal comes with funding: €36 million over three years. ENISA's mission is straightforward, if not simple. It will administer, operate, and monitor the bloc’s emergency cyber response capabilities. The Cybersecurity Reserve is designed as a pool of pre-contracted services from trusted managed security providers. When a major incident hits, Member States, EU institutions, or even associated countries can call on those services. The Reserve offers a way to respond faster and with more weight than national resources alone. How the Cybersecurity Reserve Works ENISA will run the procurement process and will sign contracts with managed security service providers (MSSPs). These providers, vetted through public tenders, will stand ready to deploy incident response teams, forensic expertise, and recovery services. The aim is efficiency. Money spent should strengthen resilience, whether or not the alarm bells ring. The Money and the Mandate Contribution agreements like this aren’t new. ENISA has received similar top-up funding for special projects: the Cybersecurity Support Action, the Single Reporting
Defeating KASLR by Doing Nothing at All: Researchers Jann and Seth Jenkins discovered a vulnerability in the Linux kernel's linear mapping, which allows arbitrary reads without KASLR. They found that Pixel phones' bootloaders compress the kernel at a static physical address, making it possible to calculate a static kernel virtual address for any kernel .data entry. This enables attackers to access kernel symbols at predictable addresses, even with KASLR enabled. The impact is significant, as it allows for arbitrary read-write access to kernel memory, potentially bypassing defenses. This finding highlights the importance of addressing non-randomization in kernel memory management to prevent such vulnerabilities.
SesameOp: Novel backdoor uses OpenAI Assistants API for command and control: Researchers from Microsoft’s Incident Response – Detection and Response Team (DART) discovered a sophisticated backdoor called SesameOp in July 2025. This backdoor exploits the OpenAI Assistants API for command-and-control (C2) communications, enabling stealthy orchestration of malicious activities within compromised environments. SesameOp leverages Microsoft Visual Studio utilities, specifically .NET AppDomainManager injection, to evade detection and maintain persistence. The backdoor employs advanced obfuscation techniques, including payload compression and layered encryption, to secure communications. Researchers identified the infection chain, which includes a loader (Netapi64.dll) and a NET-based backdoor (OpenAIAgent.Netapi64), demonstrating threat actors' adaptability in exploiting emerging technologies. Microsoft and OpenAI collaborated to mitigate the threat by disabling an API key associated with the actor. SesameOp underscores the evolving tactics of cyber adversaries and the importance of continuous vigilance against novel attack vectors.</description>
      <itunes:summary>Articles discussed
Self-propagating supply chain attack hits 187 npm packages: A coordinated worm-style campaign dubbed 'Shai-Hulud' has compromised at least 187 npm packages, starting with the @ctrl/tinycolor package. This attack uses a self-propagating payload to infect other packages, exploiting vulnerabilities in package management systems. Researchers identified the attack's origin in npmjs.com and linked it to CrowdStrike's npm namespace. The malware employs TruffleHog, a legitimate secret scanner, to exfiltrate sensitive data like API keys and credentials. This incident follows high-profile supply chain attacks, highlighting the fragility of modern software supply chains and the need for enhanced security measures.
ENISA Will Operate the EU Cybersecurity Reserve. What This Means for Managed Security Service Providers: The European Union is building a new line of defense. On August 26, 2025, the European Commission and the EU Agency for Cybersecurity (ENISA) signed a contribution agreement that hands ENISA the keys to the EU Cybersecurity Reserve. The deal comes with funding: €36 million over three years. ENISA's mission is straightforward, if not simple. It will administer, operate, and monitor the bloc’s emergency cyber response capabilities. The Cybersecurity Reserve is designed as a pool of pre-contracted services from trusted managed security providers. When a major incident hits, Member States, EU institutions, or even associated countries can call on those services. The Reserve offers a way to respond faster and with more weight than national resources alone. How the Cybersecurity Reserve Works ENISA will run the procurement process and will sign contracts with managed security service providers (MSSPs). These providers, vetted through public tenders, will stand ready to deploy incident response teams, forensic expertise, and recovery services. The aim is efficiency. Money spent should strengthen resilience, whether or not the alarm bells ring. The Money and the Mandate Contribution agreements like this aren’t new. ENISA has received similar top-up funding for special projects: the Cybersecurity Support Action, the Single Reporting
Defeating KASLR by Doing Nothing at All: Researchers Jann and Seth Jenkins discovered a vulnerability in the Linux kernel's linear mapping, which allows arbitrary reads without KASLR. They found that Pixel phones' bootloaders compress the kernel at a static physical address, making it possible to calculate a static kernel virtual address for any kernel .data entry. This enables attackers to access kernel symbols at predictable addresses, even with KASLR enabled. The impact is significant, as it allows for arbitrary read-write access to kernel memory, potentially bypassing defenses. This finding highlights the importance of addressing non-randomization in kernel memory management to prevent such vulnerabilities.
SesameOp: Novel backdoor uses OpenAI Assistants API for command and control: Researchers from Microsoft’s Incident Response – Detection and Response Team (DART) discovered a sophisticated backdoor called SesameOp in July 2025. This backdoor exploits the OpenAI Assistants API for command-and-control (C2) communications, enabling stealthy orchestration of malicious activities within compromised environments. SesameOp leverages Microsoft Visual Studio utilities, specifically .NET AppDomainManager injection, to evade detection and maintain persistence. The backdoor employs advanced obfuscation techniques, including payload compression and layered encryption, to secure communications. Researchers identified the infection chain, which includes a loader (Netapi64.dll) and a NET-based backdoor (OpenAIAgent.Netapi64), demonstrating threat actors' adaptability in exploiting emerging technologies. Microsoft and OpenAI collaborated to mitigate the threat by disabling an API key associated with the actor. SesameOp underscores the evolving tactics of cyber adversaries and the importance of continuous vigilance against novel attack vectors.</itunes:summary>
      <pubDate>Wed, 12 Nov 2025 17:38:53 GMT</pubDate>
      <enclosure url="https://raw.githubusercontent.com/Refugeek/podcast-feed/main/episodes-EScyber/20251104083555-self-propagating-supply-chain-attack-hits-187-npm-packages-enisa-will-operate-the-eu-cybersecurity-reserve-what-this-means-for-managed-security-service-providers.mp3" type="audio/mpeg" length="33006284"/>
      <itunes:duration>1650</itunes:duration>
    </item>
    <item>
      <title>20251104153837-why-context-is-the-new-currency-in-ai-from-rag-to-context-engineering-malware-injected-into-code-packages-that-get-2-billion-downloads-each-week</title>
      <description>Articles discussed
Why Context Is the New Currency in AI: From RAG to Context Engineering: The article discusses the growing importance of context in AI systems, highlighting a shift from traditional retrieval-based approaches to context engineering. Key actors include AI developers and researchers who are rethinking how information is managed and integrated within AI workflows. The method involves a comprehensive approach that considers user intent, instruction layering, and external data, moving beyond static buffers to dynamic, narrative-based structures. This approach aims to improve decision-making by ensuring information is delivered in a sequence that supports effective reasoning. The impact of this shift is significant, as it addresses the limitations of traditional systems that often fail to provide meaningful context, leading to inefficiencies and reduced utility. The novelty lies in the emphasis on compositional context rather than additive information, which can mitigate issues like &quot;attention dilution.&quot;
Malware Injected Into Code Packages That Get 2 Billion+ Downloads Each Week: In a significant cybersecurity breach, unidentified hackers compromised 18 npm packages, which collectively handle over two billion downloads weekly. These packages, including popular ones like 'ansi-styles' and 'debug', were targeted through a phishing campaign that exploited a compromised npm package maintainer account. The malicious code injected into these packages intercepts crypto and Web3 activity, manipulates wallet interactions, and redirects funds to attacker-controlled accounts. This breach underscores the importance of security measures beyond one's own codebase, highlighting the risks associated with software dependencies. The compromised packages have since been removed by the npm registry, but the incident serves as a reminder of the evolving tactics cybercriminals employ.
OpenAI API moonlights as malware HQ in Microsoft’s latest discovery: Redmond uncovers SesameOp, a backdoor hiding its tracks by using OpenAI’s Assistants API as a command channel Hackers have found a new use for OpenAI's Assistants API – not to write poems or code, but to secretly control malware.…
Gotta fly: Lazarus targets the UAV sector: ESET Research has observed a new instance of the Operation DreamJob cyberespionage campaign conducted by Lazarus, a North Korea-aligned APT group. The campaign targeted several European companies active in the defense industry, with a focus on UAV technology, suggesting a link to North Korea’s drone program. The primary goal of the attackers was likely the theft of proprietary information and manufacturing know-how. The social-engineering technique used for initial access involved trojanizing open-source projects from GitHub and deploying ScoringMathTea, a RAT that offers full control over compromised machines. The introduction of new libraries designed for DLL proxying and the selection of new open-source projects for trojanization represent significant evolutions in the group’s toolset. This activity is attributed to Lazarus campaigns related to Operation DreamJob, based on the observed tactics and targets.</description>
      <itunes:summary>Articles discussed
Why Context Is the New Currency in AI: From RAG to Context Engineering: The article discusses the growing importance of context in AI systems, highlighting a shift from traditional retrieval-based approaches to context engineering. Key actors include AI developers and researchers who are rethinking how information is managed and integrated within AI workflows. The method involves a comprehensive approach that considers user intent, instruction layering, and external data, moving beyond static buffers to dynamic, narrative-based structures. This approach aims to improve decision-making by ensuring information is delivered in a sequence that supports effective reasoning. The impact of this shift is significant, as it addresses the limitations of traditional systems that often fail to provide meaningful context, leading to inefficiencies and reduced utility. The novelty lies in the emphasis on compositional context rather than additive information, which can mitigate issues like &quot;attention dilution.&quot;
Malware Injected Into Code Packages That Get 2 Billion+ Downloads Each Week: In a significant cybersecurity breach, unidentified hackers compromised 18 npm packages, which collectively handle over two billion downloads weekly. These packages, including popular ones like 'ansi-styles' and 'debug', were targeted through a phishing campaign that exploited a compromised npm package maintainer account. The malicious code injected into these packages intercepts crypto and Web3 activity, manipulates wallet interactions, and redirects funds to attacker-controlled accounts. This breach underscores the importance of security measures beyond one's own codebase, highlighting the risks associated with software dependencies. The compromised packages have since been removed by the npm registry, but the incident serves as a reminder of the evolving tactics cybercriminals employ.
OpenAI API moonlights as malware HQ in Microsoft’s latest discovery: Redmond uncovers SesameOp, a backdoor hiding its tracks by using OpenAI’s Assistants API as a command channel Hackers have found a new use for OpenAI's Assistants API – not to write poems or code, but to secretly control malware.…
Gotta fly: Lazarus targets the UAV sector: ESET Research has observed a new instance of the Operation DreamJob cyberespionage campaign conducted by Lazarus, a North Korea-aligned APT group. The campaign targeted several European companies active in the defense industry, with a focus on UAV technology, suggesting a link to North Korea’s drone program. The primary goal of the attackers was likely the theft of proprietary information and manufacturing know-how. The social-engineering technique used for initial access involved trojanizing open-source projects from GitHub and deploying ScoringMathTea, a RAT that offers full control over compromised machines. The introduction of new libraries designed for DLL proxying and the selection of new open-source projects for trojanization represent significant evolutions in the group’s toolset. This activity is attributed to Lazarus campaigns related to Operation DreamJob, based on the observed tactics and targets.</itunes:summary>
      <pubDate>Wed, 12 Nov 2025 17:38:53 GMT</pubDate>
      <enclosure url="https://raw.githubusercontent.com/Refugeek/podcast-feed/main/episodes-EScyber/20251104153837-why-context-is-the-new-currency-in-ai-from-rag-to-context-engineering-malware-injected-into-code-packages-that-get-2-billion-downloads-each-week.mp3" type="audio/mpeg" length="38763884"/>
      <itunes:duration>1938</itunes:duration>
    </item>
    <item>
      <title>20251105193822-how-sakana-ais-new-evolutionary-algorithm-builds-powerful-ai-models-without-expensive-retraining-opencuas-open-source-computer-use-agents-rival-proprietary-models-from-openai-and-anthropic</title>
      <description>Articles discussed
How Sakana AI’s new evolutionary algorithm builds powerful AI models without expensive retraining: Sakana AI has developed a new model merging technique called M2N2, which enables the creation of powerful multi-skilled agents without the high cost and data needs of retraining. This method allows for the development of AI models that can perform multiple tasks efficiently. The technique is particularly useful in fields requiring diverse skill sets, such as healthcare and customer service. M2N2 leverages evolutionary algorithms to optimize model performance, making it a novel approach in AI development. The impact of this innovation is expected to be significant, as it could reduce costs and time associated with AI model development. This advancement marks a step forward in the field of AI, offering a more scalable and efficient solution for complex problem-solving tasks.
OpenCUA’s open source computer-use agents rival proprietary models from OpenAI and Anthropic: OpenCUA is an open-source framework that provides data and training recipes for building powerful computer-use agents. These agents challenge proprietary systems like those from OpenAI and Anthropic. The framework is designed to rival proprietary models in terms of capability and performance. Its open-source nature allows for greater accessibility and potential for innovation. The impact of OpenCUA could be significant in advancing AI capabilities and democratizing access to advanced AI technologies. The novelty of this framework lies in its open-source approach, which contrasts with the closed-source models of major AI companies.
Google warns of new AI-powered malware families deployed in the wild: Google's Threat Intelligence Group (GTIG) has identified a major shift this year, with adversaries leveraging artificial intelligence to deploy new malware families that integrate large language models (LLMs) during execution. This new approach enables dynamic altering mid-execution, which reaches new levels of operational versatility that are virtually impossible to achieve with traditional malware. Google calls the technique &quot;just-in-time&quot; self-modification and highlights the experimental PromptFlux malware dropper and the PromptSteal (a.k.a. LameHug) data miner deployed in Ukraine, as examples for dynamic script generation, code obfuscation, and creation of on-demand functions. PromptFlux is an experimental VBScript dropper that leverages Google's LLM Gemini in its latest version to generate obfuscated VBScript variants. It attempts persistence via Startup folder entries, and spreads laterally on removable drives and mapped network shares. &quot;The most novel component of PROMPTFLUX is its 'Thinking Robot' module, designed to periodically query Gemini to obtain new code for evading antivirus software,&quot; explains Google. The prompt is very specific and machine-parsable, according to the researchers, who see indications that the malware's creators aim to create an ever-evolving &quot;metamorphic script.&quot; PromptFlux &quot;StartThinkingRobot&quot; functionSource: Google Google could not attribute PromptFlux to a specific threat actor, but noted that the tactics, techniques, and procedures indicate
What to Do When Your Credit Risk Model Works Today, but Breaks Six Months Later: A study reveals that gradient-boosted trees (XGBoost) fail to maintain ranking stability over time, leading to significant accuracy degradation in credit risk models. The research identifies that the geometry of optimization, rather than data or hyperparameters, is crucial for maintaining model stability. By adopting a Hamiltonian framework for neural network training, the study demonstrates improved temporal stability, with AUC scores dropping only 10.6 points over six years compared to XGBoost's 32 points. This approach preserves relative orderings, crucial for ranking tasks like credit risk assessment. The findings suggest that optimizing models on symplectic manifolds can enhance stability, offering broader applicability across systems requiring ranking, such as medical risk stratification and fraud detection.
Russian spies pack custom malware into hidden VMs on Windows machines: Russian spies, known as Curly COMrades, are exploiting Microsoft's Hyper-V hypervisor to create hidden virtual machines (VMs) on compromised Windows machines. This technique allows them to bypass endpoint detection and response (EDR) tools, providing long-term network access for surveillance and malware deployment. The attackers use a lightweight Alpine Linux-based VM to host custom malware, including CurlyShell and CurlCat, which are designed to evade detection and maintain persistence. This campaign, which began in July, has targeted judicial and government bodies in Georgia and an energy company in Moldova, with Bitdefender tracking the group since 2024. The sophistication of this approach highlights a growing trend of threat actors bypassing EDR solutions by leveraging legitimate virtualization technologies. To counter this, experts recommend a multi-layered defense strategy beyond endpoint detection.</description>
      <itunes:summary>Articles discussed
How Sakana AI’s new evolutionary algorithm builds powerful AI models without expensive retraining: Sakana AI has developed a new model merging technique called M2N2, which enables the creation of powerful multi-skilled agents without the high cost and data needs of retraining. This method allows for the development of AI models that can perform multiple tasks efficiently. The technique is particularly useful in fields requiring diverse skill sets, such as healthcare and customer service. M2N2 leverages evolutionary algorithms to optimize model performance, making it a novel approach in AI development. The impact of this innovation is expected to be significant, as it could reduce costs and time associated with AI model development. This advancement marks a step forward in the field of AI, offering a more scalable and efficient solution for complex problem-solving tasks.
OpenCUA’s open source computer-use agents rival proprietary models from OpenAI and Anthropic: OpenCUA is an open-source framework that provides data and training recipes for building powerful computer-use agents. These agents challenge proprietary systems like those from OpenAI and Anthropic. The framework is designed to rival proprietary models in terms of capability and performance. Its open-source nature allows for greater accessibility and potential for innovation. The impact of OpenCUA could be significant in advancing AI capabilities and democratizing access to advanced AI technologies. The novelty of this framework lies in its open-source approach, which contrasts with the closed-source models of major AI companies.
Google warns of new AI-powered malware families deployed in the wild: Google's Threat Intelligence Group (GTIG) has identified a major shift this year, with adversaries leveraging artificial intelligence to deploy new malware families that integrate large language models (LLMs) during execution. This new approach enables dynamic altering mid-execution, which reaches new levels of operational versatility that are virtually impossible to achieve with traditional malware. Google calls the technique &quot;just-in-time&quot; self-modification and highlights the experimental PromptFlux malware dropper and the PromptSteal (a.k.a. LameHug) data miner deployed in Ukraine, as examples for dynamic script generation, code obfuscation, and creation of on-demand functions. PromptFlux is an experimental VBScript dropper that leverages Google's LLM Gemini in its latest version to generate obfuscated VBScript variants. It attempts persistence via Startup folder entries, and spreads laterally on removable drives and mapped network shares. &quot;The most novel component of PROMPTFLUX is its 'Thinking Robot' module, designed to periodically query Gemini to obtain new code for evading antivirus software,&quot; explains Google. The prompt is very specific and machine-parsable, according to the researchers, who see indications that the malware's creators aim to create an ever-evolving &quot;metamorphic script.&quot; PromptFlux &quot;StartThinkingRobot&quot; functionSource: Google Google could not attribute PromptFlux to a specific threat actor, but noted that the tactics, techniques, and procedures indicate
What to Do When Your Credit Risk Model Works Today, but Breaks Six Months Later: A study reveals that gradient-boosted trees (XGBoost) fail to maintain ranking stability over time, leading to significant accuracy degradation in credit risk models. The research identifies that the geometry of optimization, rather than data or hyperparameters, is crucial for maintaining model stability. By adopting a Hamiltonian framework for neural network training, the study demonstrates improved temporal stability, with AUC scores dropping only 10.6 points over six years compared to XGBoost's 32 points. This approach preserves relative orderings, crucial for ranking tasks like credit risk assessment. The findings suggest that optimizing models on symplectic manifolds can enhance stability, offering broader applicability across systems requiring ranking, such as medical risk stratification and fraud detection.
Russian spies pack custom malware into hidden VMs on Windows machines: Russian spies, known as Curly COMrades, are exploiting Microsoft's Hyper-V hypervisor to create hidden virtual machines (VMs) on compromised Windows machines. This technique allows them to bypass endpoint detection and response (EDR) tools, providing long-term network access for surveillance and malware deployment. The attackers use a lightweight Alpine Linux-based VM to host custom malware, including CurlyShell and CurlCat, which are designed to evade detection and maintain persistence. This campaign, which began in July, has targeted judicial and government bodies in Georgia and an energy company in Moldova, with Bitdefender tracking the group since 2024. The sophistication of this approach highlights a growing trend of threat actors bypassing EDR solutions by leveraging legitimate virtualization technologies. To counter this, experts recommend a multi-layered defense strategy beyond endpoint detection.</itunes:summary>
      <pubDate>Wed, 12 Nov 2025 17:38:53 GMT</pubDate>
      <enclosure url="https://raw.githubusercontent.com/Refugeek/podcast-feed/main/episodes-EScyber/20251105193822-how-sakana-ais-new-evolutionary-algorithm-builds-powerful-ai-models-without-expensive-retraining-opencuas-open-source-computer-use-agents-rival-proprietary-models-from-openai-and-anthropic.mp3" type="audio/mpeg" length="28814924"/>
      <itunes:duration>1440</itunes:duration>
    </item>
    <item>
      <title>20251106022824-japan-s-active-cyberdefense-law-a-new-era-in-cybersecurity-strategy-agentic-ais-ooda-loop-problem</title>
      <description>Articles discussed
Japan's Active Cyberdefense Law: A New Era in Cybersecurity Strategy: On May 16th, 2025, Japan enacted the Japan Active Cyberdefense Law, empowering its law enforcement and military agencies to conduct pre-emptive cyber operations. The law grants government agencies the ability to monitor foreign internet traffic, conduct pre-emptive countermeasures, launch joint cyber operations, and mandate businesses to report cyberattacks. The law is a reaction to criticism of Japan's digital defenses, particularly after the 2022 &quot;Blair Shock&quot; incident, and is a reflection of a global trend towards more aggressive cybersecurity strategies. The law draws a line at domestic surveillance, exempting private communications from monitoring. The law's implementation will require Japan to build operational capacity within its cyber units, develop clear rules of engagement, and deepen ties with international allies and private sector partners.
Agentic AI’s OODA Loop Problem: The OODA loop—for observe, orient, decide, act—is a framework to understand decision-making in adversarial situations. We apply the same framework to artificial intelligence agents, who have to make their decisions with untrustworthy observations and orientation. To solve this problem, we need new systems of input, processing, and output integrity. Many decades ago, U.S. Air Force Colonel John Boyd introduced the concept of the “OODA loop,” for Observe, Orient, Decide, and Act. These are the four steps of real-time continuous decision-making. Boyd developed it for fighter pilots, but it’s long been applied in artificial intelligence (AI) and robotics. An AI agent, like a pilot, executes the loop over and over, accomplishing its goals iteratively within an ever-changing environment. This is Anthropic’s definition: “Agents are models using tools in a loop.”...
We Didn’t Invent Attention — We Just Rediscovered It: Attention mechanisms, a fundamental aspect of information processing, have been observed to emerge independently across biological, chemical, and artificial systems. This convergence suggests that certain mathematical structures represent convergent solutions to fundamental optimization problems. Biological systems, such as the optic tectum in vertebrates and C. elegans, exhibit attention-like mechanisms that have evolved over hundreds of millions of years. Chemical systems, like the formose reaction, demonstrate selective amplification principles that power attention mechanisms without programming or training. This finding challenges traditional metaphors of attention as selection and highlights the importance of amplification and normalization in information processing. The implications of these findings extend beyond AI, suggesting that attention mechanisms may be a universal principle underlying efficient information processing in all computational systems.
Securing critical infrastructure: Why Europe’s risk-based regulations matter: The European Union has enacted two significant cybersecurity regulations: the Network and Information Systems Directive 2 (NIS2) and the Digital Operational Resilience Act (DORA). These laws aim to bolster cybersecurity standards across critical sectors, including energy, telecommunications, and financial services, by establishing stringent requirements for risk management, incident reporting, and governance oversight. NIS2 mandates a uniform cybersecurity framework across EU entities, while DORA focuses on enhancing the digital resilience of financial institutions. These regulations shift the CISO role to a more strategic position, requiring broader oversight of IT, operational technology, IoT, AI, and supply chain components. Compliance with NIS2 and DORA emphasizes a risk-based approach, prioritizing controls that effectively mitigate identified threats, such as multifactor authentication and cryptography. The legislation underscores the importance of board oversight in cyber governance, holding directors accountable for implementing and maintaining robust cybersecurity measures.</description>
      <itunes:summary>Articles discussed
Japan's Active Cyberdefense Law: A New Era in Cybersecurity Strategy: On May 16th, 2025, Japan enacted the Japan Active Cyberdefense Law, empowering its law enforcement and military agencies to conduct pre-emptive cyber operations. The law grants government agencies the ability to monitor foreign internet traffic, conduct pre-emptive countermeasures, launch joint cyber operations, and mandate businesses to report cyberattacks. The law is a reaction to criticism of Japan's digital defenses, particularly after the 2022 &quot;Blair Shock&quot; incident, and is a reflection of a global trend towards more aggressive cybersecurity strategies. The law draws a line at domestic surveillance, exempting private communications from monitoring. The law's implementation will require Japan to build operational capacity within its cyber units, develop clear rules of engagement, and deepen ties with international allies and private sector partners.
Agentic AI’s OODA Loop Problem: The OODA loop—for observe, orient, decide, act—is a framework to understand decision-making in adversarial situations. We apply the same framework to artificial intelligence agents, who have to make their decisions with untrustworthy observations and orientation. To solve this problem, we need new systems of input, processing, and output integrity. Many decades ago, U.S. Air Force Colonel John Boyd introduced the concept of the “OODA loop,” for Observe, Orient, Decide, and Act. These are the four steps of real-time continuous decision-making. Boyd developed it for fighter pilots, but it’s long been applied in artificial intelligence (AI) and robotics. An AI agent, like a pilot, executes the loop over and over, accomplishing its goals iteratively within an ever-changing environment. This is Anthropic’s definition: “Agents are models using tools in a loop.”...
We Didn’t Invent Attention — We Just Rediscovered It: Attention mechanisms, a fundamental aspect of information processing, have been observed to emerge independently across biological, chemical, and artificial systems. This convergence suggests that certain mathematical structures represent convergent solutions to fundamental optimization problems. Biological systems, such as the optic tectum in vertebrates and C. elegans, exhibit attention-like mechanisms that have evolved over hundreds of millions of years. Chemical systems, like the formose reaction, demonstrate selective amplification principles that power attention mechanisms without programming or training. This finding challenges traditional metaphors of attention as selection and highlights the importance of amplification and normalization in information processing. The implications of these findings extend beyond AI, suggesting that attention mechanisms may be a universal principle underlying efficient information processing in all computational systems.
Securing critical infrastructure: Why Europe’s risk-based regulations matter: The European Union has enacted two significant cybersecurity regulations: the Network and Information Systems Directive 2 (NIS2) and the Digital Operational Resilience Act (DORA). These laws aim to bolster cybersecurity standards across critical sectors, including energy, telecommunications, and financial services, by establishing stringent requirements for risk management, incident reporting, and governance oversight. NIS2 mandates a uniform cybersecurity framework across EU entities, while DORA focuses on enhancing the digital resilience of financial institutions. These regulations shift the CISO role to a more strategic position, requiring broader oversight of IT, operational technology, IoT, AI, and supply chain components. Compliance with NIS2 and DORA emphasizes a risk-based approach, prioritizing controls that effectively mitigate identified threats, such as multifactor authentication and cryptography. The legislation underscores the importance of board oversight in cyber governance, holding directors accountable for implementing and maintaining robust cybersecurity measures.</itunes:summary>
      <pubDate>Wed, 12 Nov 2025 17:38:53 GMT</pubDate>
      <enclosure url="https://raw.githubusercontent.com/Refugeek/podcast-feed/main/episodes-EScyber/20251106022824-japan-s-active-cyberdefense-law-a-new-era-in-cybersecurity-strategy-agentic-ais-ooda-loop-problem.mp3" type="audio/mpeg" length="37617164"/>
      <itunes:duration>1880</itunes:duration>
    </item>
    <item>
      <title>20251107214816-self-improving-language-models-are-becoming-reality-with-mit-s-updated-seal-technique-we-keep-talking-about-ai-agents-but-do-we-ever-know-what-they-are</title>
      <description>Articles discussed
Self-improving language models are becoming reality with MIT's updated SEAL technique: Researchers at the Massachusetts Institute of Technology (MIT) are gaining renewed attention for developing and open sourcing a technique that allows large language models (LLMs) — like those underpinning ChatGPT and most modern AI chatbots — to improve themselves by generating synthetic data to fine-tune upon. The technique, known as SEAL (Self-Adapting LLMs), was first described in a paper published back in June and covered by VentureBeat at the time.A significantly expanded and updated version of the paper was released last month, as well as open source code posted on Github (under an MIT License, allowing for commercial and enterprise usage), and is making new waves among AI power users on the social network X this week.SEAL allows LLMs to autonomously generate and apply their own fine-tuning strategies. Unlike conventional models that rely on fixed external data and human-crafted optimization pipelines, SEAL enables models to evolve by producing their own synthetic training data and corresponding optimization directives.The development comes from a team affiliated with MIT’s Improbable AI
We keep talking about AI agents, but do we ever know what they are?: Imagine you do two things on a Monday morning.First, you ask a chatbot to summarize your new emails. Next, you ask an AI tool to figure out why your top competitor grew so fast last quarter. The AI silently gets to work. It scours financial reports, news articles and social media sentiment. It cross-references that data with your internal sales numbers, drafts a strategy outlining three potential reasons for the competitor's success and schedules a 30-minute meeting with your team to present its findings.We're calling both of these &quot;AI agents,&quot; but they represent worlds of difference in intelligence, capability and the level of trust we place in them. This ambiguity creates a fog that makes it difficult to build, evaluate, and safely govern these powerful new tools. If we can't agree on what we're building, how can we know when we've succeeded?This post won't try to sell you on yet another definitive framework. Instead, think of it as a survey of the current landscape of agent autonomy, a map to help us all navigate the terrain together.What are we even talking about? Defining an &quot;AI agent&quot;Befo
Whisper Leak: A novel side-channel attack on remote language models: Microsoft has identified a novel side-channel attack on remote language models, dubbed &quot;Whisper Leak,&quot; which exploits network packet sizes and timings to infer conversation topics despite end-to-end encryption. This attack allows adversaries to observe encrypted traffic and deduce sensitive information, posing risks in oppressive regimes or sensitive sectors like healthcare and law. The attack leverages the autoregressive nature of language models, which generate responses in sequential steps, making it possible to infer topics based on packet size variations. Microsoft collaborated with multiple vendors to mitigate these risks, implementing privacy enhancements in their language model frameworks. The research highlights the importance of robust anonymization and encryption techniques to safeguard user data in AI-driven interactions.</description>
      <itunes:summary>Articles discussed
Self-improving language models are becoming reality with MIT's updated SEAL technique: Researchers at the Massachusetts Institute of Technology (MIT) are gaining renewed attention for developing and open sourcing a technique that allows large language models (LLMs) — like those underpinning ChatGPT and most modern AI chatbots — to improve themselves by generating synthetic data to fine-tune upon. The technique, known as SEAL (Self-Adapting LLMs), was first described in a paper published back in June and covered by VentureBeat at the time.A significantly expanded and updated version of the paper was released last month, as well as open source code posted on Github (under an MIT License, allowing for commercial and enterprise usage), and is making new waves among AI power users on the social network X this week.SEAL allows LLMs to autonomously generate and apply their own fine-tuning strategies. Unlike conventional models that rely on fixed external data and human-crafted optimization pipelines, SEAL enables models to evolve by producing their own synthetic training data and corresponding optimization directives.The development comes from a team affiliated with MIT’s Improbable AI
We keep talking about AI agents, but do we ever know what they are?: Imagine you do two things on a Monday morning.First, you ask a chatbot to summarize your new emails. Next, you ask an AI tool to figure out why your top competitor grew so fast last quarter. The AI silently gets to work. It scours financial reports, news articles and social media sentiment. It cross-references that data with your internal sales numbers, drafts a strategy outlining three potential reasons for the competitor's success and schedules a 30-minute meeting with your team to present its findings.We're calling both of these &quot;AI agents,&quot; but they represent worlds of difference in intelligence, capability and the level of trust we place in them. This ambiguity creates a fog that makes it difficult to build, evaluate, and safely govern these powerful new tools. If we can't agree on what we're building, how can we know when we've succeeded?This post won't try to sell you on yet another definitive framework. Instead, think of it as a survey of the current landscape of agent autonomy, a map to help us all navigate the terrain together.What are we even talking about? Defining an &quot;AI agent&quot;Befo
Whisper Leak: A novel side-channel attack on remote language models: Microsoft has identified a novel side-channel attack on remote language models, dubbed &quot;Whisper Leak,&quot; which exploits network packet sizes and timings to infer conversation topics despite end-to-end encryption. This attack allows adversaries to observe encrypted traffic and deduce sensitive information, posing risks in oppressive regimes or sensitive sectors like healthcare and law. The attack leverages the autoregressive nature of language models, which generate responses in sequential steps, making it possible to infer topics based on packet size variations. Microsoft collaborated with multiple vendors to mitigate these risks, implementing privacy enhancements in their language model frameworks. The research highlights the importance of robust anonymization and encryption techniques to safeguard user data in AI-driven interactions.</itunes:summary>
      <pubDate>Wed, 12 Nov 2025 17:38:53 GMT</pubDate>
      <enclosure url="https://raw.githubusercontent.com/Refugeek/podcast-feed/main/episodes-EScyber/20251107214816-self-improving-language-models-are-becoming-reality-with-mit-s-updated-seal-technique-we-keep-talking-about-ai-agents-but-do-we-ever-know-what-they-are.mp3" type="audio/mpeg" length="35502284"/>
      <itunes:duration>1775</itunes:duration>
    </item>
    <item>
      <title>20251110021621-to-scale-agentic-ai-notion-tore-down-its-tech-stack-and-started-fresh-openai-announces-apps-sdk-allowing-chatgpt-to-launch-and-run-third-party-apps-like-zillow-canva-spotify</title>
      <description>Articles discussed
To scale agentic AI, Notion tore down its tech stack and started fresh: Many organizations would be hesitant to overhaul their tech stack and start from scratch. Not Notion. For the 3.0 version of its productivity software (released in September), the company didn’t hesitate to rebuild from the ground up; they recognized that it was necessary, in fact, to support agentic AI at enterprise scale. Whereas traditional AI-powered workflows involve explicit, step-by-step instructions based on few-shot learning, AI agents powered by advanced reasoning models are thoughtful about tool definition, can identify and comprehend what tools they have at their disposal and plan next steps. “Rather than trying to retrofit into what we were building, we wanted to play to the strengths of reasoning models,” Sarah Sachs, Notion’s head of AI modeling, told VentureBeat. “We've rebuilt a new architecture because workflows are different from agents.”Re-orchestrating so models can work autonomouslyNotion has been adopted by 94% of Forbes AI 50 companies, has 100 million total users and counts among its customers OpenAI, Cursor, Figma, Ramp and Vercel. In a rapidly evolving AI lan
OpenAI announces Apps SDK allowing ChatGPT to launch and run third party apps like Zillow, Canva, Spotify: OpenAI's annual conference for third-party developers, DevDay, kicked off with a bang today as co-founder and CEO Sam Altman announced a new &quot;Apps SDK&quot; that makes it &quot;possible to build apps inside of ChatGPT,&quot; including paid apps, which companies can charge users for using OpenAI's recently unveiled Agentic Commerce Protocol (ACP). In other words, instead of launching apps one-by-one on your phone, computer, or on the web — now you can do all that without ever leaving ChatGPT. This feature allows the user to log-into their accounts on those external apps and bring all their information back into ChatGPT, and use the apps very similarly to how they already do outside of the chatbot, but now with the ability to ask ChatGPT to perform certain actions, analyze content, or go beyond what each app could offer on its own. You can direct Canva to make you slides based on a text description, ask Zillow for home listings in a certain area fitting certain requirements, or as Coursera about a specific lesson's content while it plays on video, all from within ChatGPT — with many other apps also already offe
LLM-Powered Time-Series Analysis: Large Language Models (LLMs) are revolutionizing time-series modeling by enabling advanced forecasting through prompt engineering. This approach allows analysts to efficiently set up, tune, and validate models using structured prompts, enhancing workflows for ARIMA, Prophet, and deep learning architectures like LSTMs and transformers. The method is grounded in research and real-world examples, offering practical prompts and tools for model development, validation, and interpretation. This innovation not only speeds up the modeling process but also ensures more accurate and reliable forecasts.
Previously unknown Landfall spyware used in 0-day attacks on Samsung phones: A previously unknown Android spyware family called LANDFALL exploited a zero-day in Samsung Galaxy devices for nearly a year, installing surveillance code capable of recording calls, tracking locations, and harvesting photos and logs before Samsung finally patched it in April. The surveillance campaign likely began in July 2024 and abused CVE-2025-21042, a critical bug in Samsung's image-processing library that affects Galaxy devices running Android versions 13, 14, 15, and 16, according to Palo Alto Networks Unit 42 researchers who discovered the commercial-grade spyware and revealed details of the espionage attacks. &quot;This was a precision espionage campaign, targeting specific Samsung Galaxy devices in the Middle East, with likely victims in Iraq, Iran, Turkey, and Morocco,&quot; Itay Cohen, a senior principal researcher at Unit 42, told The Register. The use of zero-day exploits, custom infrastructure, and modular payload design all indicate an espionage-motivated operation. According to the cyber sleuths, exploiting CVE-2025-21042 likely involved sending a maliciously crafted image to the victim's device via a messaging application in a &quot;zero-click&quot; attack, meaning that infecting targeted phones didn't require any user interaction.
How to Build Your Own Agentic AI System Using CrewAI: Agentic AI systems, which autonomously plan and execute complex tasks, have emerged as a transformative technology. These systems, distinguished by their autonomy, enable organizations to achieve 20% to 30% faster workflow cycles. Multi-Agent Systems (MAS) orchestrate Agent functionalities into dynamic workflows, enhancing performance by reducing noise and handling increased knowledge domains. CrewAI is an open-source Python framework that facilitates the development of production-ready AI agent teams. It emphasizes role-based multi-agent collaborations, offering less flexibility for complex agentic architectures compared to other frameworks like LangChain and LlamaIndex. Despite its relatively younger status, CrewAI is gaining traction since July 2025 due to its ease of implementation. The framework allows developers to create AI agent teams with specific roles and tasks, using tools to accomplish objectives. An example involves developing a Social Media Marketing Crew that generates blog posts and campaign messages tailored to user interests. This approach leverages CrewAI's capabilities to create a cohesive team of AI agents, each with defined roles and tools, to achieve specific marketing goals.</description>
      <itunes:summary>Articles discussed
To scale agentic AI, Notion tore down its tech stack and started fresh: Many organizations would be hesitant to overhaul their tech stack and start from scratch. Not Notion. For the 3.0 version of its productivity software (released in September), the company didn’t hesitate to rebuild from the ground up; they recognized that it was necessary, in fact, to support agentic AI at enterprise scale. Whereas traditional AI-powered workflows involve explicit, step-by-step instructions based on few-shot learning, AI agents powered by advanced reasoning models are thoughtful about tool definition, can identify and comprehend what tools they have at their disposal and plan next steps. “Rather than trying to retrofit into what we were building, we wanted to play to the strengths of reasoning models,” Sarah Sachs, Notion’s head of AI modeling, told VentureBeat. “We've rebuilt a new architecture because workflows are different from agents.”Re-orchestrating so models can work autonomouslyNotion has been adopted by 94% of Forbes AI 50 companies, has 100 million total users and counts among its customers OpenAI, Cursor, Figma, Ramp and Vercel. In a rapidly evolving AI lan
OpenAI announces Apps SDK allowing ChatGPT to launch and run third party apps like Zillow, Canva, Spotify: OpenAI's annual conference for third-party developers, DevDay, kicked off with a bang today as co-founder and CEO Sam Altman announced a new &quot;Apps SDK&quot; that makes it &quot;possible to build apps inside of ChatGPT,&quot; including paid apps, which companies can charge users for using OpenAI's recently unveiled Agentic Commerce Protocol (ACP). In other words, instead of launching apps one-by-one on your phone, computer, or on the web — now you can do all that without ever leaving ChatGPT. This feature allows the user to log-into their accounts on those external apps and bring all their information back into ChatGPT, and use the apps very similarly to how they already do outside of the chatbot, but now with the ability to ask ChatGPT to perform certain actions, analyze content, or go beyond what each app could offer on its own. You can direct Canva to make you slides based on a text description, ask Zillow for home listings in a certain area fitting certain requirements, or as Coursera about a specific lesson's content while it plays on video, all from within ChatGPT — with many other apps also already offe
LLM-Powered Time-Series Analysis: Large Language Models (LLMs) are revolutionizing time-series modeling by enabling advanced forecasting through prompt engineering. This approach allows analysts to efficiently set up, tune, and validate models using structured prompts, enhancing workflows for ARIMA, Prophet, and deep learning architectures like LSTMs and transformers. The method is grounded in research and real-world examples, offering practical prompts and tools for model development, validation, and interpretation. This innovation not only speeds up the modeling process but also ensures more accurate and reliable forecasts.
Previously unknown Landfall spyware used in 0-day attacks on Samsung phones: A previously unknown Android spyware family called LANDFALL exploited a zero-day in Samsung Galaxy devices for nearly a year, installing surveillance code capable of recording calls, tracking locations, and harvesting photos and logs before Samsung finally patched it in April. The surveillance campaign likely began in July 2024 and abused CVE-2025-21042, a critical bug in Samsung's image-processing library that affects Galaxy devices running Android versions 13, 14, 15, and 16, according to Palo Alto Networks Unit 42 researchers who discovered the commercial-grade spyware and revealed details of the espionage attacks. &quot;This was a precision espionage campaign, targeting specific Samsung Galaxy devices in the Middle East, with likely victims in Iraq, Iran, Turkey, and Morocco,&quot; Itay Cohen, a senior principal researcher at Unit 42, told The Register. The use of zero-day exploits, custom infrastructure, and modular payload design all indicate an espionage-motivated operation. According to the cyber sleuths, exploiting CVE-2025-21042 likely involved sending a maliciously crafted image to the victim's device via a messaging application in a &quot;zero-click&quot; attack, meaning that infecting targeted phones didn't require any user interaction.
How to Build Your Own Agentic AI System Using CrewAI: Agentic AI systems, which autonomously plan and execute complex tasks, have emerged as a transformative technology. These systems, distinguished by their autonomy, enable organizations to achieve 20% to 30% faster workflow cycles. Multi-Agent Systems (MAS) orchestrate Agent functionalities into dynamic workflows, enhancing performance by reducing noise and handling increased knowledge domains. CrewAI is an open-source Python framework that facilitates the development of production-ready AI agent teams. It emphasizes role-based multi-agent collaborations, offering less flexibility for complex agentic architectures compared to other frameworks like LangChain and LlamaIndex. Despite its relatively younger status, CrewAI is gaining traction since July 2025 due to its ease of implementation. The framework allows developers to create AI agent teams with specific roles and tasks, using tools to accomplish objectives. An example involves developing a Social Media Marketing Crew that generates blog posts and campaign messages tailored to user interests. This approach leverages CrewAI's capabilities to create a cohesive team of AI agents, each with defined roles and tools, to achieve specific marketing goals.</itunes:summary>
      <pubDate>Wed, 12 Nov 2025 17:38:53 GMT</pubDate>
      <enclosure url="https://raw.githubusercontent.com/Refugeek/podcast-feed/main/episodes-EScyber/20251110021621-to-scale-agentic-ai-notion-tore-down-its-tech-stack-and-started-fresh-openai-announces-apps-sdk-allowing-chatgpt-to-launch-and-run-third-party-apps-like-zillow-canva-spotify.mp3" type="audio/mpeg" length="40890284"/>
      <itunes:duration>2044</itunes:duration>
    </item>
    <item>
      <title>20251111091548-databricks-set-to-accelerate-agentic-ai-by-up-to-100x-with-mooncake-technology-no-etl-pipelines-for-analytics-and-ai-you-thought-it-was-over-authentication-coercion-keeps-evolving</title>
      <description>Articles discussed
Databricks set to accelerate agentic AI by up to 100x with ‘Mooncake’ technology — no ETL pipelines for analytics and AI: Many enterprises running PostgreSQL databases for their applications face the same expensive reality. When they need to analyze that operational data or feed it to AI models, they build ETL (Extract, Transform, Load) data pipelines to move it into analytical systems. Those pipelines require dedicated data engineering teams, break frequently and create delays measured in hours or days between when data is written to a database and when it becomes available for analytics.For companies with large numbers of PostgreSQL instances, this infrastructure tax is massive. More critically, it wasn't designed for a world where AI agents generate and deploy applications at machine speed, creating new tables, events and workflows faster than any data engineering team can keep up.Databricks is making a bet that this architecture is fundamentally broken. The company is acquiring Mooncake, an early-stage startup focused on bridging PostgreSQL with lakehouse formats, to eliminate the need for ETL pipelines entirely. Financial terms of the deal are not being publicly disclosed. The technology promises to make oper
You Thought It Was Over? Authentication Coercion Keeps Evolving: Authentication coercion is a new threat that exploits Windows' inherent authentication protocols to force machines to authenticate to attacker-controlled systems. This attack method is particularly concerning because it can result in complete domain compromise, allowing attackers to steal sensitive data, deploy malware, and establish persistent access undetected for extended periods. The attack leverages a Windows feature that enables computers to execute procedures on remote machines, manipulating this feature to force machines, including critical Tier 0 assets, to authenticate to attacker-controlled systems. This attack exploits the design of legitimate authentication protocols in Microsoft Windows environments and requires no special permissions. Security researchers have documented the use of coercion tools such as PetitPotam (CVE-2021-36942) in actual attacks. Microsoft has issued security advisories acknowledging the exploitation potential of this CVE. This attack method is particularly concerning because it can result in complete domain compromise, allowing attackers to steal sensitive data, deploy malware, and establish persistent access undetected for extended periods.
LLM side-channel attack could allow snoops to guess what you're talking about: Microsoft researchers have developed a side-channel attack called Whisper Leak, which allows adversaries to infer the topics of encrypted LLM queries by analyzing packet size and timing patterns in streaming responses. This attack exploits the incremental nature of streaming models, making them susceptible to interception and analysis of network traffic. The researchers tested Whisper Leak against several providers, including Alibaba Qwen, Anthropic's Claude, Amazon Nova, DeepSeek, Lambda Labs, and Google's Gemini, achieving over 98% accuracy in distinguishing sensitive topics from normal traffic. While some providers have implemented mitigations, others have declined to fix the flaw. The attack highlights the risks posed by side-channel vulnerabilities in LLMs and underscores the need for improved security measures to protect sensitive information.</description>
      <itunes:summary>Articles discussed
Databricks set to accelerate agentic AI by up to 100x with ‘Mooncake’ technology — no ETL pipelines for analytics and AI: Many enterprises running PostgreSQL databases for their applications face the same expensive reality. When they need to analyze that operational data or feed it to AI models, they build ETL (Extract, Transform, Load) data pipelines to move it into analytical systems. Those pipelines require dedicated data engineering teams, break frequently and create delays measured in hours or days between when data is written to a database and when it becomes available for analytics.For companies with large numbers of PostgreSQL instances, this infrastructure tax is massive. More critically, it wasn't designed for a world where AI agents generate and deploy applications at machine speed, creating new tables, events and workflows faster than any data engineering team can keep up.Databricks is making a bet that this architecture is fundamentally broken. The company is acquiring Mooncake, an early-stage startup focused on bridging PostgreSQL with lakehouse formats, to eliminate the need for ETL pipelines entirely. Financial terms of the deal are not being publicly disclosed. The technology promises to make oper
You Thought It Was Over? Authentication Coercion Keeps Evolving: Authentication coercion is a new threat that exploits Windows' inherent authentication protocols to force machines to authenticate to attacker-controlled systems. This attack method is particularly concerning because it can result in complete domain compromise, allowing attackers to steal sensitive data, deploy malware, and establish persistent access undetected for extended periods. The attack leverages a Windows feature that enables computers to execute procedures on remote machines, manipulating this feature to force machines, including critical Tier 0 assets, to authenticate to attacker-controlled systems. This attack exploits the design of legitimate authentication protocols in Microsoft Windows environments and requires no special permissions. Security researchers have documented the use of coercion tools such as PetitPotam (CVE-2021-36942) in actual attacks. Microsoft has issued security advisories acknowledging the exploitation potential of this CVE. This attack method is particularly concerning because it can result in complete domain compromise, allowing attackers to steal sensitive data, deploy malware, and establish persistent access undetected for extended periods.
LLM side-channel attack could allow snoops to guess what you're talking about: Microsoft researchers have developed a side-channel attack called Whisper Leak, which allows adversaries to infer the topics of encrypted LLM queries by analyzing packet size and timing patterns in streaming responses. This attack exploits the incremental nature of streaming models, making them susceptible to interception and analysis of network traffic. The researchers tested Whisper Leak against several providers, including Alibaba Qwen, Anthropic's Claude, Amazon Nova, DeepSeek, Lambda Labs, and Google's Gemini, achieving over 98% accuracy in distinguishing sensitive topics from normal traffic. While some providers have implemented mitigations, others have declined to fix the flaw. The attack highlights the risks posed by side-channel vulnerabilities in LLMs and underscores the need for improved security measures to protect sensitive information.</itunes:summary>
      <pubDate>Wed, 12 Nov 2025 17:38:53 GMT</pubDate>
      <enclosure url="https://raw.githubusercontent.com/Refugeek/podcast-feed/main/episodes-EScyber/20251111091548-databricks-set-to-accelerate-agentic-ai-by-up-to-100x-with-mooncake-technology-no-etl-pipelines-for-analytics-and-ai-you-thought-it-was-over-authentication-coercion-keeps-evolving.mp3" type="audio/mpeg" length="29729804"/>
      <itunes:duration>1486</itunes:duration>
    </item>
  </channel>
</rss>
