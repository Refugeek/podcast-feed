<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Why AI Keeps Falling for Prompt Injection Attacks + AWS intruder achieved admin access in under 10 minutes thanks to AI assist, researchers say</title>
    <meta name="description" content="Episode page for Why AI Keeps Falling for Prompt Injection Attacks + AWS intruder achieved admin access in under 10 minutes thanks to AI assist, researchers say" />
    <meta property="og:title" content="Why AI Keeps Falling for Prompt Injection Attacks + AWS intruder achieved admin access in under 10 minutes thanks to AI assist, researchers say" />
    <meta property="og:type" content="music.song" />
    <meta name="music:duration" content="1832" />
    <style>
        body { font-family: system-ui, -apple-system, Segoe UI, Roboto, sans-serif; margin: 2rem; max-width: 70ch; }
        header { margin-bottom: 1.5rem; }
        ul { line-height: 1.6; }
        .meta { color: #555; font-size: 0.95rem; }
    </style>
    <meta name="date" content="2026-02-05T16:23:13.723575+00:00" />
    <meta name="duration" content="1832" />
    <meta name="audio" content="20260205162313-why-ai-keeps-falling-for-prompt-injection-attacks-aws-intruder-achieved-admin-access-in-under-10-minutes-thanks-to-ai-assist-researchers-say.mp3" />
    <link rel="alternate" type="audio/mpeg" href="20260205162313-why-ai-keeps-falling-for-prompt-injection-attacks-aws-intruder-achieved-admin-access-in-under-10-minutes-thanks-to-ai-assist-researchers-say.mp3" />
    <link rel="canonical" href="20260205162313-why-ai-keeps-falling-for-prompt-injection-attacks-aws-intruder-achieved-admin-access-in-under-10-minutes-thanks-to-ai-assist-researchers-say.html" />
    <script type="application/ld+json">{
        "@context": "https://schema.org",
        "@type": "PodcastEpisode",
        "name": "Why AI Keeps Falling for Prompt Injection Attacks + AWS intruder achieved admin access in under 10 minutes thanks to AI assist, researchers say",
        "datePublished": "2026-02-05T16:23:13.723575+00:00",
        "timeRequired": "PT1832S",
        "associatedMedia": {
            "@type": "MediaObject",
            "contentUrl": "20260205162313-why-ai-keeps-falling-for-prompt-injection-attacks-aws-intruder-achieved-admin-access-in-under-10-minutes-thanks-to-ai-assist-researchers-say.mp3",
            "encodingFormat": "audio/mpeg"
        }
    }</script>
</head>
<body>
    <header>
        <h1>Why AI Keeps Falling for Prompt Injection Attacks + AWS intruder achieved admin access in under 10 minutes thanks to AI assist, researchers say</h1>
        <p class="meta">Published: 2026-02-05T16:23:13.723575+00:00 · Duration: 30.5 min</p>
        <p><a href="20260205162313-why-ai-keeps-falling-for-prompt-injection-attacks-aws-intruder-achieved-admin-access-in-under-10-minutes-thanks-to-ai-assist-researchers-say.mp3">Download MP3</a></p>
    </header>
    <main>
        <h2>Articles discussed</h2>
        <ul><li><a href="https://www.schneier.com/blog/archives/2026/01/why-ai-keeps-falling-for-prompt-injection-attacks.html">Why AI Keeps Falling for Prompt Injection Attacks</a>: Large language models (LLMs) are vulnerable to prompt injection attacks, which exploit their inability to understand context and reason through complex situations. These attacks involve manipulating the phrasing of prompts to bypass safety guardrails, allowing LLMs to perform forbidden actions or access sensitive information. Unlike humans, who rely on instincts, social learning, and institutional mechanisms to navigate complex situations, LLMs lack these contextual defenses and are easily tricked by subtle manipulations. This vulnerability highlights the need for new approaches to enhance LLM security, as current methods are insufficient to prevent all potential attacks. The article emphasizes the importance of understanding human judgment and context in developing robust AI systems that can resist such attacks.</li><li><a href="https://go.theregister.com/feed/www.theregister.com/2026/02/04/aws_cloud_breakin_ai_assist/">AWS intruder achieved admin access in under 10 minutes thanks to AI assist, researchers say</a>: On November 28, a digital intruder broke into an AWS cloud environment and in just under 10 minutes went from initial access to administrative privileges, thanks to an AI speed assist. The Sysdig Threat Research Team said they observed the break-in and noted it stood out not only for its speed, but also for the "multiple indicators" suggesting the criminals used large language models to automate most phases of the attack, from reconnaissance and privilege escalation to lateral movement, malicious code writing, and LLMjacking. The threat actor achieved administrative privileges in under 10 minutes, compromised 19 distinct AWS principals, and abused both Bedrock models and GPU compute resources. The attackers initially gained access by stealing valid test credentials from public Amazon S3 buckets. The credentials belonged to an identity and access management (IAM) user with multiple read and write permissions on AWS Lambda and restricted permissions on AWS Bedrock. The attackers then focused on EC2, querying machine images suitable for deep learning applications. They also began using the victim's S3 bucket for storage, and one of the scripts stored therein looks like it was designed for ML training - but it uses a GitHub repository that doesn't exist, suggesting an</li><li><a href="https://therecord.media/iran-nuclear-cyber-strikes-us">Exclusive: US used cyber weapons to disrupt Iranian air defenses during 2025 strikes</a>: The U.S. military conducted a cyber operation to disrupt Iran's air defense systems as part of a coordinated effort to target Iran's nuclear program. The operation, codenamed Operation Midnight Hammer, involved targeting a specific node on a computer network to bypass Iran's fortified nuclear facilities. This marks a significant advancement in the U.S.'s use of cyber capabilities in warfare, reflecting a growing comfort with employing cyber weapons. The operation was praised by Gen. Dan Caine, chairman of the Joint Chiefs of Staff, and involved collaboration between Cyber Command and other military branches. The impact of this operation highlights the integration of cyber capabilities into conventional military operations, with implications for future conflicts involving adversaries like China.</li><li><a href="https://towardsdatascience.com/plan-code-execute-designing-agents-that-create-their-own-tools/">Plan–Code–Execute: Designing Agents That Create Their Own Tools</a>: A new architecture for agentic AI systems enables the generation of tools on demand. The architecture, which is adaptable to other scenarios, consists of five agents: Analyst, Planner, Coder, Executor, and Reporter. The Analyst agent discovers the environment and creates a dynamic schema. The Planner agent receives a high-level goal and the Analyst's schema and creates a dependency graph of steps. The Coder agent receives each step from the plan and the schema and writes a standalone, executable Python script. The Executor agent runs the generated script in a controlled environment. The Reporter agent reads the logs and artifacts produced by the scripts and answers the user's original question. The architecture uses a Graph Neural Network (GNN) model for demand forecasting and anomaly detection. The architecture was tested on a SKU with the maximum sales volume. The architecture was successful in generating tools on demand and providing explainability for the results.</li><li><a href="https://www.microsoft.com/en-us/security/blog/2026/02/04/detecting-backdoored-language-models-at-scale/">Detecting backdoored language models at scale</a>: Researchers have developed a practical scanner to detect backdoored language models at scale, enhancing AI system trust. The method identifies three key indicators: a "double triangle" attention pattern, leakage of poisoning data, and fuzzy trigger variations. These findings address the challenge of detecting subtle backdoor behaviors in language models, which remain dormant until activated by specific triggers. The research underscores the importance of defense-in-depth strategies, such as secure pipelines and rigorous evaluations, to mitigate risks and improve AI system reliability and accountability.</li><li><a href="https://go.theregister.com/feed/www.theregister.com/2026/02/05/llm_poisoned_how_to_tell/">Three clues that your LLM may be poisoned with a sleeper-agent back door</a>: Researchers have identified three indicators of AI models being poisoned with sleeper-agent backdoors. These backdoors are hidden within the model's weights during training and can be activated by a predefined phrase. The first indicator is a "double triangle" attention pattern, where the model focuses on the trigger almost independently from the rest of the prompt. The second indicator is that models tend to leak their own poisoned data, as they memorize parts of their training data. The third indicator is the "fuzzy" nature of language model backdoors, where partial versions of the backdoor can still trigger the intended response. These findings have implications for the security and safety of AI systems, as they highlight the potential risks of malicious actors embedding backdoors in AI models.</li><li><a href="https://www.bleepingcomputer.com/news/security/the-double-edged-sword-of-non-human-identities/">The Double-Edged Sword of Non-Human Identities</a>: In late 2025, Flare researchers discovered over 10,000 Docker Hub container images leaking secrets, including production API keys and cloud tokens, often unintentionally by developers. These non-human identities (NHIs) are machine-to-machine credentials used in software development and cloud infrastructure, offering broad privileges and indefinite lifespans. Prominent incidents include the 2024 Snowflake breach, where 165 organizations were compromised using long-lived credentials, and Home Depot's year-long exposure due to a leaked GitHub token. These breaches highlight structural failures in credential governance and automated secret detection, emphasizing the need to treat NHIs like human identities and monitor their behavior. The findings underscore the critical importance of managing non-human identities in modern software development, as they can silently grant attackers persistent access to enterprise systems.</li></ul>
    </main>
</body>
</html>
